{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output files are written out in parent directory: C:\\Users\\tgoforth\\Documents\\IPM temporal resolution project\\outputs\n",
      "output files are written out in parent directory: C:\\Users\\tgoforth\\Documents\\IPM temporal resolution project\\outputs/load\n"
     ]
    }
   ],
   "source": [
    "#importing packages needed for analysis\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from pandas import DataFrame\n",
    "\n",
    "path = os.getcwd()\n",
    "#print(path)\n",
    "\n",
    "load_dur = pd.read_csv('../outputs/load_long_format.csv')\n",
    "solar_dur = pd.read_csv('../outputs/solar_long_format.csv')\n",
    "wind_dur = pd.read_csv('../outputs/wind_long_format.csv')\n",
    "\n",
    "## UNCOMMENT WHICH PROFILE TO BE USED\n",
    "x = load_dur\n",
    "x_name = 'load'\n",
    "x_name2 = 'Load'\n",
    "x_column = 'Load'\n",
    "\n",
    "#x = solar_dur\n",
    "#x_name = 'solar'\n",
    "#x_name2 = 'Solar_Gen'\n",
    "#x_column = 'TRG6'\n",
    "\n",
    "#x = wind_dur\n",
    "#x_name = 'wind'\n",
    "#x_name2 = 'Wind_Gen'\n",
    "#x_column = 'TRG4'\n",
    "\n",
    "#this code creates an output directory in the parent director, if one does not exist yet\n",
    "#Note: this is where all of the output files will be written, since outputs are large this saves space in git\n",
    "path = os.getcwd()\n",
    "parent = os.path.dirname(path)\n",
    "outputs_dir = parent+'\\outputs'\n",
    "if not os.path.exists(outputs_dir):\n",
    "    os.makedirs(outputs_dir)\n",
    "print('output files are written out in parent directory: '+outputs_dir)\n",
    "\n",
    "outputs_x = outputs_dir+'/'+x_name\n",
    "if not os.path.exists(outputs_x):\n",
    "    os.makedirs(outputs_x)\n",
    "print('output files are written out in parent directory: '+outputs_x)\n",
    "\n",
    "x = x[['Region','R_Group','R_Subgroup','Season','Month','Day','Hour',x_column]]\n",
    "years = pd.read_csv('inputs/years.csv').dropna()\n",
    "#print(x.head())\n",
    "\n",
    "unique_r = pd.Series(load_dur['Region'].unique()).dropna()\n",
    "#print(unique_r)\n",
    "reg_count = unique_r.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region R_Group R_Subgroup  Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "0  ERC_REST     ERC       REST  winter      1    1     1  34807    1  2016   \n",
      "1  ERC_REST     ERC       REST  winter      1    2     1  34716    2  2016   \n",
      "2  ERC_REST     ERC       REST  winter      1    3     1  34736    3  2016   \n",
      "3  ERC_REST     ERC       REST  winter      1    4     1  35914    4  2016   \n",
      "4  ERC_REST     ERC       REST  winter      1    5     1  33845    5  2016   \n",
      "\n",
      "        Date  DOW  Weekday  \n",
      "0 2016-01-01    4     True  \n",
      "1 2016-01-02    5    False  \n",
      "2 2016-01-03    6    False  \n",
      "3 2016-01-04    0     True  \n",
      "4 2016-01-05    1     True  \n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "daydata = pd.read_csv('inputs/days_365.csv')\n",
    "#print(daydata.tail())\n",
    "daydata = daydata.drop(columns='Month')\n",
    "x2 = pd.merge(x,daydata,on=['Day'],how='left')\n",
    "#print(x2.tail())\n",
    "\n",
    "#sets the year for each region\n",
    "x2['Year']=2011\n",
    "x2.loc[x2['R_Group'] == 'ERC', 'Year'] = 2016\n",
    "#print(x2.head())\n",
    "\n",
    "#Creates a date column\n",
    "x2 = x2.rename(columns={'Day':'DOY','DayofMo':'Day'})\n",
    "x2['Date']=pd.to_datetime(x2[['Year', 'Month', 'Day']], errors='coerce')\n",
    "#print(x2.tail())\n",
    "\n",
    "#convert date to a datetime type \n",
    "#x2['Date'] = pd.to_datetime(x2['Date'])\n",
    "x2['DOW'] = x2['Date'].dt.weekday\n",
    "\n",
    "#check if it is a weekday or not \n",
    "weekday = pd.read_csv('inputs/weekday.csv')\n",
    "x2 = pd.merge(x2,weekday,on='DOW',how='left')\n",
    "print(x2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Monthly, single day type, 24 hours (288 segments)\n",
    "#### Methodology: Using groupby function to group first by month, then by 24 hours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Month  Hour  Hour_Tot      Tot           Avg\n",
      "0  ERC_REST      1     1        31  1042446  33627.290323\n",
      "1  ERC_REST      1     2        31  1051661  33924.548387\n",
      "2  ERC_REST      1     3        31  1079739  34830.290323\n",
      "3  ERC_REST      1     4        31  1144492  36919.096774\n",
      "4  ERC_REST      1     5        31  1239385  39980.161290\n",
      "number of segments in dataset = 288.0\n",
      "\n",
      "         Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  \\\n",
      "46075  ERC_REST     ERC       REST  shoulder      3   86     3  26989   27   \n",
      "23088  ERC_REST     ERC       REST  shoulder      4   94     2  27006    4   \n",
      "46061  ERC_REST     ERC       REST  shoulder      3   72     3  27025   13   \n",
      "\n",
      "       Year       Date  DOW  Weekday  Hour_Tot     Tot           Avg  \n",
      "46075  2016 2016-03-27    6    False        31  886708  28603.483871  \n",
      "23088  2016 2016-04-04    0     True        30  870591  29019.700000  \n",
      "46061  2016 2016-03-13    6    False        31  886708  28603.483871  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case1_x = x2.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case1 = case1_x.groupby(['Region','Month','Hour'],as_index=False).agg(aggregations)\n",
    "case1.columns = case1.columns.droplevel(0)\n",
    "case1.columns = ['Region','Month','Hour','Hour_Tot','Tot','Avg']\n",
    "print(case1.head())\n",
    "print('number of segments in dataset =',case1.shape[0]/reg_count)\n",
    "case1.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_1dt_mon_24hr.csv')\n",
    "print()\n",
    "\n",
    "case1_x2 = pd.merge(case1_x,case1,on=['Region','Month','Hour'],how='left')\n",
    "case1_x2 = case1_x2.sort_values(['Region',x_column])\n",
    "print(case1_x2.head(3))\n",
    "print('number of rows in dataset =',case1_x2.shape[0])\n",
    "case1_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_1dt_mon_24hr.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: Season, single day-type, 24 hours (72 segments)\n",
    "#### Methodology: Use groupby function to group by season and hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region    Season  Hour  Hour_Tot      Tot           Avg\n",
      "0  ERC_REST  shoulder     1       122  3635975  29803.073770\n",
      "1  ERC_REST  shoulder     2       122  3587428  29405.147541\n",
      "2  ERC_REST  shoulder     3       122  3603145  29533.975410\n",
      "3  ERC_REST  shoulder     4       122  3731428  30585.475410\n",
      "4  ERC_REST  shoulder     5       122  3992001  32721.319672\n",
      "number of segments in dataset = 72.0\n",
      "\n",
      "         Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  \\\n",
      "46075  ERC_REST     ERC       REST  shoulder      3   86     3  26989   27   \n",
      "23088  ERC_REST     ERC       REST  shoulder      4   94     2  27006    4   \n",
      "46061  ERC_REST     ERC       REST  shoulder      3   72     3  27025   13   \n",
      "\n",
      "       Year       Date  DOW  Weekday  Hour_Tot      Tot           Avg  \n",
      "46075  2016 2016-03-27    6    False       122  3603145  29533.975410  \n",
      "23088  2016 2016-04-04    0     True       122  3587428  29405.147541  \n",
      "46061  2016 2016-03-13    6    False       122  3603145  29533.975410  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case2_x = x2.copy()\n",
    "#case2_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "#case2_seasons = case2_seasons.drop(['bimonthly'], axis=1)\n",
    "#case2_seasons = case2_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "#print(case2_x.head())\n",
    "#case2_x = pd.merge(case2_x, case2_seasons, on='Month', how='left')\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case2 = case2_x.groupby(['Region','Season','Hour'],as_index=False).agg(aggregations)\n",
    "case2.columns = case2.columns.droplevel(0)\n",
    "case2.columns = ['Region','Season','Hour','Hour_Tot','Tot','Avg']\n",
    "print(case2.head())\n",
    "print('number of segments in dataset =',case2.shape[0]/reg_count)\n",
    "case2.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_1dt_sea_24hr.csv')\n",
    "print()\n",
    "\n",
    "case2_x2 = pd.merge(case2_x,case2,on=['Region','Season','Hour'],how='left')\n",
    "case2_x2 = case2_x2.sort_values(['Region',x_column])\n",
    "print(case2_x2.head(3))\n",
    "print('number of rows in dataset =',case2_x2.shape[0])\n",
    "case2_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_1dt_sea_24hr.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3: Monthly, weekend/weekday, 24 hours (576 segments)\n",
    "#### Metholodogy: Use groupby function to group by month, then weekend/weekday, then by 24 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Month  Hour  Hour_Tot      Tot           Avg\n",
      "0  ERC_REST      1     1        31  1042446  33627.290323\n",
      "1  ERC_REST      1     2        31  1051661  33924.548387\n",
      "2  ERC_REST      1     3        31  1079739  34830.290323\n",
      "3  ERC_REST      1     4        31  1144492  36919.096774\n",
      "4  ERC_REST      1     5        31  1239385  39980.161290\n",
      "number of segments in dataset = 576.0\n",
      "\n",
      "         Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  \\\n",
      "46075  ERC_REST     ERC       REST  shoulder      3   86     3  26989   27   \n",
      "23088  ERC_REST     ERC       REST  shoulder      4   94     2  27006    4   \n",
      "46061  ERC_REST     ERC       REST  shoulder      3   72     3  27025   13   \n",
      "46054  ERC_REST     ERC       REST  shoulder      3   65     3  27059    6   \n",
      "23059  ERC_REST     ERC       REST  shoulder      3   65     2  27069    6   \n",
      "\n",
      "       Year       Date  DOW  Weekday  Hour_Tot     Tot           Avg  \n",
      "46075  2016 2016-03-27    6    False         8  222011  27751.375000  \n",
      "23088  2016 2016-04-04    0     True        21  612083  29146.809524  \n",
      "46061  2016 2016-03-13    6    False         8  222011  27751.375000  \n",
      "46054  2016 2016-03-06    6    False         8  222011  27751.375000  \n",
      "23059  2016 2016-03-06    6    False         8  221751  27718.875000  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case3_x = x2.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case3 = case3_x.groupby(['Region','Month','Weekday','Hour'],as_index=False).agg(aggregations)\n",
    "case3.columns = case3.columns.droplevel(0)\n",
    "case3.columns = ['Region','Month','Weekday','Hour','Hour_Tot','Tot','Avg']\n",
    "print(case1.head())\n",
    "print('number of segments in dataset =',case3.shape[0]/reg_count)\n",
    "case3.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_2dt_mon_24hr.csv')\n",
    "print()\n",
    "\n",
    "case3_x2 = pd.merge(case3_x,case3,on=['Region','Month','Weekday','Hour'],how='left')\n",
    "case3_x2 = case3_x2.sort_values(['Region',x_column])\n",
    "print(case3_x2.head())\n",
    "print('number of rows in dataset =',case3_x2.shape[0])\n",
    "case3_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_2dt_mon_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 4: Season, weekend/weekday, 24 hours (144 segments)\n",
    "#### Methodology: groupby season, weekday, hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Region  Season  Weekday  Hour  Hour_Tot     Tot          Avg\n",
      "9067  WEC_SDGE  winter     True    20        63  190280  3020.317460\n",
      "9068  WEC_SDGE  winter     True    21        63  182258  2892.984127\n",
      "9069  WEC_SDGE  winter     True    22        63  167541  2659.380952\n",
      "9070  WEC_SDGE  winter     True    23        63  149291  2369.698413\n",
      "9071  WEC_SDGE  winter     True    24        63  133301  2115.888889\n",
      "number of segments in dataset = 144.0\n",
      "\n",
      "         Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  \\\n",
      "46075  ERC_REST     ERC       REST  shoulder      3   86     3  26989   27   \n",
      "23088  ERC_REST     ERC       REST  shoulder      4   94     2  27006    4   \n",
      "46061  ERC_REST     ERC       REST  shoulder      3   72     3  27025   13   \n",
      "46054  ERC_REST     ERC       REST  shoulder      3   65     3  27059    6   \n",
      "23059  ERC_REST     ERC       REST  shoulder      3   65     2  27069    6   \n",
      "\n",
      "       Year       Date  DOW  Weekday  Hour_Tot      Tot           Avg  \n",
      "46075  2016 2016-03-27    6    False        35  1003593  28674.085714  \n",
      "23088  2016 2016-04-04    0     True        87  2578430  29637.126437  \n",
      "46061  2016 2016-03-13    6    False        35  1003593  28674.085714  \n",
      "46054  2016 2016-03-06    6    False        35  1003593  28674.085714  \n",
      "23059  2016 2016-03-06    6    False        35  1008998  28828.514286  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case4_x = x2.copy()\n",
    "#case4_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "#case4_seasons = case4_seasons.drop(['bimonthly'], axis=1)\n",
    "#case4_seasons = case4_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "#case4_x = pd.merge(case4_x, case4_seasons, on='Month', how='left')\n",
    "#print(case4_x)\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case4 = case4_x.groupby(['Region','Season','Weekday','Hour'],as_index=False).agg(aggregations)\n",
    "case4.columns = case4.columns.droplevel(0)\n",
    "case4.columns = ['Region','Season','Weekday','Hour','Hour_Tot','Tot','Avg']\n",
    "print(case4.tail())\n",
    "print('number of segments in dataset =',case4.shape[0]/reg_count)\n",
    "case4.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_2dt_sea_24hr.csv')\n",
    "print()\n",
    "\n",
    "case4_x2 = pd.merge(case4_x,case4,on=['Region','Season','Weekday','Hour'],how='left')\n",
    "case4_x2 = case4_x2.sort_values(['Region',x_column])\n",
    "print(case4_x2.head())\n",
    "print('number of rows in dataset =',case4_x2.shape[0])\n",
    "case4_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_2dt_sea_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 hour interval day types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Region R_Group R_Subgroup  Season  Month  DOY  Hour  Load  Day  Year  \\\n",
      "551875  WECC_WY     WEC         WY  winter     12  361    24  1767   27  2011   \n",
      "551876  WECC_WY     WEC         WY  winter     12  362    24  1744   28  2011   \n",
      "551877  WECC_WY     WEC         WY  winter     12  363    24  1805   29  2011   \n",
      "551878  WECC_WY     WEC         WY  winter     12  364    24  1791   30  2011   \n",
      "551879  WECC_WY     WEC         WY  winter     12  365    24  1834   31  2011   \n",
      "\n",
      "             Date  DOW  Weekday  4-hr  \n",
      "551875 2011-12-27    1     True     6  \n",
      "551876 2011-12-28    2     True     6  \n",
      "551877 2011-12-29    3     True     6  \n",
      "551878 2011-12-30    4     True     6  \n",
      "551879 2011-12-31    5    False     6  \n"
     ]
    }
   ],
   "source": [
    "#read in 4 hour interval counter\n",
    "interval_4hr = pd.read_csv('inputs/interval_4hr.csv')\n",
    "#print(interval_4hr)\n",
    "\n",
    "x3 = pd.merge(x2,interval_4hr,on='Hour',how='left')\n",
    "print(x3.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 5: Monthly, single day type, 4 hour intervals (72 segments)\n",
    "#### Methodology: use groupby by month, 4 hour intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Month  4-hr  Hour_Tot      Tot           Avg\n",
      "0  ERC_REST      1     1       124  4318338  34825.306452\n",
      "1  ERC_REST      1     2       124  5025668  40529.580645\n",
      "2  ERC_REST      1     3       124  4711362  37994.854839\n",
      "3  ERC_REST      1     4       124  4504207  36324.250000\n",
      "4  ERC_REST      1     5       124  4847894  39095.919355\n",
      "number of segments in dataset = 72.0\n",
      "\n",
      "         Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  \\\n",
      "46075  ERC_REST     ERC       REST  shoulder      3   86     3  26989   27   \n",
      "23088  ERC_REST     ERC       REST  shoulder      4   94     2  27006    4   \n",
      "46061  ERC_REST     ERC       REST  shoulder      3   72     3  27025   13   \n",
      "46054  ERC_REST     ERC       REST  shoulder      3   65     3  27059    6   \n",
      "23059  ERC_REST     ERC       REST  shoulder      3   65     2  27069    6   \n",
      "\n",
      "       Year       Date  DOW  Weekday  4-hr  Hour_Tot      Tot           Avg  \n",
      "46075  2016 2016-03-27    6    False     1       124  3572927  28813.927419  \n",
      "23088  2016 2016-04-04    0     True     1       120  3539489  29495.741667  \n",
      "46061  2016 2016-03-13    6    False     1       124  3572927  28813.927419  \n",
      "46054  2016 2016-03-06    6    False     1       124  3572927  28813.927419  \n",
      "23059  2016 2016-03-06    6    False     1       124  3572927  28813.927419  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case5_x = x3.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case5 = case5_x.groupby(['Region','Month','4-hr'],as_index=False).agg(aggregations)\n",
    "case5.columns = case5.columns.droplevel(0)\n",
    "case5.columns = ['Region','Month','4-hr','Hour_Tot','Tot','Avg']\n",
    "print(case5.head())\n",
    "print('number of segments in dataset =',case5.shape[0]/reg_count)\n",
    "case5.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_1dt_mon_4hr.csv')\n",
    "print()\n",
    "\n",
    "case5_x2 = pd.merge(case5_x,case5,on=['Region','Month','4-hr'],how='left')\n",
    "case5_x2 = case5_x2.sort_values(['Region',x_column])\n",
    "print(case5_x2.head())\n",
    "print('number of rows in dataset =',case5_x2.shape[0])\n",
    "case5_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_1dt_mon_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 6: Bi-monthly weekend/weekday day-types, 4-hour intervals\n",
    "#### Methodology: use groupby function and bimonthly groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Bimonth  Weekday  4-hr  Hour_Tot      Tot           Avg\n",
      "0  ERC_REST        1    False     1        72  2362801  32816.680556\n",
      "1  ERC_REST        1    False     2        72  2705068  37570.388889\n",
      "2  ERC_REST        1    False     3        72  2622321  36421.125000\n",
      "3  ERC_REST        1    False     4        72  2539521  35271.125000\n",
      "4  ERC_REST        1    False     5        72  2729661  37911.958333\n",
      "number of segments in dataset = 72.0\n",
      "\n",
      "         Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  \\\n",
      "46075  ERC_REST     ERC       REST  shoulder      3   86     3  26989   27   \n",
      "23088  ERC_REST     ERC       REST  shoulder      4   94     2  27006    4   \n",
      "46061  ERC_REST     ERC       REST  shoulder      3   72     3  27025   13   \n",
      "46054  ERC_REST     ERC       REST  shoulder      3   65     3  27059    6   \n",
      "23059  ERC_REST     ERC       REST  shoulder      3   65     2  27069    6   \n",
      "\n",
      "       Year       Date  DOW  Weekday  4-hr  Bimonth  Hour_Tot      Tot  \\\n",
      "46075  2016 2016-03-27    6    False     1        2        68  1932774   \n",
      "23088  2016 2016-04-04    0     True     1        2       176  5179642   \n",
      "46061  2016 2016-03-13    6    False     1        2        68  1932774   \n",
      "46054  2016 2016-03-06    6    False     1        2        68  1932774   \n",
      "23059  2016 2016-03-06    6    False     1        2        68  1932774   \n",
      "\n",
      "                Avg  \n",
      "46075  28423.147059  \n",
      "23088  29429.784091  \n",
      "46061  28423.147059  \n",
      "46054  28423.147059  \n",
      "23059  28423.147059  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case6_x = x3.copy()\n",
    "case6_bimonth = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case6_bimonth = case6_bimonth.drop(['seasonal'], axis=1)\n",
    "case6_bimonth = case6_bimonth.rename(columns={'bimonthly':'Bimonth','month':'Month'})\n",
    "\n",
    "case6_x = pd.merge(case6_x, case6_bimonth, on='Month', how='left')\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case6 = case6_x.groupby(['Region','Bimonth','Weekday','4-hr'],as_index=False).agg(aggregations)\n",
    "case6.columns = case6.columns.droplevel(0)\n",
    "case6.columns = ['Region','Bimonth','Weekday','4-hr','Hour_Tot','Tot','Avg']\n",
    "print(case6.head())\n",
    "print('number of segments in dataset =',case6.shape[0]/reg_count)\n",
    "case6.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_2dt_bim_4hr.csv')\n",
    "print()\n",
    "\n",
    "case6_x2 = pd.merge(case6_x,case6,on=['Region','Bimonth','Weekday','4-hr'],how='left')\n",
    "case6_x2 = case6_x2.sort_values(['Region',x_column])\n",
    "print(case6_x2.head())\n",
    "print('number of rows in dataset =',case6_x2.shape[0])\n",
    "case6_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_2dt_bim_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 7: Season-based months, weekend/weekday day-types, 4-hour intervals\n",
    "#### Methodology: groupby function and applied season groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Season_Group  Weekday  4-hr  Hour_Tot      Tot           Avg\n",
      "0  ERC_REST             1    False     1       108  3533655  32719.027778\n",
      "1  ERC_REST             1    False     2       108  3948984  36564.666667\n",
      "2  ERC_REST             1    False     3       108  3994962  36990.388889\n",
      "3  ERC_REST             1    False     4       108  3897223  36085.398148\n",
      "4  ERC_REST             1    False     5       108  4166114  38575.129630\n",
      "number of segments in dataset = 60.0\n",
      "\n",
      "         Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  \\\n",
      "46075  ERC_REST     ERC       REST  shoulder      3   86     3  26989   27   \n",
      "23088  ERC_REST     ERC       REST  shoulder      4   94     2  27006    4   \n",
      "46061  ERC_REST     ERC       REST  shoulder      3   72     3  27025   13   \n",
      "46054  ERC_REST     ERC       REST  shoulder      3   65     3  27059    6   \n",
      "23059  ERC_REST     ERC       REST  shoulder      3   65     2  27069    6   \n",
      "\n",
      "       Year       Date  DOW  Weekday  4-hr  Season_Group  Hour_Tot       Tot  \\\n",
      "46075  2016 2016-03-27    6    False     1             2       140   4051780   \n",
      "23088  2016 2016-04-04    0     True     1             2       348  10506196   \n",
      "46061  2016 2016-03-13    6    False     1             2       140   4051780   \n",
      "46054  2016 2016-03-06    6    False     1             2       140   4051780   \n",
      "23059  2016 2016-03-06    6    False     1             2       140   4051780   \n",
      "\n",
      "                Avg  \n",
      "46075  28941.285714  \n",
      "23088  30190.218391  \n",
      "46061  28941.285714  \n",
      "46054  28941.285714  \n",
      "23059  28941.285714  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case7_x = x3.copy()\n",
    "case7_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case7_seasons = case7_seasons.drop(['bimonthly'], axis=1)\n",
    "case7_seasons = case7_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "\n",
    "case7_x = pd.merge(case7_x, case7_seasons, on='Month', how='left')\n",
    "#print(case4_load)\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case7 = case7_x.groupby(['Region','Season_Group','Weekday','4-hr'],as_index=False).agg(aggregations)\n",
    "case7.columns = case7.columns.droplevel(0)\n",
    "case7.columns = ['Region','Season_Group','Weekday','4-hr','Hour_Tot','Tot','Avg']\n",
    "print(case7.head())\n",
    "print('number of segments in dataset =',case7.shape[0]/reg_count)\n",
    "case7.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_2dt_sgp_4hr.csv')\n",
    "print()\n",
    "\n",
    "case7_x2 = pd.merge(case7_x,case7,on=['Region','Season_Group','Weekday','4-hr'],how='left')\n",
    "case7_x2 = case7_x2.sort_values(['Region',x_column])\n",
    "print(case7_x2.head())\n",
    "print('number of rows in dataset =',case7_x2.shape[0])\n",
    "case7_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_2dt_sgp_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 8: Seasons, weekend/weekday day-types, 4-hour intervals\n",
    "#### Methodology: groupby function and applied season groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region    Season  Weekday  4-hr  Hour_Tot      Tot           Avg\n",
      "0  ERC_REST  shoulder    False     1       140  4051780  28941.285714\n",
      "1  ERC_REST  shoulder    False     2       140  4378835  31277.392857\n",
      "2  ERC_REST  shoulder    False     3       140  4993309  35666.492857\n",
      "3  ERC_REST  shoulder    False     4       140  5296142  37829.585714\n",
      "4  ERC_REST  shoulder    False     5       140  5276131  37686.650000\n",
      "number of segments in dataset = 36.0\n",
      "\n",
      "         Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  \\\n",
      "46075  ERC_REST     ERC       REST  shoulder      3   86     3  26989   27   \n",
      "23088  ERC_REST     ERC       REST  shoulder      4   94     2  27006    4   \n",
      "46061  ERC_REST     ERC       REST  shoulder      3   72     3  27025   13   \n",
      "46054  ERC_REST     ERC       REST  shoulder      3   65     3  27059    6   \n",
      "23059  ERC_REST     ERC       REST  shoulder      3   65     2  27069    6   \n",
      "\n",
      "       Year       Date  DOW  Weekday  4-hr  Hour_Tot       Tot           Avg  \n",
      "46075  2016 2016-03-27    6    False     1       140   4051780  28941.285714  \n",
      "23088  2016 2016-04-04    0     True     1       348  10506196  30190.218391  \n",
      "46061  2016 2016-03-13    6    False     1       140   4051780  28941.285714  \n",
      "46054  2016 2016-03-06    6    False     1       140   4051780  28941.285714  \n",
      "23059  2016 2016-03-06    6    False     1       140   4051780  28941.285714  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case8_x = x3.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case8 = case8_x.groupby(['Region','Season','Weekday','4-hr'],as_index=False).agg(aggregations)\n",
    "case8.columns = case8.columns.droplevel(0)\n",
    "case8.columns = ['Region','Season','Weekday','4-hr','Hour_Tot','Tot','Avg']\n",
    "print(case8.head())\n",
    "print('number of segments in dataset =',case8.shape[0]/reg_count)\n",
    "case8.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_2dt_sea_4hr.csv')\n",
    "print()\n",
    "\n",
    "case8_x2 = pd.merge(case8_x,case8,on=['Region','Season','Weekday','4-hr'],how='left')\n",
    "case8_x2 = case8_x2.sort_values(['Region',x_column])\n",
    "print(case8_x2.head())\n",
    "print('number of rows in dataset =',case8_x2.shape[0])\n",
    "case8_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_2dt_sea_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day type: Three (Weekday, Weekend, Peak Load)\n",
    "## Find Peak Load Days in Each Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows in regional data = 365.0\n",
      "      Region  Month  Load_MD_Max\n",
      "0   ERC_REST      1      1000605\n",
      "1   ERC_REST      2       932895\n",
      "2   ERC_REST      3       886217\n",
      "3   ERC_REST      4       983152\n",
      "4   ERC_REST      5      1083459\n",
      "5   ERC_REST      6      1211630\n",
      "6   ERC_REST      7      1267082\n",
      "7   ERC_REST      8      1332321\n",
      "8   ERC_REST      9      1231350\n",
      "9   ERC_REST     10      1109240\n",
      "10  ERC_REST     11      1004548\n",
      "11  ERC_REST     12      1176963\n",
      "\n",
      "     Region R_Group R_Subgroup  Season  Month  DOY  Hour  Load_MD_Tot  \\\n",
      "0  ERC_REST     ERC       REST  winter      1    1     1       917588   \n",
      "1  ERC_REST     ERC       REST  winter      1    2     1       863132   \n",
      "2  ERC_REST     ERC       REST  winter      1    3     1       934513   \n",
      "3  ERC_REST     ERC       REST  winter      1    4     1       960576   \n",
      "4  ERC_REST     ERC       REST  winter      1    5     1       916216   \n",
      "\n",
      "   Load_MD_Max  \n",
      "0      1000605  \n",
      "1      1000605  \n",
      "2      1000605  \n",
      "3      1000605  \n",
      "4      1000605  \n"
     ]
    }
   ],
   "source": [
    "#create temporary DF to find peak\n",
    "load = load_dur.copy()\n",
    "\n",
    "#groupby region, month, and day to sum the total day\n",
    "aggregations1 = {'Load':sum}\n",
    "md_sum = load.groupby(['Region','Month','Day'],as_index=False).agg(aggregations1)\n",
    "md_sum.columns = ['Region','Month','Day','Load_MD_Tot']\n",
    "#print(md_sum.tail())\n",
    "print('number of rows in regional data =',md_sum.shape[0]/reg_count)\n",
    "\n",
    "md_sum2 = pd.merge(load,md_sum,on=['Region','Month','Day'],how='left')\n",
    "#print(md_sum2.tail())\n",
    "\n",
    "#groupby region and month to find maximum \n",
    "aggregations2 = {'Load_MD_Tot':max}\n",
    "md_max = md_sum2.groupby(['Region','Month'],as_index=False).agg(aggregations2)\n",
    "md_max.columns = ['Region','Month','Load_MD_Max']\n",
    "print(md_max[0:12])\n",
    "print()\n",
    "\n",
    "peakd = pd.merge(md_sum2,md_max,on=['Region','Month'],how='left')\n",
    "peakd = peakd.drop(columns=['Unnamed: 0','Load'])\n",
    "peakd = peakd.rename(columns={'Day':'DOY'})\n",
    "print(peakd.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 29.5 MiB for an array with shape (7, 551880) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-9c942f6408b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_peak\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpeakd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Region'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'R_Group'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'R_Subgroup'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Season'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Month'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'DOY'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Hour'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mx_peak\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_peak\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'Weekday'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'Day_Type'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#print(x_peak.tail())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#Return True if the load total equals the day identified as the max\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     )\n\u001b[1;32m---> 88\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    657\u001b[0m             \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mllabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoin_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m             \u001b[0mconcat_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 659\u001b[1;33m             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m         )\n\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m   2020\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2021\u001b[0m             b = make_block(\n\u001b[1;32m-> 2022\u001b[1;33m                 \u001b[0mconcatenate_join_units\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2023\u001b[0m                 \u001b[0mplacement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2024\u001b[0m             )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\concat.py\u001b[0m in \u001b[0;36mconcatenate_join_units\u001b[1;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[0;32m    246\u001b[0m     to_concat = [\n\u001b[0;32m    247\u001b[0m         \u001b[0mju\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_reindexed_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupcasted_na\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupcasted_na\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mju\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mjoin_units\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m     ]\n\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\concat.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    246\u001b[0m     to_concat = [\n\u001b[0;32m    247\u001b[0m         \u001b[0mju\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_reindexed_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupcasted_na\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupcasted_na\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mju\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mjoin_units\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m     ]\n\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\concat.py\u001b[0m in \u001b[0;36mget_reindexed_values\u001b[1;34m(self, empty_dtype, upcasted_na)\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malgos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake_nd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, out, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m   1655\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"F\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1656\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1657\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1659\u001b[0m     func = _get_take_nd_function(\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 29.5 MiB for an array with shape (7, 551880) and data type int64"
     ]
    }
   ],
   "source": [
    "x_peak = pd.merge(x2,peakd,on=['Region','R_Group','R_Subgroup','Season','Month','DOY','Hour'],how='left')\n",
    "x_peak = x_peak.rename(columns={'Weekday':'Day_Type'})\n",
    "#print(x_peak.tail())\n",
    "\n",
    "#Return True if the load total equals the day identified as the max\n",
    "x_peak.loc[x_peak['Day_Type'] == True, 'Day_Type'] = 'Weekday'\n",
    "x_peak.loc[x_peak['Day_Type'] == False, 'Day_Type'] = 'Weekend'\n",
    "x_peak.loc[x_peak['Load_MD_Tot'] == x_peak['Load_MD_Max'], 'Day_Type'] = 'Peak'\n",
    "x_peak = x_peak.drop(['Load_MD_Tot','Load_MD_Max'], axis=1)\n",
    "print(x_peak[0:15])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Monthly, Weekend/weekday/peak day-types, 24 hours (864 segments)\n",
    "#### Methodology: similar to two day type, just adding in peak day types to sort by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Month Day_Type  Hour  Hour_Tot    Tot      Avg\n",
      "0  ERC_REST      1     Peak     1         1  38567  38567.0\n",
      "1  ERC_REST      1     Peak     2         1  39107  39107.0\n",
      "2  ERC_REST      1     Peak     3         1  40311  40311.0\n",
      "3  ERC_REST      1     Peak     4         1  43115  43115.0\n",
      "4  ERC_REST      1     Peak     5         1  47186  47186.0\n",
      "number of segments in dataset = 864.0\n",
      "\n",
      "         Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  \\\n",
      "46075  ERC_REST     ERC       REST  shoulder      3   86     3  26989   27   \n",
      "23088  ERC_REST     ERC       REST  shoulder      4   94     2  27006    4   \n",
      "46061  ERC_REST     ERC       REST  shoulder      3   72     3  27025   13   \n",
      "\n",
      "       Year       Date  DOW Day_Type  Hour_Tot     Tot        Avg  \n",
      "46075  2016 2016-03-27    6  Weekend         8  222011  27751.375  \n",
      "23088  2016 2016-04-04    0  Weekday        20  579213  28960.650  \n",
      "46061  2016 2016-03-13    6  Weekend         8  222011  27751.375  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case1_x = x_peak.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case1 = case1_x.groupby(['Region','Month','Day_Type','Hour'],as_index=False).agg(aggregations)\n",
    "case1.columns = case1.columns.droplevel(0)\n",
    "case1.columns = ['Region','Month','Day_Type','Hour','Hour_Tot','Tot','Avg']\n",
    "print(case1.head())\n",
    "print('number of segments in dataset =',case1.shape[0]/reg_count)\n",
    "case1.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_3dt_mon_24hr.csv')\n",
    "print()\n",
    "\n",
    "case1_x2 = pd.merge(case1_x,case1,on=['Region','Month','Day_Type','Hour'],how='left')\n",
    "case1_x2 = case1_x2.sort_values(['Region',x_column])\n",
    "print(case1_x2.head(3))\n",
    "print('number of rows in dataset =',case1_x2.shape[0])\n",
    "case1_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_3dt_mon_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: Season, weekend/weekday/peak day-types, 24-hours (216 segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region    Season Day_Type  Hour  Hour_Tot     Tot       Avg\n",
      "0  ERC_REST  shoulder     Peak     1         4  132690  33172.50\n",
      "1  ERC_REST  shoulder     Peak     2         4  130802  32700.50\n",
      "2  ERC_REST  shoulder     Peak     3         4  131751  32937.75\n",
      "3  ERC_REST  shoulder     Peak     4         4  137910  34477.50\n",
      "4  ERC_REST  shoulder     Peak     5         4  150015  37503.75\n",
      "number of segments in dataset = 216.0\n",
      "\n",
      "         Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  \\\n",
      "46075  ERC_REST     ERC       REST  shoulder      3   86     3  26989   27   \n",
      "23088  ERC_REST     ERC       REST  shoulder      4   94     2  27006    4   \n",
      "46061  ERC_REST     ERC       REST  shoulder      3   72     3  27025   13   \n",
      "\n",
      "       Year       Date  DOW Day_Type  Hour_Tot     Tot        Avg  \n",
      "46075  2016 2016-03-27    6  Weekend         8  222011  27751.375  \n",
      "23088  2016 2016-04-04    0  Weekday        20  579213  28960.650  \n",
      "46061  2016 2016-03-13    6  Weekend         8  222011  27751.375  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case2_x = x_peak.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case2 = case2_x.groupby(['Region','Season','Day_Type','Hour'],as_index=False).agg(aggregations)\n",
    "case2.columns = case2.columns.droplevel(0)\n",
    "case2.columns = ['Region','Season','Day_Type','Hour','Hour_Tot','Tot','Avg']\n",
    "print(case2.head())\n",
    "print('number of segments in dataset =',case2.shape[0]/reg_count)\n",
    "case2.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_3dt_sea_24hr.csv')\n",
    "print()\n",
    "\n",
    "case2_x2 = pd.merge(case2_x,case2,on=['Region','Season','Day_Type','Hour'],how='left')\n",
    "case2_x2 = case2_x2.sort_values(['Region',x_column])\n",
    "print(case1_x2.head(3))\n",
    "print('number of rows in dataset =',case1_x2.shape[0])\n",
    "case2_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_3dt_sea_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3: Annual, weekend/weekday/peak day-types, 24-hours (72 segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region Day_Type  Hour  Hour_Tot     Tot           Avg\n",
      "0  ERC_REST     Peak     1        12  455893  37991.083333\n",
      "1  ERC_REST     Peak     2        12  450804  37567.000000\n",
      "2  ERC_REST     Peak     3        12  454384  37865.333333\n",
      "3  ERC_REST     Peak     4        12  473371  39447.583333\n",
      "4  ERC_REST     Peak     5        12  505999  42166.583333\n",
      "number of segments in dataset = 72.0\n",
      "\n",
      "         Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  \\\n",
      "46075  ERC_REST     ERC       REST  shoulder      3   86     3  26989   27   \n",
      "23088  ERC_REST     ERC       REST  shoulder      4   94     2  27006    4   \n",
      "46061  ERC_REST     ERC       REST  shoulder      3   72     3  27025   13   \n",
      "\n",
      "       Year       Date  DOW Day_Type  Hour_Tot      Tot           Avg  \n",
      "46075  2016 2016-03-27    6  Weekend       104  3359100  32299.038462  \n",
      "23088  2016 2016-04-04    0  Weekday       249  8176806  32838.578313  \n",
      "46061  2016 2016-03-13    6  Weekend       104  3359100  32299.038462  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case3_x = x_peak.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case3 = case3_x.groupby(['Region','Day_Type','Hour'],as_index=False).agg(aggregations)\n",
    "case3.columns = case3.columns.droplevel(0)\n",
    "case3.columns = ['Region','Day_Type','Hour','Hour_Tot','Tot','Avg']\n",
    "print(case3.head())\n",
    "print('number of segments in dataset =',case3.shape[0]/reg_count)\n",
    "case3.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_3dt_ann_24hr.csv')\n",
    "print()\n",
    "\n",
    "case3_x2 = pd.merge(case3_x,case3,on=['Region','Day_Type','Hour'],how='left')\n",
    "case3_x2 = case3_x2.sort_values(['Region',x_column])\n",
    "print(case3_x2.head(3))\n",
    "print('number of rows in dataset =',case3_x2.shape[0])\n",
    "case3_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_3dt_ann_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 hour interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Region R_Group R_Subgroup  Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "0   ERC_REST     ERC       REST  winter      1    1     1  34807    1  2016   \n",
      "1   ERC_REST     ERC       REST  winter      1    2     1  34716    2  2016   \n",
      "2   ERC_REST     ERC       REST  winter      1    3     1  34736    3  2016   \n",
      "3   ERC_REST     ERC       REST  winter      1    4     1  35914    4  2016   \n",
      "4   ERC_REST     ERC       REST  winter      1    5     1  33845    5  2016   \n",
      "5   ERC_REST     ERC       REST  winter      1    6     1  30995    6  2016   \n",
      "6   ERC_REST     ERC       REST  winter      1    7     1  30186    7  2016   \n",
      "7   ERC_REST     ERC       REST  winter      1    8     1  30745    8  2016   \n",
      "8   ERC_REST     ERC       REST  winter      1    9     1  38702    9  2016   \n",
      "9   ERC_REST     ERC       REST  winter      1   10     1  38567   10  2016   \n",
      "10  ERC_REST     ERC       REST  winter      1   11     1  37495   11  2016   \n",
      "11  ERC_REST     ERC       REST  winter      1   12     1  34274   12  2016   \n",
      "12  ERC_REST     ERC       REST  winter      1   13     1  31815   13  2016   \n",
      "13  ERC_REST     ERC       REST  winter      1   14     1  29614   14  2016   \n",
      "14  ERC_REST     ERC       REST  winter      1   15     1  31453   15  2016   \n",
      "\n",
      "         Date  DOW Day_Type  4-hr  \n",
      "0  2016-01-01    4  Weekday     1  \n",
      "1  2016-01-02    5  Weekend     1  \n",
      "2  2016-01-03    6  Weekend     1  \n",
      "3  2016-01-04    0  Weekday     1  \n",
      "4  2016-01-05    1  Weekday     1  \n",
      "5  2016-01-06    2  Weekday     1  \n",
      "6  2016-01-07    3  Weekday     1  \n",
      "7  2016-01-08    4  Weekday     1  \n",
      "8  2016-01-09    5  Weekend     1  \n",
      "9  2016-01-10    6     Peak     1  \n",
      "10 2016-01-11    0  Weekday     1  \n",
      "11 2016-01-12    1  Weekday     1  \n",
      "12 2016-01-13    2  Weekday     1  \n",
      "13 2016-01-14    3  Weekday     1  \n",
      "14 2016-01-15    4  Weekday     1  \n"
     ]
    }
   ],
   "source": [
    "#read in 4 hour interval counter\n",
    "#interval_4hr = pd.read_csv('inputs/interval_4hr.csv')\n",
    "#print(interval_4hr.head())\n",
    "#print(x_peak.head())\n",
    "\n",
    "x_peak2 = pd.merge(x_peak,interval_4hr,on='Hour',how='left')\n",
    "print(x_peak2.head(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 4: Bi-monthly, weekend/weekday/peak day-types, 4-hour intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Bimonth Day_Type  4-hr  Hour_Tot     Tot        Avg\n",
      "0  ERC_REST        1     Peak     1         8  312443  39055.375\n",
      "1  ERC_REST        1     Peak     2         8  366535  45816.875\n",
      "2  ERC_REST        1     Peak     3         8  322104  40263.000\n",
      "3  ERC_REST        1     Peak     4         8  297348  37168.500\n",
      "4  ERC_REST        1     Peak     5         8  332533  41566.625\n",
      "number of segments in dataset = 108.0\n",
      "\n",
      "         Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  \\\n",
      "46075  ERC_REST     ERC       REST  shoulder      3   86     3  26989   27   \n",
      "23088  ERC_REST     ERC       REST  shoulder      4   94     2  27006    4   \n",
      "46061  ERC_REST     ERC       REST  shoulder      3   72     3  27025   13   \n",
      "46054  ERC_REST     ERC       REST  shoulder      3   65     3  27059    6   \n",
      "23059  ERC_REST     ERC       REST  shoulder      3   65     2  27069    6   \n",
      "\n",
      "       Year       Date  DOW Day_Type  4-hr  Bimonth  Hour_Tot      Tot  \\\n",
      "46075  2016 2016-03-27    6  Weekend     1        2        68  1932774   \n",
      "23088  2016 2016-04-04    0  Weekday     1        2       168  4924979   \n",
      "46061  2016 2016-03-13    6  Weekend     1        2        68  1932774   \n",
      "46054  2016 2016-03-06    6  Weekend     1        2        68  1932774   \n",
      "23059  2016 2016-03-06    6  Weekend     1        2        68  1932774   \n",
      "\n",
      "                Avg  \n",
      "46075  28423.147059  \n",
      "23088  29315.351190  \n",
      "46061  28423.147059  \n",
      "46054  28423.147059  \n",
      "23059  28423.147059  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case4_x = x_peak2.copy()\n",
    "case4_bimonth = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case4_bimonth = case4_bimonth.drop(['seasonal'], axis=1)\n",
    "case4_bimonth = case4_bimonth.rename(columns={'bimonthly':'Bimonth','month':'Month'})\n",
    "\n",
    "case4_x = pd.merge(case4_x, case4_bimonth, on='Month', how='left')\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case4 = case4_x.groupby(['Region','Bimonth','Day_Type','4-hr'],as_index=False).agg(aggregations)\n",
    "case4.columns = case4.columns.droplevel(0)\n",
    "case4.columns = ['Region','Bimonth','Day_Type','4-hr','Hour_Tot','Tot','Avg']\n",
    "print(case4.head())\n",
    "print('number of segments in dataset =',case4.shape[0]/reg_count)\n",
    "case4.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_3dt_bim_4hr.csv')\n",
    "print()\n",
    "\n",
    "case4_x2 = pd.merge(case4_x,case4,on=['Region','Bimonth','Day_Type','4-hr'],how='left')\n",
    "case4_x2 = case4_x2.sort_values(['Region',x_column])\n",
    "print(case4_x2.head())\n",
    "print('number of rows in dataset =',case4_x2.shape[0])\n",
    "case4_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_3dt_bim_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 5: Season-based months, weekend/weekday/peak day-types, 4-hour intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Season_Group Day_Type  4-hr  Hour_Tot     Tot           Avg\n",
      "0  ERC_REST             1     Peak     1        12  504276  42023.000000\n",
      "1  ERC_REST             1     Peak     2        12  583360  48613.333333\n",
      "2  ERC_REST             1     Peak     3        12  527061  43921.750000\n",
      "3  ERC_REST             1     Peak     4        12  478413  39867.750000\n",
      "4  ERC_REST             1     Peak     5        12  530620  44218.333333\n",
      "number of segments in dataset = 90.0\n",
      "\n",
      "         Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  \\\n",
      "46075  ERC_REST     ERC       REST  shoulder      3   86     3  26989   27   \n",
      "23088  ERC_REST     ERC       REST  shoulder      4   94     2  27006    4   \n",
      "46061  ERC_REST     ERC       REST  shoulder      3   72     3  27025   13   \n",
      "46054  ERC_REST     ERC       REST  shoulder      3   65     3  27059    6   \n",
      "23059  ERC_REST     ERC       REST  shoulder      3   65     2  27069    6   \n",
      "\n",
      "       Year       Date  DOW Day_Type  4-hr  Season_Group  Hour_Tot      Tot  \\\n",
      "46075  2016 2016-03-27    6  Weekend     1             2       140  4051780   \n",
      "23088  2016 2016-04-04    0  Weekday     1             2       332  9973043   \n",
      "46061  2016 2016-03-13    6  Weekend     1             2       140  4051780   \n",
      "46054  2016 2016-03-06    6  Weekend     1             2       140  4051780   \n",
      "23059  2016 2016-03-06    6  Weekend     1             2       140  4051780   \n",
      "\n",
      "                Avg  \n",
      "46075  28941.285714  \n",
      "23088  30039.286145  \n",
      "46061  28941.285714  \n",
      "46054  28941.285714  \n",
      "23059  28941.285714  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case5_x = x_peak2.copy()\n",
    "case5_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case5_seasons = case5_seasons.drop(['bimonthly'], axis=1)\n",
    "case5_seasons = case5_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "\n",
    "case5_x = pd.merge(case5_x, case5_seasons, on='Month', how='left')\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case5 = case5_x.groupby(['Region','Season_Group','Day_Type','4-hr'],as_index=False).agg(aggregations)\n",
    "case5.columns = case5.columns.droplevel(0)\n",
    "case5.columns = ['Region','Season_Group','Day_Type','4-hr','Hour_Tot','Tot','Avg']\n",
    "print(case5.head())\n",
    "print('number of segments in dataset =',case5.shape[0]/reg_count)\n",
    "case5.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_3dt_sgp_4hr.csv')\n",
    "print()\n",
    "\n",
    "case5_x2 = pd.merge(case5_x,case5,on=['Region','Season_Group','Day_Type','4-hr'],how='left')\n",
    "case5_x2 = case5_x2.sort_values(['Region',x_column])\n",
    "print(case5_x2.head())\n",
    "print('number of rows in dataset =',case5_x2.shape[0])\n",
    "case5_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_3dt_sgp_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 6: Season, weekend/weekday/peak day-types, 4-hour intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region    Season Day_Type  4-hr  Hour_Tot     Tot         Avg\n",
      "0  ERC_REST  shoulder     Peak     1        16  533153  33322.0625\n",
      "1  ERC_REST  shoulder     Peak     2        16  619476  38717.2500\n",
      "2  ERC_REST  shoulder     Peak     3        16  707544  44221.5000\n",
      "3  ERC_REST  shoulder     Peak     4        16  785332  49083.2500\n",
      "4  ERC_REST  shoulder     Peak     5        16  743101  46443.8125\n",
      "number of segments in dataset = 54.0\n",
      "\n",
      "         Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  \\\n",
      "46075  ERC_REST     ERC       REST  shoulder      3   86     3  26989   27   \n",
      "23088  ERC_REST     ERC       REST  shoulder      4   94     2  27006    4   \n",
      "46061  ERC_REST     ERC       REST  shoulder      3   72     3  27025   13   \n",
      "46054  ERC_REST     ERC       REST  shoulder      3   65     3  27059    6   \n",
      "23059  ERC_REST     ERC       REST  shoulder      3   65     2  27069    6   \n",
      "\n",
      "       Year       Date  DOW Day_Type  4-hr  Hour_Tot      Tot           Avg  \n",
      "46075  2016 2016-03-27    6  Weekend     1       140  4051780  28941.285714  \n",
      "23088  2016 2016-04-04    0  Weekday     1       332  9973043  30039.286145  \n",
      "46061  2016 2016-03-13    6  Weekend     1       140  4051780  28941.285714  \n",
      "46054  2016 2016-03-06    6  Weekend     1       140  4051780  28941.285714  \n",
      "23059  2016 2016-03-06    6  Weekend     1       140  4051780  28941.285714  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case6_x = x_peak2.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case6 = case6_x.groupby(['Region','Season','Day_Type','4-hr'],as_index=False).agg(aggregations)\n",
    "case6.columns = case6.columns.droplevel(0)\n",
    "case6.columns = ['Region','Season','Day_Type','4-hr','Hour_Tot','Tot','Avg']\n",
    "print(case6.head())\n",
    "print('number of segments in dataset =',case6.shape[0]/reg_count)\n",
    "case6.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_3dt_sea_4hr.csv')\n",
    "print()\n",
    "\n",
    "case6_x2 = pd.merge(case6_x,case6,on=['Region','Season','Day_Type','4-hr'],how='left')\n",
    "case6_x2 = case6_x2.sort_values(['Region',x_column])\n",
    "print(case6_x2.head())\n",
    "print('number of rows in dataset =',case6_x2.shape[0])\n",
    "case6_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_3dt_sea_4hr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
