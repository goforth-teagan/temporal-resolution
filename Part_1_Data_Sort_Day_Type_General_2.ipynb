{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output files are written out in parent directory: C:\\Users\\cmarcy\\Desktop\\py_projects\\temporal\\new_version\\outputs\n"
     ]
    }
   ],
   "source": [
    "#importing packages needed for analysis\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from pandas import DataFrame\n",
    "\n",
    "path = os.getcwd()\n",
    "#print(path)\n",
    "\n",
    "#this code creates an output directory in the parent director, if one does not exist yet\n",
    "#Note: this is where all of the output files will be written, since outputs are large this saves space in git\n",
    "path = os.getcwd()\n",
    "parent = os.path.dirname(path)\n",
    "outputs_dir = parent+'\\outputs'\n",
    "if not os.path.exists(outputs_dir):\n",
    "    os.makedirs(outputs_dir)\n",
    "print('output files are written out in parent directory: '+outputs_dir)\n",
    "\n",
    "load_dur = pd.read_csv('../outputs/load_long_format.csv')\n",
    "solar_dur = pd.read_csv('../outputs/solar_long_format.csv')\n",
    "wind_dur = pd.read_csv('../outputs/wind_long_format.csv')\n",
    "\n",
    "## UNCOMMENT WHICH PROFILE TO BE USED\n",
    "x = load_dur\n",
    "x_name = 'load'\n",
    "x_name2 = 'Load'\n",
    "x_column = 'Load'\n",
    "\n",
    "#x = solar_dur\n",
    "#x_name = 'solar'\n",
    "#x_name2 = 'Solar_Gen'\n",
    "#x_column = 'TRG6'\n",
    "\n",
    "#x = wind_dur\n",
    "#x_name = 'wind'\n",
    "#x_name2 = 'Wind_Gen'\n",
    "#x_column = 'TRG4'\n",
    "\n",
    "x = x[['Region','R_Group','R_Subgroup','Season','Month','Day','Hour',x_column]]\n",
    "years = pd.read_csv('inputs/years.csv').dropna()\n",
    "#print(x.head())\n",
    "\n",
    "unique_r = pd.Series(load_dur['Region'].unique()).dropna()\n",
    "#print(unique_r)\n",
    "reg_count = unique_r.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region R_Group R_Subgroup  Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "0  ERC_REST     ERC       REST  winter      1    1     1  34807    1  2016   \n",
      "1  ERC_REST     ERC       REST  winter      1    2     1  34716    2  2016   \n",
      "2  ERC_REST     ERC       REST  winter      1    3     1  34736    3  2016   \n",
      "3  ERC_REST     ERC       REST  winter      1    4     1  35914    4  2016   \n",
      "4  ERC_REST     ERC       REST  winter      1    5     1  33845    5  2016   \n",
      "\n",
      "        Date  DOW  Weekday  \n",
      "0 2016-01-01    4     True  \n",
      "1 2016-01-02    5    False  \n",
      "2 2016-01-03    6    False  \n",
      "3 2016-01-04    0     True  \n",
      "4 2016-01-05    1     True  \n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "daydata = pd.read_csv('inputs/days_365.csv')\n",
    "#print(daydata.tail())\n",
    "daydata = daydata.drop(columns='Month')\n",
    "x2 = pd.merge(x,daydata,on=['Day'],how='left')\n",
    "#print(x2.tail())\n",
    "\n",
    "#sets the year for each region\n",
    "x2['Year']=2011\n",
    "x2.loc[x2['R_Group'] == 'ERC', 'Year'] = 2016\n",
    "#print(x2.head())\n",
    "\n",
    "#Creates a date column\n",
    "x2 = x2.rename(columns={'Day':'DOY','DayofMo':'Day'})\n",
    "x2['Date']=pd.to_datetime(x2[['Year', 'Month', 'Day']], errors='coerce')\n",
    "#print(x2.tail())\n",
    "\n",
    "#convert date to a datetime type \n",
    "#x2['Date'] = pd.to_datetime(x2['Date'])\n",
    "x2['DOW'] = x2['Date'].dt.weekday\n",
    "\n",
    "#check if it is a weekday or not \n",
    "weekday = pd.read_csv('inputs/weekday.csv')\n",
    "x2 = pd.merge(x2,weekday,on='DOW',how='left')\n",
    "print(x2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Monthly, single day type, 24 hours (288 segments)\n",
    "#### Methodology: Using groupby function to group first by month, then by 24 hours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Month  Hour  Hour_Tot  Load_Tot      Load_Avg\n",
      "0  ERC_REST      1     1        31   1042446  33627.290323\n",
      "1  ERC_REST      1     2        31   1051661  33924.548387\n",
      "2  ERC_REST      1     3        31   1079739  34830.290323\n",
      "3  ERC_REST      1     4        31   1144492  36919.096774\n",
      "4  ERC_REST      1     5        31   1239385  39980.161290\n",
      "number of segments in dataset = 288.0\n",
      "\n",
      "     R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "1545     ERC       REST  shoulder      3   86     3  26989   27  2016   \n",
      "823      ERC       REST  shoulder      4   94     2  27006    4  2016   \n",
      "1531     ERC       REST  shoulder      3   72     3  27025   13  2016   \n",
      "\n",
      "           Date  DOW  Weekday  Hour_Tot  Load_Tot      Load_Avg  \n",
      "1545 2016-03-27    6    False        31    886708  28603.483871  \n",
      "823  2016-04-04    0     True        30    870591  29019.700000  \n",
      "1531 2016-03-13    6    False        31    886708  28603.483871  \n",
      "number of rows in dataset = 17520\n"
     ]
    }
   ],
   "source": [
    "case1_x = x2.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case1 = case1_x.groupby(['Region','Month','Hour'],as_index=False).agg(aggregations)\n",
    "case1.columns = case1.columns.droplevel(0)\n",
    "case1.columns = ['Region','Month','Hour','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case1.head())\n",
    "print('number of segments in dataset =',case1.shape[0]/reg_count)\n",
    "case1.to_csv('../outputs/'+x_name+'_segments_1daytype_monthly_24hr.csv')\n",
    "print()\n",
    "\n",
    "case1_x2 = pd.merge(case1_x,case1,on=['Region','Month','Hour'],how='left')\n",
    "case1_x2 = case1_x2.sort_values(['Region',x_column]).drop(case1_x2.columns[0], axis=1)\n",
    "print(case1_x2.head(3))\n",
    "print('number of rows in dataset =',case1_x2.shape[0])\n",
    "case1_x2.to_csv('../outputs/'+x_name+'_8760_1daytype_monthly_24hr.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: Season, single day-type, 24 hours (72 segments)\n",
    "#### Methodology: Use groupby function to group by season and hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region    Season  Hour  Hour_Tot  Load_Tot      Load_Avg\n",
      "0  ERC_REST  shoulder     1       122   3635975  29803.073770\n",
      "1  ERC_REST  shoulder     2       122   3587428  29405.147541\n",
      "2  ERC_REST  shoulder     3       122   3603145  29533.975410\n",
      "3  ERC_REST  shoulder     4       122   3731428  30585.475410\n",
      "4  ERC_REST  shoulder     5       122   3992001  32721.319672\n",
      "number of segments in dataset = 72.0\n",
      "\n",
      "     R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "1545     ERC       REST  shoulder      3   86     3  26989   27  2016   \n",
      "823      ERC       REST  shoulder      4   94     2  27006    4  2016   \n",
      "1531     ERC       REST  shoulder      3   72     3  27025   13  2016   \n",
      "\n",
      "           Date  DOW  Weekday  Hour_Tot  Load_Tot      Load_Avg  \n",
      "1545 2016-03-27    6    False       122   3603145  29533.975410  \n",
      "823  2016-04-04    0     True       122   3587428  29405.147541  \n",
      "1531 2016-03-13    6    False       122   3603145  29533.975410  \n",
      "number of rows in dataset = 17520\n"
     ]
    }
   ],
   "source": [
    "case2_x = x2.copy()\n",
    "#case2_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "#case2_seasons = case2_seasons.drop(['bimonthly'], axis=1)\n",
    "#case2_seasons = case2_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "#print(case2_x.head())\n",
    "#case2_x = pd.merge(case2_x, case2_seasons, on='Month', how='left')\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case2 = case2_x.groupby(['Region','Season','Hour'],as_index=False).agg(aggregations)\n",
    "case2.columns = case2.columns.droplevel(0)\n",
    "case2.columns = ['Region','Season','Hour','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case2.head())\n",
    "print('number of segments in dataset =',case2.shape[0]/reg_count)\n",
    "case2.to_csv('../outputs/'+x_name+'_segments_1daytype_season_24hr.csv')\n",
    "print()\n",
    "\n",
    "case2_x2 = pd.merge(case2_x,case2,on=['Region','Season','Hour'],how='left')\n",
    "case2_x2 = case2_x2.sort_values(['Region',x_column]).drop(case2_x2.columns[0],axis=1)\n",
    "print(case2_x2.head(3))\n",
    "print('number of rows in dataset =',case2_x2.shape[0])\n",
    "case2_x2.to_csv('../outputs/'+x_name+'_8760_1daytype_season_24hr.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3: Monthly, weekend/weekday, 24 hours (576 segments)\n",
    "#### Metholodogy: Use groupby function to group by month, then weekend/weekday, then by 24 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Month  Hour  Hour_Tot  Load_Tot      Load_Avg\n",
      "0  ERC_REST      1     1        31   1042446  33627.290323\n",
      "1  ERC_REST      1     2        31   1051661  33924.548387\n",
      "2  ERC_REST      1     3        31   1079739  34830.290323\n",
      "3  ERC_REST      1     4        31   1144492  36919.096774\n",
      "4  ERC_REST      1     5        31   1239385  39980.161290\n",
      "number of segments in dataset = 576.0\n",
      "\n",
      "     R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "1545     ERC       REST  shoulder      3   86     3  26989   27  2016   \n",
      "823      ERC       REST  shoulder      4   94     2  27006    4  2016   \n",
      "1531     ERC       REST  shoulder      3   72     3  27025   13  2016   \n",
      "1524     ERC       REST  shoulder      3   65     3  27059    6  2016   \n",
      "794      ERC       REST  shoulder      3   65     2  27069    6  2016   \n",
      "\n",
      "           Date  DOW  Weekday  Hour_Tot  Load_Tot      Load_Avg  \n",
      "1545 2016-03-27    6    False         8    222011  27751.375000  \n",
      "823  2016-04-04    0     True        21    612083  29146.809524  \n",
      "1531 2016-03-13    6    False         8    222011  27751.375000  \n",
      "1524 2016-03-06    6    False         8    222011  27751.375000  \n",
      "794  2016-03-06    6    False         8    221751  27718.875000  \n",
      "number of rows in dataset = 17520\n"
     ]
    }
   ],
   "source": [
    "case3_x = x2.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case3 = case3_x.groupby(['Region','Month','Weekday','Hour'],as_index=False).agg(aggregations)\n",
    "case3.columns = case3.columns.droplevel(0)\n",
    "case3.columns = ['Region','Month','Weekday','Hour','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case1.head())\n",
    "print('number of segments in dataset =',case3.shape[0]/reg_count)\n",
    "case3.to_csv('../outputs/'+x_name+'_segments_2daytype_monthly_24hr.csv')\n",
    "print()\n",
    "\n",
    "case3_x2 = pd.merge(case3_x,case3,on=['Region','Month','Weekday','Hour'],how='left')\n",
    "case3_x2 = case3_x2.sort_values(['Region',x_column]).drop(case3_x2.columns[0],axis=1)\n",
    "print(case3_x2.head())\n",
    "print('number of rows in dataset =',case3_x2.shape[0])\n",
    "case3_x2.to_csv('../outputs/'+x_name+'_8760_2daytype_monthly_24hr.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 4: Season, weekend/weekday, 24 hours (144 segments)\n",
    "#### Methodology: groupby season, weekday, hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Region  Season  Weekday  Hour  Hour_Tot  Load_Tot     Load_Avg\n",
      "283  ERC_WEST  winter     True    20        63    205537  3262.492063\n",
      "284  ERC_WEST  winter     True    21        63    199198  3161.873016\n",
      "285  ERC_WEST  winter     True    22        63    191477  3039.317460\n",
      "286  ERC_WEST  winter     True    23        63    184976  2936.126984\n",
      "287  ERC_WEST  winter     True    24        63    182272  2893.206349\n",
      "number of segments in dataset = 144.0\n",
      "\n",
      "     R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "1545     ERC       REST  shoulder      3   86     3  26989   27  2016   \n",
      "823      ERC       REST  shoulder      4   94     2  27006    4  2016   \n",
      "1531     ERC       REST  shoulder      3   72     3  27025   13  2016   \n",
      "1524     ERC       REST  shoulder      3   65     3  27059    6  2016   \n",
      "794      ERC       REST  shoulder      3   65     2  27069    6  2016   \n",
      "\n",
      "           Date  DOW  Weekday  Hour_Tot  Load_Tot      Load_Avg  \n",
      "1545 2016-03-27    6    False        35   1003593  28674.085714  \n",
      "823  2016-04-04    0     True        87   2578430  29637.126437  \n",
      "1531 2016-03-13    6    False        35   1003593  28674.085714  \n",
      "1524 2016-03-06    6    False        35   1003593  28674.085714  \n",
      "794  2016-03-06    6    False        35   1008998  28828.514286  \n",
      "number of rows in dataset = 17520\n"
     ]
    }
   ],
   "source": [
    "case4_x = x2.copy()\n",
    "#case4_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "#case4_seasons = case4_seasons.drop(['bimonthly'], axis=1)\n",
    "#case4_seasons = case4_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "#case4_x = pd.merge(case4_x, case4_seasons, on='Month', how='left')\n",
    "#print(case4_x)\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case4 = case4_x.groupby(['Region','Season','Weekday','Hour'],as_index=False).agg(aggregations)\n",
    "case4.columns = case4.columns.droplevel(0)\n",
    "case4.columns = ['Region','Season','Weekday','Hour','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case4.tail())\n",
    "print('number of segments in dataset =',case4.shape[0]/reg_count)\n",
    "case4.to_csv('../outputs/'+x_name+'_segments_2daytype_season_24hr.csv')\n",
    "print()\n",
    "\n",
    "case4_x2 = pd.merge(case4_x,case4,on=['Region','Season','Weekday','Hour'],how='left')\n",
    "case4_x2 = case4_x2.sort_values(['Region',x_column]).drop(case4_x2.columns[0],axis=1)\n",
    "print(case4_x2.head())\n",
    "print('number of rows in dataset =',case4_x2.shape[0])\n",
    "case4_x2.to_csv('../outputs/'+x_name+'_8760_2daytype_season_24hr.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 hour interval day types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Region R_Group R_Subgroup  Season  Month  DOY  Hour  Load  Day  Year  \\\n",
      "17515  ERC_WEST     ERC       WEST  winter     12  361    24  2674   27  2016   \n",
      "17516  ERC_WEST     ERC       WEST  winter     12  362    24  2662   28  2016   \n",
      "17517  ERC_WEST     ERC       WEST  winter     12  363    24  3059   29  2016   \n",
      "17518  ERC_WEST     ERC       WEST  winter     12  364    24  2893   30  2016   \n",
      "17519  ERC_WEST     ERC       WEST  winter     12  365    24  2594   31  2016   \n",
      "\n",
      "            Date  DOW  Weekday  4-hr  \n",
      "17515 2016-12-27    1     True     6  \n",
      "17516 2016-12-28    2     True     6  \n",
      "17517 2016-12-29    3     True     6  \n",
      "17518 2016-12-30    4     True     6  \n",
      "17519 2016-12-31    5    False     6  \n"
     ]
    }
   ],
   "source": [
    "#read in 4 hour interval counter\n",
    "interval_4hr = pd.read_csv('inputs/interval_4hr.csv')\n",
    "#print(interval_4hr)\n",
    "\n",
    "x3 = pd.merge(x2,interval_4hr,on='Hour',how='left')\n",
    "print(x3.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 5: Monthly, single day type, 4 hour intervals (72 segments)\n",
    "#### Methodology: use groupby by month, 4 hour intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Month  4-hr  Hour_Tot  Load_Tot      Load_Avg\n",
      "0  ERC_REST      1     1       124   4318338  34825.306452\n",
      "1  ERC_REST      1     2       124   5025668  40529.580645\n",
      "2  ERC_REST      1     3       124   4711362  37994.854839\n",
      "3  ERC_REST      1     4       124   4504207  36324.250000\n",
      "4  ERC_REST      1     5       124   4847894  39095.919355\n",
      "number of segments in dataset = 72.0\n",
      "\n",
      "     R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "1545     ERC       REST  shoulder      3   86     3  26989   27  2016   \n",
      "823      ERC       REST  shoulder      4   94     2  27006    4  2016   \n",
      "1531     ERC       REST  shoulder      3   72     3  27025   13  2016   \n",
      "1524     ERC       REST  shoulder      3   65     3  27059    6  2016   \n",
      "794      ERC       REST  shoulder      3   65     2  27069    6  2016   \n",
      "\n",
      "           Date  DOW  Weekday  4-hr  Hour_Tot  Load_Tot      Load_Avg  \n",
      "1545 2016-03-27    6    False     1       124   3572927  28813.927419  \n",
      "823  2016-04-04    0     True     1       120   3539489  29495.741667  \n",
      "1531 2016-03-13    6    False     1       124   3572927  28813.927419  \n",
      "1524 2016-03-06    6    False     1       124   3572927  28813.927419  \n",
      "794  2016-03-06    6    False     1       124   3572927  28813.927419  \n",
      "number of rows in dataset = 17520\n"
     ]
    }
   ],
   "source": [
    "case5_x = x3.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case5 = case5_x.groupby(['Region','Month','4-hr'],as_index=False).agg(aggregations)\n",
    "case5.columns = case5.columns.droplevel(0)\n",
    "case5.columns = ['Region','Month','4-hr','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case5.head())\n",
    "print('number of segments in dataset =',case5.shape[0]/reg_count)\n",
    "case5.to_csv('../outputs/'+x_name+'_segments_2daytype_month_4hr.csv')\n",
    "print()\n",
    "\n",
    "case5_x2 = pd.merge(case5_x,case5,on=['Region','Month','4-hr'],how='left')\n",
    "case5_x2 = case5_x2.sort_values(['Region',x_column]).drop(case5_x2.columns[0],axis=1)\n",
    "print(case5_x2.head())\n",
    "print('number of rows in dataset =',case5_x2.shape[0])\n",
    "case5_x2.to_csv('../outputs/'+x_name+'_8760_2daytype_month_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 6: Bi-monthly weekend/weekday day-types, 4-hour intervals\n",
    "#### Methodology: use groupby function and bimonthly groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Bimonth  Weekday  4-hr  Hour_Tot  Load_Tot      Load_Avg\n",
      "0  ERC_REST        1    False     1        72   2362801  32816.680556\n",
      "1  ERC_REST        1    False     2        72   2705068  37570.388889\n",
      "2  ERC_REST        1    False     3        72   2622321  36421.125000\n",
      "3  ERC_REST        1    False     4        72   2539521  35271.125000\n",
      "4  ERC_REST        1    False     5        72   2729661  37911.958333\n",
      "number of segments in dataset = 72.0\n",
      "\n",
      "     R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "1545     ERC       REST  shoulder      3   86     3  26989   27  2016   \n",
      "823      ERC       REST  shoulder      4   94     2  27006    4  2016   \n",
      "1531     ERC       REST  shoulder      3   72     3  27025   13  2016   \n",
      "1524     ERC       REST  shoulder      3   65     3  27059    6  2016   \n",
      "794      ERC       REST  shoulder      3   65     2  27069    6  2016   \n",
      "\n",
      "           Date  DOW  Weekday  4-hr  Bimonth  Hour_Tot  Load_Tot      Load_Avg  \n",
      "1545 2016-03-27    6    False     1        2        68   1932774  28423.147059  \n",
      "823  2016-04-04    0     True     1        2       176   5179642  29429.784091  \n",
      "1531 2016-03-13    6    False     1        2        68   1932774  28423.147059  \n",
      "1524 2016-03-06    6    False     1        2        68   1932774  28423.147059  \n",
      "794  2016-03-06    6    False     1        2        68   1932774  28423.147059  \n",
      "number of rows in dataset = 17520\n"
     ]
    }
   ],
   "source": [
    "case6_x = x3.copy()\n",
    "case6_bimonth = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case6_bimonth = case6_bimonth.drop(['seasonal'], axis=1)\n",
    "case6_bimonth = case6_bimonth.rename(columns={'bimonthly':'Bimonth','month':'Month'})\n",
    "\n",
    "case6_x = pd.merge(case6_x, case6_bimonth, on='Month', how='left')\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case6 = case6_x.groupby(['Region','Bimonth','Weekday','4-hr'],as_index=False).agg(aggregations)\n",
    "case6.columns = case6.columns.droplevel(0)\n",
    "case6.columns = ['Region','Bimonth','Weekday','4-hr','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case6.head())\n",
    "print('number of segments in dataset =',case6.shape[0]/reg_count)\n",
    "case6.to_csv('../outputs/'+x_name+'_segments_2daytype_bimonth_4hr.csv')\n",
    "print()\n",
    "\n",
    "case6_x2 = pd.merge(case6_x,case6,on=['Region','Bimonth','Weekday','4-hr'],how='left')\n",
    "case6_x2 = case6_x2.sort_values(['Region',x_column]).drop(case6_x2.columns[0],axis=1)\n",
    "print(case6_x2.head())\n",
    "print('number of rows in dataset =',case6_x2.shape[0])\n",
    "case6_x2.to_csv('../outputs/'+x_name+'_8760_2daytype_bimonth_4hr.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 7: Season-based months, weekend/weekday day-types, 4-hour intervals\n",
    "#### Methodology: groupby function and applied season groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Season_Group  Weekday  4-hr  Hour_Tot  Load_Tot      Load_Avg\n",
      "0  ERC_REST             1    False     1       108   3533655  32719.027778\n",
      "1  ERC_REST             1    False     2       108   3948984  36564.666667\n",
      "2  ERC_REST             1    False     3       108   3994962  36990.388889\n",
      "3  ERC_REST             1    False     4       108   3897223  36085.398148\n",
      "4  ERC_REST             1    False     5       108   4166114  38575.129630\n",
      "number of segments in dataset = 60.0\n",
      "\n",
      "     R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "1545     ERC       REST  shoulder      3   86     3  26989   27  2016   \n",
      "823      ERC       REST  shoulder      4   94     2  27006    4  2016   \n",
      "1531     ERC       REST  shoulder      3   72     3  27025   13  2016   \n",
      "1524     ERC       REST  shoulder      3   65     3  27059    6  2016   \n",
      "794      ERC       REST  shoulder      3   65     2  27069    6  2016   \n",
      "\n",
      "           Date  DOW  Weekday  4-hr  Season_Group  Hour_Tot  Load_Tot  \\\n",
      "1545 2016-03-27    6    False     1             2       140   4051780   \n",
      "823  2016-04-04    0     True     1             2       348  10506196   \n",
      "1531 2016-03-13    6    False     1             2       140   4051780   \n",
      "1524 2016-03-06    6    False     1             2       140   4051780   \n",
      "794  2016-03-06    6    False     1             2       140   4051780   \n",
      "\n",
      "          Load_Avg  \n",
      "1545  28941.285714  \n",
      "823   30190.218391  \n",
      "1531  28941.285714  \n",
      "1524  28941.285714  \n",
      "794   28941.285714  \n",
      "number of rows in dataset = 17520\n"
     ]
    }
   ],
   "source": [
    "case7_x = x3.copy()\n",
    "case7_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case7_seasons = case7_seasons.drop(['bimonthly'], axis=1)\n",
    "case7_seasons = case7_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "\n",
    "case7_x = pd.merge(case7_x, case7_seasons, on='Month', how='left')\n",
    "#print(case4_load)\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case7 = case7_x.groupby(['Region','Season_Group','Weekday','4-hr'],as_index=False).agg(aggregations)\n",
    "case7.columns = case7.columns.droplevel(0)\n",
    "case7.columns = ['Region','Season_Group','Weekday','4-hr','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case7.head())\n",
    "print('number of segments in dataset =',case7.shape[0]/reg_count)\n",
    "case7.to_csv('../outputs/'+x_name+'_segments_2daytype_seasonbased_4hr.csv')\n",
    "print()\n",
    "\n",
    "case7_x2 = pd.merge(case7_x,case7,on=['Region','Season_Group','Weekday','4-hr'],how='left')\n",
    "case7_x2 = case7_x2.sort_values(['Region',x_column]).drop(case7_x2.columns[0],axis=1)\n",
    "print(case7_x2.head())\n",
    "print('number of rows in dataset =',case7_x2.shape[0])\n",
    "case7_x2.to_csv('../outputs/'+x_name+'_8760_2daytype_seasonbased_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 8: Seasons, weekend/weekday day-types, 4-hour intervals\n",
    "#### Methodology: groupby function and applied season groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region    Season  Weekday  4-hr  Hour_Tot  Load_Tot      Load_Avg\n",
      "0  ERC_REST  shoulder    False     1       140   4051780  28941.285714\n",
      "1  ERC_REST  shoulder    False     2       140   4378835  31277.392857\n",
      "2  ERC_REST  shoulder    False     3       140   4993309  35666.492857\n",
      "3  ERC_REST  shoulder    False     4       140   5296142  37829.585714\n",
      "4  ERC_REST  shoulder    False     5       140   5276131  37686.650000\n",
      "number of segments in dataset = 36.0\n",
      "\n",
      "     R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "1545     ERC       REST  shoulder      3   86     3  26989   27  2016   \n",
      "823      ERC       REST  shoulder      4   94     2  27006    4  2016   \n",
      "1531     ERC       REST  shoulder      3   72     3  27025   13  2016   \n",
      "1524     ERC       REST  shoulder      3   65     3  27059    6  2016   \n",
      "794      ERC       REST  shoulder      3   65     2  27069    6  2016   \n",
      "\n",
      "           Date  DOW  Weekday  4-hr  Hour_Tot  Load_Tot      Load_Avg  \n",
      "1545 2016-03-27    6    False     1       140   4051780  28941.285714  \n",
      "823  2016-04-04    0     True     1       348  10506196  30190.218391  \n",
      "1531 2016-03-13    6    False     1       140   4051780  28941.285714  \n",
      "1524 2016-03-06    6    False     1       140   4051780  28941.285714  \n",
      "794  2016-03-06    6    False     1       140   4051780  28941.285714  \n",
      "number of rows in dataset = 17520\n"
     ]
    }
   ],
   "source": [
    "case8_x = x3.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case8 = case8_x.groupby(['Region','Season','Weekday','4-hr'],as_index=False).agg(aggregations)\n",
    "case8.columns = case8.columns.droplevel(0)\n",
    "case8.columns = ['Region','Season','Weekday','4-hr','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case8.head())\n",
    "print('number of segments in dataset =',case8.shape[0]/reg_count)\n",
    "case8.to_csv('../outputs/'+x_name+'_segments_2daytype_seasonbased_4hr.csv')\n",
    "print()\n",
    "\n",
    "case8_x2 = pd.merge(case8_x,case8,on=['Region','Season','Weekday','4-hr'],how='left')\n",
    "case8_x2 = case8_x2.sort_values(['Region',x_column]).drop(case8_x2.columns[0],axis=1)\n",
    "print(case8_x2.head())\n",
    "print('number of rows in dataset =',case8_x2.shape[0])\n",
    "case8_x2.to_csv('../outputs/'+x_name+'_8760_2daytype_seasonbased_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day type: Three (Weekday, Weekend, Peak Load)\n",
    "## Find Peak Load Days in Each Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows in regional data = 365.0\n",
      "      Region  Month  Load_MD_Max\n",
      "0   ERC_REST      1      1000605\n",
      "1   ERC_REST      2       932895\n",
      "2   ERC_REST      3       886217\n",
      "3   ERC_REST      4       983152\n",
      "4   ERC_REST      5      1083459\n",
      "5   ERC_REST      6      1211630\n",
      "6   ERC_REST      7      1267082\n",
      "7   ERC_REST      8      1332321\n",
      "8   ERC_REST      9      1231350\n",
      "9   ERC_REST     10      1109240\n",
      "10  ERC_REST     11      1004548\n",
      "11  ERC_REST     12      1176963\n",
      "\n",
      "     Region R_Group R_Subgroup  Season  Month  DOY  Hour  Load_MD_Tot  \\\n",
      "0  ERC_REST     ERC       REST  winter      1    1     1       917588   \n",
      "1  ERC_REST     ERC       REST  winter      1    2     1       863132   \n",
      "2  ERC_REST     ERC       REST  winter      1    3     1       934513   \n",
      "3  ERC_REST     ERC       REST  winter      1    4     1       960576   \n",
      "4  ERC_REST     ERC       REST  winter      1    5     1       916216   \n",
      "\n",
      "   Load_MD_Max  \n",
      "0      1000605  \n",
      "1      1000605  \n",
      "2      1000605  \n",
      "3      1000605  \n",
      "4      1000605  \n"
     ]
    }
   ],
   "source": [
    "#create temporary DF to find peak\n",
    "load = load_dur.copy()\n",
    "\n",
    "#groupby region, month, and day to sum the total day\n",
    "aggregations1 = {'Load':sum}\n",
    "md_sum = load.groupby(['Region','Month','Day'],as_index=False).agg(aggregations1)\n",
    "md_sum.columns = ['Region','Month','Day','Load_MD_Tot']\n",
    "#print(md_sum.tail())\n",
    "print('number of rows in regional data =',md_sum.shape[0]/reg_count)\n",
    "\n",
    "md_sum2 = pd.merge(load,md_sum,on=['Region','Month','Day'],how='left')\n",
    "#print(md_sum2.tail())\n",
    "\n",
    "#groupby region and month to find maximum \n",
    "aggregations2 = {'Load_MD_Tot':max}\n",
    "md_max = md_sum2.groupby(['Region','Month'],as_index=False).agg(aggregations2)\n",
    "md_max.columns = ['Region','Month','Load_MD_Max']\n",
    "print(md_max[0:12])\n",
    "print()\n",
    "\n",
    "peakd = pd.merge(md_sum2,md_max,on=['Region','Month'],how='left')\n",
    "peakd = peakd.drop(columns=['Unnamed: 0','Load'])\n",
    "peakd = peakd.rename(columns={'Day':'DOY'})\n",
    "print(peakd.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Region R_Group R_Subgroup  Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "0   ERC_REST     ERC       REST  winter      1    1     1  34807    1  2016   \n",
      "1   ERC_REST     ERC       REST  winter      1    2     1  34716    2  2016   \n",
      "2   ERC_REST     ERC       REST  winter      1    3     1  34736    3  2016   \n",
      "3   ERC_REST     ERC       REST  winter      1    4     1  35914    4  2016   \n",
      "4   ERC_REST     ERC       REST  winter      1    5     1  33845    5  2016   \n",
      "5   ERC_REST     ERC       REST  winter      1    6     1  30995    6  2016   \n",
      "6   ERC_REST     ERC       REST  winter      1    7     1  30186    7  2016   \n",
      "7   ERC_REST     ERC       REST  winter      1    8     1  30745    8  2016   \n",
      "8   ERC_REST     ERC       REST  winter      1    9     1  38702    9  2016   \n",
      "9   ERC_REST     ERC       REST  winter      1   10     1  38567   10  2016   \n",
      "10  ERC_REST     ERC       REST  winter      1   11     1  37495   11  2016   \n",
      "11  ERC_REST     ERC       REST  winter      1   12     1  34274   12  2016   \n",
      "12  ERC_REST     ERC       REST  winter      1   13     1  31815   13  2016   \n",
      "13  ERC_REST     ERC       REST  winter      1   14     1  29614   14  2016   \n",
      "14  ERC_REST     ERC       REST  winter      1   15     1  31453   15  2016   \n",
      "\n",
      "         Date  DOW Day_Type  \n",
      "0  2016-01-01    4  Weekday  \n",
      "1  2016-01-02    5  Weekend  \n",
      "2  2016-01-03    6  Weekend  \n",
      "3  2016-01-04    0  Weekday  \n",
      "4  2016-01-05    1  Weekday  \n",
      "5  2016-01-06    2  Weekday  \n",
      "6  2016-01-07    3  Weekday  \n",
      "7  2016-01-08    4  Weekday  \n",
      "8  2016-01-09    5  Weekend  \n",
      "9  2016-01-10    6     Peak  \n",
      "10 2016-01-11    0  Weekday  \n",
      "11 2016-01-12    1  Weekday  \n",
      "12 2016-01-13    2  Weekday  \n",
      "13 2016-01-14    3  Weekday  \n",
      "14 2016-01-15    4  Weekday  \n"
     ]
    }
   ],
   "source": [
    "x_peak = pd.merge(x2,peakd,on=['Region','R_Group','R_Subgroup','Season','Month','DOY','Hour'],how='left')\n",
    "x_peak = x_peak.rename(columns={'Weekday':'Day_Type'})\n",
    "#print(x_peak.tail())\n",
    "\n",
    "#Return True if the load total equals the day identified as the max\n",
    "x_peak.loc[x_peak['Day_Type'] == True, 'Day_Type'] = 'Weekday'\n",
    "x_peak.loc[x_peak['Day_Type'] == False, 'Day_Type'] = 'Weekend'\n",
    "x_peak.loc[x_peak['Load_MD_Tot'] == x_peak['Load_MD_Max'], 'Day_Type'] = 'Peak'\n",
    "x_peak = x_peak.drop(['Load_MD_Tot','Load_MD_Max'], axis=1)\n",
    "print(x_peak[0:15])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Monthly, Weekend/weekday/peak day-types, 24 hours (864 segments)\n",
    "#### Methodology: similar to two day type, just adding in peak day types to sort by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Month Day_Type  Hour  Hour_Tot  Load_Tot  Load_Avg\n",
      "0  ERC_REST      1     Peak     1         1     38567   38567.0\n",
      "1  ERC_REST      1     Peak     2         1     39107   39107.0\n",
      "2  ERC_REST      1     Peak     3         1     40311   40311.0\n",
      "3  ERC_REST      1     Peak     4         1     43115   43115.0\n",
      "4  ERC_REST      1     Peak     5         1     47186   47186.0\n",
      "number of segments in dataset = 864.0\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   27   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006    4   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   13   \n",
      "\n",
      "      Year       Date  DOW Day_Type  Hour_Tot  Load_Tot   Load_Avg  \n",
      "1545  2016 2016-03-27    6  Weekend         8    222011  27751.375  \n",
      "823   2016 2016-04-04    0  Weekday        20    579213  28960.650  \n",
      "1531  2016 2016-03-13    6  Weekend         8    222011  27751.375  \n",
      "number of rows in dataset = 17520\n"
     ]
    }
   ],
   "source": [
    "case1_x = x_peak.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case1 = case1_x.groupby(['Region','Month','Day_Type','Hour'],as_index=False).agg(aggregations)\n",
    "case1.columns = case1.columns.droplevel(0)\n",
    "case1.columns = ['Region','Month','Day_Type','Hour','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case1.head())\n",
    "print('number of segments in dataset =',case1.shape[0]/reg_count)\n",
    "case1.to_csv('../outputs/'+x_name+'_segments_3daytype_monthly_24hr.csv')\n",
    "print()\n",
    "\n",
    "case1_x2 = pd.merge(case1_x,case1,on=['Region','Month','Day_Type','Hour'],how='left')\n",
    "case1_x2 = case1_x2.sort_values(['Region',x_column])\n",
    "print(case1_x2.head(3))\n",
    "print('number of rows in dataset =',case1_x2.shape[0])\n",
    "case1_x2.to_csv('../outputs/'+x_name+'_8760_3daytype_monthly_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: Season, weekend/weekday/peak day-types, 24-hours (216 segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region    Season Day_Type  Hour  Hour_Tot  Load_Tot  Load_Avg\n",
      "0  ERC_REST  shoulder     Peak     1         4    132690  33172.50\n",
      "1  ERC_REST  shoulder     Peak     2         4    130802  32700.50\n",
      "2  ERC_REST  shoulder     Peak     3         4    131751  32937.75\n",
      "3  ERC_REST  shoulder     Peak     4         4    137910  34477.50\n",
      "4  ERC_REST  shoulder     Peak     5         4    150015  37503.75\n",
      "number of segments in dataset = 216.0\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   27   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006    4   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   13   \n",
      "\n",
      "      Year       Date  DOW Day_Type  Hour_Tot  Load_Tot   Load_Avg  \n",
      "1545  2016 2016-03-27    6  Weekend         8    222011  27751.375  \n",
      "823   2016 2016-04-04    0  Weekday        20    579213  28960.650  \n",
      "1531  2016 2016-03-13    6  Weekend         8    222011  27751.375  \n",
      "number of rows in dataset = 17520\n"
     ]
    }
   ],
   "source": [
    "case2_x = x_peak.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case2 = case2_x.groupby(['Region','Season','Day_Type','Hour'],as_index=False).agg(aggregations)\n",
    "case2.columns = case2.columns.droplevel(0)\n",
    "case2.columns = ['Region','Season','Day_Type','Hour','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case2.head())\n",
    "print('number of segments in dataset =',case2.shape[0]/reg_count)\n",
    "case2.to_csv('../outputs/'+x_name+'_segments_3daytype_season_24hr.csv')\n",
    "print()\n",
    "\n",
    "case2_x2 = pd.merge(case2_x,case2,on=['Region','Season','Day_Type','Hour'],how='left')\n",
    "case2_x2 = case2_x2.sort_values(['Region',x_column])\n",
    "print(case1_x2.head(3))\n",
    "print('number of rows in dataset =',case1_x2.shape[0])\n",
    "case2_x2.to_csv('../outputs/'+x_name+'_8760_3daytype_season_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3: Annual, weekend/weekday/peak day-types, 24-hours (72 segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region Day_Type  Hour  Hour_Tot  Load_Tot      Load_Avg\n",
      "0  ERC_REST     Peak     1        12    455893  37991.083333\n",
      "1  ERC_REST     Peak     2        12    450804  37567.000000\n",
      "2  ERC_REST     Peak     3        12    454384  37865.333333\n",
      "3  ERC_REST     Peak     4        12    473371  39447.583333\n",
      "4  ERC_REST     Peak     5        12    505999  42166.583333\n",
      "number of segments in dataset = 72.0\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   27   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006    4   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   13   \n",
      "\n",
      "      Year       Date  DOW Day_Type  Hour_Tot  Load_Tot      Load_Avg  \n",
      "1545  2016 2016-03-27    6  Weekend       104   3359100  32299.038462  \n",
      "823   2016 2016-04-04    0  Weekday       249   8176806  32838.578313  \n",
      "1531  2016 2016-03-13    6  Weekend       104   3359100  32299.038462  \n",
      "number of rows in dataset = 17520\n"
     ]
    }
   ],
   "source": [
    "case3_x = x_peak.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case3 = case3_x.groupby(['Region','Day_Type','Hour'],as_index=False).agg(aggregations)\n",
    "case3.columns = case3.columns.droplevel(0)\n",
    "case3.columns = ['Region','Day_Type','Hour','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case3.head())\n",
    "print('number of segments in dataset =',case3.shape[0]/reg_count)\n",
    "case3.to_csv('../outputs/'+x_name+'_segments_3daytype_annual_24hr.csv')\n",
    "print()\n",
    "\n",
    "case3_x2 = pd.merge(case3_x,case3,on=['Region','Day_Type','Hour'],how='left')\n",
    "case3_x2 = case3_x2.sort_values(['Region',x_column])\n",
    "print(case3_x2.head(3))\n",
    "print('number of rows in dataset =',case3_x2.shape[0])\n",
    "case3_x2.to_csv('../outputs/'+x_name+'_8760_3daytype_annual_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 hour interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Region R_Group R_Subgroup  Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "0   ERC_REST     ERC       REST  winter      1    1     1  34807    1  2016   \n",
      "1   ERC_REST     ERC       REST  winter      1    2     1  34716    2  2016   \n",
      "2   ERC_REST     ERC       REST  winter      1    3     1  34736    3  2016   \n",
      "3   ERC_REST     ERC       REST  winter      1    4     1  35914    4  2016   \n",
      "4   ERC_REST     ERC       REST  winter      1    5     1  33845    5  2016   \n",
      "5   ERC_REST     ERC       REST  winter      1    6     1  30995    6  2016   \n",
      "6   ERC_REST     ERC       REST  winter      1    7     1  30186    7  2016   \n",
      "7   ERC_REST     ERC       REST  winter      1    8     1  30745    8  2016   \n",
      "8   ERC_REST     ERC       REST  winter      1    9     1  38702    9  2016   \n",
      "9   ERC_REST     ERC       REST  winter      1   10     1  38567   10  2016   \n",
      "10  ERC_REST     ERC       REST  winter      1   11     1  37495   11  2016   \n",
      "11  ERC_REST     ERC       REST  winter      1   12     1  34274   12  2016   \n",
      "12  ERC_REST     ERC       REST  winter      1   13     1  31815   13  2016   \n",
      "13  ERC_REST     ERC       REST  winter      1   14     1  29614   14  2016   \n",
      "14  ERC_REST     ERC       REST  winter      1   15     1  31453   15  2016   \n",
      "\n",
      "         Date  DOW Day_Type  4-hr  \n",
      "0  2016-01-01    4  Weekday     1  \n",
      "1  2016-01-02    5  Weekend     1  \n",
      "2  2016-01-03    6  Weekend     1  \n",
      "3  2016-01-04    0  Weekday     1  \n",
      "4  2016-01-05    1  Weekday     1  \n",
      "5  2016-01-06    2  Weekday     1  \n",
      "6  2016-01-07    3  Weekday     1  \n",
      "7  2016-01-08    4  Weekday     1  \n",
      "8  2016-01-09    5  Weekend     1  \n",
      "9  2016-01-10    6     Peak     1  \n",
      "10 2016-01-11    0  Weekday     1  \n",
      "11 2016-01-12    1  Weekday     1  \n",
      "12 2016-01-13    2  Weekday     1  \n",
      "13 2016-01-14    3  Weekday     1  \n",
      "14 2016-01-15    4  Weekday     1  \n"
     ]
    }
   ],
   "source": [
    "#read in 4 hour interval counter\n",
    "#interval_4hr = pd.read_csv('inputs/interval_4hr.csv')\n",
    "#print(interval_4hr.head())\n",
    "#print(x_peak.head())\n",
    "\n",
    "x_peak2 = pd.merge(x_peak,interval_4hr,on='Hour',how='left')\n",
    "print(x_peak2.head(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 4: Bi-monthly, weekend/weekday/peak day-types, 4-hour intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Bimonth Day_Type  4-hr  Hour_Tot  Load_Tot   Load_Avg\n",
      "0  ERC_REST        1     Peak     1         8    312443  39055.375\n",
      "1  ERC_REST        1     Peak     2         8    366535  45816.875\n",
      "2  ERC_REST        1     Peak     3         8    322104  40263.000\n",
      "3  ERC_REST        1     Peak     4         8    297348  37168.500\n",
      "4  ERC_REST        1     Peak     5         8    332533  41566.625\n",
      "number of segments in dataset = 108.0\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   27   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006    4   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   13   \n",
      "1524  ERC_REST     ERC       REST  shoulder      3   65     3  27059    6   \n",
      "794   ERC_REST     ERC       REST  shoulder      3   65     2  27069    6   \n",
      "\n",
      "      Year       Date  DOW Day_Type  4-hr  Bimonth  Hour_Tot  Load_Tot  \\\n",
      "1545  2016 2016-03-27    6  Weekend     1        2        68   1932774   \n",
      "823   2016 2016-04-04    0  Weekday     1        2       168   4924979   \n",
      "1531  2016 2016-03-13    6  Weekend     1        2        68   1932774   \n",
      "1524  2016 2016-03-06    6  Weekend     1        2        68   1932774   \n",
      "794   2016 2016-03-06    6  Weekend     1        2        68   1932774   \n",
      "\n",
      "          Load_Avg  \n",
      "1545  28423.147059  \n",
      "823   29315.351190  \n",
      "1531  28423.147059  \n",
      "1524  28423.147059  \n",
      "794   28423.147059  \n",
      "number of rows in dataset = 17520\n"
     ]
    }
   ],
   "source": [
    "case4_x = x_peak2.copy()\n",
    "case4_bimonth = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case4_bimonth = case4_bimonth.drop(['seasonal'], axis=1)\n",
    "case4_bimonth = case4_bimonth.rename(columns={'bimonthly':'Bimonth','month':'Month'})\n",
    "\n",
    "case4_x = pd.merge(case4_x, case4_bimonth, on='Month', how='left')\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case4 = case4_x.groupby(['Region','Bimonth','Day_Type','4-hr'],as_index=False).agg(aggregations)\n",
    "case4.columns = case4.columns.droplevel(0)\n",
    "case4.columns = ['Region','Bimonth','Day_Type','4-hr','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case4.head())\n",
    "print('number of segments in dataset =',case4.shape[0]/reg_count)\n",
    "case4.to_csv('../outputs/'+x_name+'_segments_3daytype_bimonth_4hr.csv')\n",
    "print()\n",
    "\n",
    "case4_x2 = pd.merge(case4_x,case4,on=['Region','Bimonth','Day_Type','4-hr'],how='left')\n",
    "case4_x2 = case4_x2.sort_values(['Region',x_column])\n",
    "print(case4_x2.head())\n",
    "print('number of rows in dataset =',case4_x2.shape[0])\n",
    "case4_x2.to_csv('../outputs/'+x_name+'_8760_3daytype_bimonth_4hr.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 5: Season-based months, weekend/weekday/peak day-types, 4-hour intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Season_Group Day_Type  4-hr  Hour_Tot  Load_Tot      Load_Avg\n",
      "0  ERC_REST             1     Peak     1        12    504276  42023.000000\n",
      "1  ERC_REST             1     Peak     2        12    583360  48613.333333\n",
      "2  ERC_REST             1     Peak     3        12    527061  43921.750000\n",
      "3  ERC_REST             1     Peak     4        12    478413  39867.750000\n",
      "4  ERC_REST             1     Peak     5        12    530620  44218.333333\n",
      "number of segments in dataset = 90.0\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   27   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006    4   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   13   \n",
      "1524  ERC_REST     ERC       REST  shoulder      3   65     3  27059    6   \n",
      "794   ERC_REST     ERC       REST  shoulder      3   65     2  27069    6   \n",
      "\n",
      "      Year       Date  DOW Day_Type  4-hr  Season_Group  Hour_Tot  Load_Tot  \\\n",
      "1545  2016 2016-03-27    6  Weekend     1             2       140   4051780   \n",
      "823   2016 2016-04-04    0  Weekday     1             2       332   9973043   \n",
      "1531  2016 2016-03-13    6  Weekend     1             2       140   4051780   \n",
      "1524  2016 2016-03-06    6  Weekend     1             2       140   4051780   \n",
      "794   2016 2016-03-06    6  Weekend     1             2       140   4051780   \n",
      "\n",
      "          Load_Avg  \n",
      "1545  28941.285714  \n",
      "823   30039.286145  \n",
      "1531  28941.285714  \n",
      "1524  28941.285714  \n",
      "794   28941.285714  \n",
      "number of rows in dataset = 17520\n"
     ]
    }
   ],
   "source": [
    "case5_x = x_peak2.copy()\n",
    "case5_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case5_seasons = case5_seasons.drop(['bimonthly'], axis=1)\n",
    "case5_seasons = case5_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "\n",
    "case5_x = pd.merge(case5_x, case5_seasons, on='Month', how='left')\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case5 = case5_x.groupby(['Region','Season_Group','Day_Type','4-hr'],as_index=False).agg(aggregations)\n",
    "case5.columns = case5.columns.droplevel(0)\n",
    "case5.columns = ['Region','Season_Group','Day_Type','4-hr','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case5.head())\n",
    "print('number of segments in dataset =',case5.shape[0]/reg_count)\n",
    "case5.to_csv('../outputs/'+x_name+'_segments_3daytype_seasonbased_4hr.csv')\n",
    "print()\n",
    "\n",
    "case5_x2 = pd.merge(case5_x,case5,on=['Region','Season_Group','Day_Type','4-hr'],how='left')\n",
    "case5_x2 = case5_x2.sort_values(['Region',x_column])\n",
    "print(case5_x2.head())\n",
    "print('number of rows in dataset =',case5_x2.shape[0])\n",
    "case5_x2.to_csv('../outputs/'+x_name+'_8760_3daytype_seasonbased_4hr.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 6: Season, weekend/weekday/peak day-types, 4-hour intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region    Season Day_Type  4-hr  Hour_Tot  Load_Tot    Load_Avg\n",
      "0  ERC_REST  shoulder     Peak     1        16    533153  33322.0625\n",
      "1  ERC_REST  shoulder     Peak     2        16    619476  38717.2500\n",
      "2  ERC_REST  shoulder     Peak     3        16    707544  44221.5000\n",
      "3  ERC_REST  shoulder     Peak     4        16    785332  49083.2500\n",
      "4  ERC_REST  shoulder     Peak     5        16    743101  46443.8125\n",
      "number of segments in dataset = 54.0\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   27   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006    4   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   13   \n",
      "1524  ERC_REST     ERC       REST  shoulder      3   65     3  27059    6   \n",
      "794   ERC_REST     ERC       REST  shoulder      3   65     2  27069    6   \n",
      "\n",
      "      Year       Date  DOW Day_Type  4-hr  Hour_Tot  Load_Tot      Load_Avg  \n",
      "1545  2016 2016-03-27    6  Weekend     1       140   4051780  28941.285714  \n",
      "823   2016 2016-04-04    0  Weekday     1       332   9973043  30039.286145  \n",
      "1531  2016 2016-03-13    6  Weekend     1       140   4051780  28941.285714  \n",
      "1524  2016 2016-03-06    6  Weekend     1       140   4051780  28941.285714  \n",
      "794   2016 2016-03-06    6  Weekend     1       140   4051780  28941.285714  \n",
      "number of rows in dataset = 17520\n"
     ]
    }
   ],
   "source": [
    "case6_x = x_peak2.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case6 = case6_x.groupby(['Region','Season','Day_Type','4-hr'],as_index=False).agg(aggregations)\n",
    "case6.columns = case6.columns.droplevel(0)\n",
    "case6.columns = ['Region','Season','Day_Type','4-hr','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case6.head())\n",
    "print('number of segments in dataset =',case6.shape[0]/reg_count)\n",
    "case6.to_csv('../outputs/'+x_name+'_segments_3daytype_season_4hr.csv')\n",
    "print()\n",
    "\n",
    "case6_x2 = pd.merge(case6_x,case6,on=['Region','Season','Day_Type','4-hr'],how='left')\n",
    "case6_x2 = case6_x2.sort_values(['Region',x_column])\n",
    "print(case6_x2.head())\n",
    "print('number of rows in dataset =',case6_x2.shape[0])\n",
    "case6_x2.to_csv('../outputs/'+x_name+'_8760_3daytype_season_4hr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
