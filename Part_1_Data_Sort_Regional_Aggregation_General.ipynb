{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case: Grouping hours by interconnect\n",
    "### Details: Group hours first by interconnect load in the same 6 load groups as used before, then split into regions and split by 3 seasons and 4 times of day\\\n",
    "#### Methodology: Use similar code as the one used to organize groups by season, but for interconnects, and then split into and sort by region, season, and time of day. \n",
    "### 1: Group by interconnect and 6 load groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output files are written out in parent directory: C:\\Users\\tgoforth\\Documents\\IPM temporal resolution project\\outputs\n",
      "output files are written out in parent directory: C:\\Users\\tgoforth\\Documents\\IPM temporal resolution project\\outputs/solar\n"
     ]
    }
   ],
   "source": [
    "#importing packages needed for analysis\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from pandas import DataFrame\n",
    "\n",
    "path = os.getcwd()\n",
    "#print(path)\n",
    "\n",
    "load_dur = pd.read_csv('../outputs/load_long_format.csv')\n",
    "solar_dur = pd.read_csv('../outputs/solar_long_format.csv')\n",
    "wind_dur = pd.read_csv('../outputs/wind_long_format.csv')\n",
    "\n",
    "## UNCOMMENT WHICH PROFILE TO BE USED\n",
    "#x = load_dur\n",
    "#x_name = 'load'\n",
    "#x_name2 = 'Load'\n",
    "#x_column = 'Load'\n",
    "\n",
    "x = solar_dur\n",
    "x_name = 'solar'\n",
    "x_name2 = 'Solar_Gen'\n",
    "#choose TRG \n",
    "x_column = 'TRG6'\n",
    "\n",
    "#x = wind_dur\n",
    "#x_name = 'wind'\n",
    "#x_name2 = 'Wind_Gen'\n",
    "#x_column = 'TRG4'\n",
    "\n",
    "#this code creates an output directory in the parent director, if one does not exist yet\n",
    "#Note: this is where all of the output files will be written, since outputs are large this saves space in git\n",
    "path = os.getcwd()\n",
    "parent = os.path.dirname(path)\n",
    "outputs_dir = parent+'\\outputs'\n",
    "if not os.path.exists(outputs_dir):\n",
    "    os.makedirs(outputs_dir)\n",
    "print('output files are written out in parent directory: '+outputs_dir)\n",
    "\n",
    "outputs_x = outputs_dir+'/'+x_name\n",
    "if not os.path.exists(outputs_x):\n",
    "    os.makedirs(outputs_x)\n",
    "print('output files are written out in parent directory: '+outputs_x)\n",
    "\n",
    "x = x[['Region','R_Group','R_Subgroup','Season','Month','Day','Hour',x_column]].copy()\n",
    "years = pd.read_csv('inputs/years.csv').dropna()\n",
    "\n",
    "#print(x)\n",
    "\n",
    "#add an hour counter\n",
    "x['Hour_Counter'] = (x['Hour']) + (x['Day'] - 1) * 24\n",
    "x = x.sort_values(by=['Region','Hour_Counter'])\n",
    "unique_hc = pd.Series(x['Hour_Counter'].unique()).dropna()\n",
    "#print(unique_hc.tail(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Region R_Group R_Subgroup    Season  Month  Day  Hour   TRG6  \\\n",
      "0       ERC_PHDL     ERC       PHDL  shoulder      4   95    11  829.0   \n",
      "1       ERC_PHDL     ERC       PHDL  shoulder      4   92    11  819.0   \n",
      "2       ERC_PHDL     ERC       PHDL  shoulder      4   95    12  816.0   \n",
      "3       ERC_PHDL     ERC       PHDL  shoulder      4   96    11  809.0   \n",
      "4       ERC_PHDL     ERC       PHDL  shoulder      4   95    16  807.0   \n",
      "...          ...     ...        ...       ...    ...  ...   ...    ...   \n",
      "683275   WECC_WY     WEC         WY    winter     12  365    23    NaN   \n",
      "683276   WECC_WY     WEC         WY    winter     12  365    23    NaN   \n",
      "683277   WECC_WY     WEC         WY    winter     12  365    24    NaN   \n",
      "683278   WECC_WY     WEC         WY    winter     12  365    24    NaN   \n",
      "683279   WECC_WY     WEC         WY    winter     12  365    24    NaN   \n",
      "\n",
      "        Hour_Counter      TOD Interconnect  Group  \n",
      "0               2267  middday          ERC      1  \n",
      "1               2195  middday          ERC      1  \n",
      "2               2268  middday          ERC      1  \n",
      "3               2291  middday          ERC      1  \n",
      "4               2272  middday          ERC      1  \n",
      "...              ...      ...          ...    ...  \n",
      "683275          8759    night          WEC      6  \n",
      "683276          8759    night          WEC      6  \n",
      "683277          8760    night          WEC      6  \n",
      "683278          8760    night          WEC      6  \n",
      "683279          8760    night          WEC      6  \n",
      "\n",
      "[683280 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "#first, split by interconnect and 6 load groups\n",
    "inter_x = x.copy()\n",
    "tod = pd.read_csv('inputs/time_of_day.csv')\n",
    "\n",
    "inter_x = pd.merge(inter_x,tod,on='Hour',how='left')\n",
    "\n",
    "#identifying different interconnects\n",
    "inter_x['Interconnect'] = 'Other'\n",
    "inter_x.loc[inter_x['R_Group'] == 'WEC', 'Interconnect'] = 'WEC'\n",
    "inter_x.loc[inter_x['R_Group'] == 'ERC', 'Interconnect'] = 'ERC'\n",
    "      \n",
    "#Create a list of interconnects of all regions (551880 rows)\n",
    "interconnects = inter_x.copy()\n",
    "interconnects = interconnects[['Interconnect','Hour_Counter']]\n",
    "#print(interconnects.head())\n",
    "#print(interconnects.shape)\n",
    "\n",
    "#get the number of hours in each interconnect\n",
    "inter_count = interconnects.groupby('Interconnect',as_index=False).count().rename(columns={'Hour_Counter':'Interconnect_Tot'})\n",
    "inter_count = inter_count.sort_values('Interconnect')\n",
    "#print(inter_count)\n",
    "\n",
    "#read in the group shares data\n",
    "group2 = pd.read_csv('inputs/group_shares.csv')\n",
    "\n",
    "#combined the group shares data with the interconnect/hours data\n",
    "#NOTE: if there are ever more than three interconnects or regions, this code should be updated\n",
    "group2[inter_count.iloc[0,0]] = group2['Share']*inter_count.iloc[0,1]\n",
    "group2[inter_count.iloc[1,0]] = group2['Share']*inter_count.iloc[1,1]\n",
    "group2[inter_count.iloc[2,0]] = group2['Share']*inter_count.iloc[2,1]\n",
    "group_inter2 = pd.melt(group2,id_vars=['Group','Share'],var_name='Interconnect',value_name='Interconnect_Ct')\n",
    "group_inter2['Interconnect_Counter'] = group_inter2['Interconnect_Ct'].cumsum()\n",
    "group_inter2['Interconnect_Counter'] = round(group_inter2['Interconnect_Counter'])\n",
    "#print(group2.dtypes)\n",
    "#print(group2)\n",
    "#print(group_inter)\n",
    "#print()\n",
    "\n",
    "#sort by interconnect, and then load in ascending order\n",
    "inter_x2 = inter_x.sort_values(by=['Interconnect',x_column], ascending=[True, False]).reset_index(drop=True)\n",
    "inter_x2['Interconnect_Counter'] = inter_x2.index + 1.0 \n",
    "\n",
    "#use interconnect_counter to apply groups to each interconnect \n",
    "#create list of group_inter with just the group and interconnect listed\n",
    "group_inter_index = group_inter2[['Group','Interconnect_Counter']].copy()\n",
    "\n",
    "#merge to apply groups to each interconnect value based on the counter  \n",
    "inter_x2 = pd.merge_asof(inter_x2, group_inter_index, on='Interconnect_Counter', direction='forward')\n",
    "inter_x2 = inter_x2.drop(columns=['Interconnect_Counter']).reset_index(drop=True)\n",
    "print(inter_x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Sort by region, season, and time of day\n",
    "### 3: Average load based on groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region R_Group R_Subgroup    Season  Month  Day  Hour  TRG6  \\\n",
      "0  ERC_PHDL     ERC       PHDL  shoulder      4  101    19   0.0   \n",
      "1  ERC_PHDL     ERC       PHDL  shoulder      4  107    19   0.0   \n",
      "2  ERC_PHDL     ERC       PHDL  shoulder     10  279    19   0.0   \n",
      "\n",
      "   Hour_Counter      TOD Interconnect  Group  Hour_Tot    Tot    Avg  \n",
      "0          2419  evening          ERC      5        80  482.0  6.025  \n",
      "1          2563  evening          ERC      5        80  482.0  6.025  \n",
      "2          6691  evening          ERC      5        80  482.0  6.025  \n",
      "number of rows in dataset = 683280\n"
     ]
    }
   ],
   "source": [
    "#sort by region, season, and time of day \n",
    "inter_x3 = inter_x2.sort_values(['Interconnect','Group','Region','Season','TOD'])\n",
    "\n",
    "#average load based on order of groups\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case = inter_x3.groupby(['Interconnect','Group','Region','Season','TOD'],as_index=False).agg(aggregations)\n",
    "case.columns = case.columns.droplevel(0)\n",
    "case.columns = ['Interconnect','Group','Region','Season','TOD','Hour_Tot','Tot','Avg']\n",
    "#print(case.head())\n",
    "#print('number of rows in dataset =',case.shape[0])\n",
    "case.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_interconnect.csv')\n",
    "#print()\n",
    "\n",
    "inter_x4 = pd.merge(inter_x3,case,on=['Interconnect','Group','Region','Season','TOD'],how='left')\n",
    "inter_x4 = inter_x4.sort_values(['Region',x_column]).reset_index(drop=True)\n",
    "print(inter_x4.head(3))\n",
    "print('number of rows in dataset =',inter_x4.shape[0])\n",
    "inter_x4.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_interconnect.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case: NERC Region Grouping\n",
    "### Details: First group the regions by their market group and separate into load groups there. Then, split into regions and group by season and time of day. \n",
    "#### Methodology: Similar, if not identical to the interconnect approach. They will be grouped by market groups instead of interconnects, but everything else remains the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Region R_Group R_Subgroup NERC_Region  Season  Month  Day    TOD  \\\n",
      "0       ERC_PHDL     ERC       PHDL       ERCOT  winter      1    1  night   \n",
      "1       ERC_PHDL     ERC       PHDL       ERCOT  winter      1    1  night   \n",
      "2       ERC_PHDL     ERC       PHDL       ERCOT  winter      1    1  night   \n",
      "3       ERC_PHDL     ERC       PHDL       ERCOT  winter      1    1  night   \n",
      "4       ERC_PHDL     ERC       PHDL       ERCOT  winter      1    1  night   \n",
      "...          ...     ...        ...         ...     ...    ...  ...    ...   \n",
      "683275  WEC_SDGE     WEC       SDGE        CAMX  winter     12  365  night   \n",
      "683276  WEC_SDGE     WEC       SDGE        CAMX  winter     12  365  night   \n",
      "683277  WEC_SDGE     WEC       SDGE        CAMX  winter     12  365  night   \n",
      "683278  WEC_SDGE     WEC       SDGE        CAMX  winter     12  365  night   \n",
      "683279  WEC_SDGE     WEC       SDGE        CAMX  winter     12  365  night   \n",
      "\n",
      "        Hour  Hour_Counter  TRG6  \n",
      "0          1             1   0.0  \n",
      "1          2             2   0.0  \n",
      "2          3             3   0.0  \n",
      "3          4             4   0.0  \n",
      "4          5             5   0.0  \n",
      "...      ...           ...   ...  \n",
      "683275    20          8756   0.0  \n",
      "683276    21          8757   0.0  \n",
      "683277    22          8758   0.0  \n",
      "683278    23          8759   0.0  \n",
      "683279    24          8760   0.0  \n",
      "\n",
      "[683280 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "#match NERC region ID to the region\n",
    "nerc_id = pd.read_csv('inputs/nerc_regions.csv')\n",
    "\n",
    "#merge NERC regions and load data together\n",
    "nerc_x = pd.merge(x,nerc_id, on='Region', how='left')\n",
    "\n",
    "#merge TOD info into DF\n",
    "tod = pd.read_csv('inputs/time_of_day.csv')\n",
    "nerc_x = pd.merge(nerc_x,tod,on='Hour',how='left')\n",
    "nerc_x = nerc_x[['Region','R_Group','R_Subgroup','NERC_Region','Season','Month','Day','TOD','Hour','Hour_Counter',\n",
    "                     x_column]]\n",
    "print(nerc_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Region R_Group R_Subgroup NERC_Region    Season  Month  Day  \\\n",
      "0       WEC_LADW     WEC       LADW        CAMX  shoulder      3   77   \n",
      "1       WEC_LADW     WEC       LADW        CAMX  shoulder      4   97   \n",
      "2       WEC_LADW     WEC       LADW        CAMX  shoulder      4   99   \n",
      "3       WEC_LADW     WEC       LADW        CAMX  shoulder      4   99   \n",
      "4       WEC_LADW     WEC       LADW        CAMX  shoulder      4  103   \n",
      "...          ...     ...        ...         ...       ...    ...  ...   \n",
      "683275  WECC_IID     WEC        IID        SRSG    winter     12  365   \n",
      "683276  WECC_IID     WEC        IID        SRSG    winter     12  365   \n",
      "683277  WECC_IID     WEC        IID        SRSG    winter     12  365   \n",
      "683278  WECC_IID     WEC        IID        SRSG    winter     12  365   \n",
      "683279  WECC_IID     WEC        IID        SRSG    winter     12  365   \n",
      "\n",
      "            TOD  Hour  Hour_Counter   TRG6  Group  \n",
      "0       middday    16          1840  833.0      1  \n",
      "1       middday    16          2320  833.0      1  \n",
      "2       middday    10          2362  833.0      1  \n",
      "3       middday    11          2363  833.0      1  \n",
      "4       middday    15          2463  833.0      1  \n",
      "...         ...   ...           ...    ...    ...  \n",
      "683275    night    20          8756    NaN      6  \n",
      "683276    night    21          8757    NaN      6  \n",
      "683277    night    22          8758    NaN      6  \n",
      "683278    night    23          8759    NaN      6  \n",
      "683279    night    24          8760    NaN      6  \n",
      "\n",
      "[683280 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "#Create a list of interconnects of all regions (551880 rows)\n",
    "nerc_regions = nerc_x.copy()\n",
    "nerc_regions = nerc_regions[['NERC_Region','Hour_Counter']]\n",
    "#print(nerc_regions.head())\n",
    "#print(nerc_regions.shape)\n",
    "\n",
    "#get the number of hours in each interconnect\n",
    "nerc_count = nerc_regions.groupby('NERC_Region',as_index=False).count().rename(columns={'Hour_Counter':'NERC_Tot'})\n",
    "nerc_count = nerc_count.sort_values('NERC_Region')\n",
    "#print(nerc_count)\n",
    "\n",
    "#read in the group shares data\n",
    "group4 = pd.read_csv('inputs/group_shares.csv')\n",
    "\n",
    "#combined the group shares data with the interconnect/hours data\n",
    "#NOTE: there are 15 NERC regions\n",
    "group4[nerc_count.iloc[0,0]] = group4['Share']*nerc_count.iloc[0,1]\n",
    "group4[nerc_count.iloc[1,0]] = group4['Share']*nerc_count.iloc[1,1]\n",
    "group4[nerc_count.iloc[2,0]] = group4['Share']*nerc_count.iloc[2,1]\n",
    "group4[nerc_count.iloc[3,0]] = group4['Share']*nerc_count.iloc[3,1]\n",
    "group4[nerc_count.iloc[4,0]] = group4['Share']*nerc_count.iloc[4,1]\n",
    "group4[nerc_count.iloc[5,0]] = group4['Share']*nerc_count.iloc[5,1]\n",
    "group4[nerc_count.iloc[6,0]] = group4['Share']*nerc_count.iloc[6,1]\n",
    "group4[nerc_count.iloc[7,0]] = group4['Share']*nerc_count.iloc[7,1]\n",
    "group4[nerc_count.iloc[8,0]] = group4['Share']*nerc_count.iloc[8,1]\n",
    "group4[nerc_count.iloc[9,0]] = group4['Share']*nerc_count.iloc[9,1]\n",
    "group4[nerc_count.iloc[10,0]] = group4['Share']*nerc_count.iloc[10,1]\n",
    "group4[nerc_count.iloc[11,0]] = group4['Share']*nerc_count.iloc[11,1]\n",
    "group4[nerc_count.iloc[12,0]] = group4['Share']*nerc_count.iloc[12,1]\n",
    "group4[nerc_count.iloc[13,0]] = group4['Share']*nerc_count.iloc[13,1]\n",
    "group4[nerc_count.iloc[14,0]] = group4['Share']*nerc_count.iloc[14,1]\n",
    "group_nerc = pd.melt(group4,id_vars=['Group','Share'],var_name='NERC_Region',value_name='NERC_Ct')\n",
    "group_nerc['NERC_Counter'] = group_nerc['NERC_Ct'].cumsum()\n",
    "group_nerc['NERC_Counter'] = round(group_nerc['NERC_Counter'])\n",
    "#print(group4.dtypes)\n",
    "#print(group4)\n",
    "#print(group_nerc)\n",
    "#print()\n",
    "\n",
    "#sort by interconnect, and then load in ascending order\n",
    "nerc_x2 = nerc_x.sort_values(by=['NERC_Region',x_column], ascending=[True, False]).reset_index(drop=True)\n",
    "nerc_x2['NERC_Counter'] = nerc_x2.index + 1.0 \n",
    "#print(nerc_x2)\n",
    "\n",
    "#use nerc_counter to apply groups to each nerc region \n",
    "#create list of group_inter with just the group and nerc region listed\n",
    "group_nerc_index = group_nerc[['Group','NERC_Counter']].copy()\n",
    "\n",
    "#merge to apply groups to each NERC region value based on the counter  \n",
    "nerc_x2 = pd.merge_asof(nerc_x2, group_nerc_index, on='NERC_Counter', direction='forward')\n",
    "nerc_x2 = nerc_x2.drop(columns=['NERC_Counter'])\n",
    "print(nerc_x2)\n",
    "nerc_x2.to_csv('../outputs/'+x_name+'_duration_8760_NERC_regions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NERC_Region  Group    Region    Season      TOD  Hour_Tot       Tot  \\\n",
      "0        CAMX      1  WECC_SCE  shoulder  middday         1     771.0   \n",
      "1        CAMX      1  WECC_SCE    summer  middday         8    6225.0   \n",
      "2        CAMX      1  WEC_LADW  shoulder  evening         8    6321.0   \n",
      "3        CAMX      1  WEC_LADW  shoulder  middday       142  114439.0   \n",
      "4        CAMX      1  WEC_LADW    summer  evening         5    3962.0   \n",
      "\n",
      "          Avg  \n",
      "0  771.000000  \n",
      "1  778.125000  \n",
      "2  790.125000  \n",
      "3  805.908451  \n",
      "4  792.400000  \n",
      "number of rows in dataset = 1605\n",
      "\n",
      "          Region R_Group R_Subgroup NERC_Region    Season  Month  Day  \\\n",
      "0       ERC_PHDL     ERC       PHDL       ERCOT  shoulder      4  101   \n",
      "1       ERC_PHDL     ERC       PHDL       ERCOT  shoulder      4  107   \n",
      "2       ERC_PHDL     ERC       PHDL       ERCOT  shoulder     10  279   \n",
      "3       ERC_PHDL     ERC       PHDL       ERCOT  shoulder     10  280   \n",
      "4       ERC_PHDL     ERC       PHDL       ERCOT  shoulder     10  281   \n",
      "...          ...     ...        ...         ...       ...    ...  ...   \n",
      "683275  WEC_SDGE     WEC       SDGE        CAMX    summer      6  161   \n",
      "683276  WEC_SDGE     WEC       SDGE        CAMX  shoulder      4  103   \n",
      "683277  WEC_SDGE     WEC       SDGE        CAMX  shoulder      4  120   \n",
      "683278  WEC_SDGE     WEC       SDGE        CAMX  shoulder      4  120   \n",
      "683279  WEC_SDGE     WEC       SDGE        CAMX  shoulder      4  120   \n",
      "\n",
      "            TOD  Hour  Hour_Counter   TRG6  Group  Hour_Tot     Tot      Avg  \n",
      "0       evening    19          2419    0.0      5        80   482.0    6.025  \n",
      "1       evening    19          2563    0.0      5        80   482.0    6.025  \n",
      "2       evening    19          6691    0.0      5        80   482.0    6.025  \n",
      "3       evening    19          6715    0.0      5        80   482.0    6.025  \n",
      "4       evening    19          6739    0.0      5        80   482.0    6.025  \n",
      "...         ...   ...           ...    ...    ...       ...     ...      ...  \n",
      "683275  middday    15          3855  775.0      1         1   775.0  775.000  \n",
      "683276  middday    15          2463  777.0      1         5  3917.0  783.400  \n",
      "683277  middday    12          2868  777.0      1         5  3917.0  783.400  \n",
      "683278  middday    11          2867  794.0      1         5  3917.0  783.400  \n",
      "683279  middday    10          2866  795.0      1         5  3917.0  783.400  \n",
      "\n",
      "[683280 rows x 15 columns]\n",
      "number of rows in dataset = 683280\n"
     ]
    }
   ],
   "source": [
    "#sort by region, season, and time of day \n",
    "nerc_x3 = nerc_x2.sort_values(['NERC_Region','Group','Region','Season','TOD'])\n",
    "\n",
    "#average load based on order of groups\n",
    "aggregations2 = {x_column:['count',sum,'mean']}\n",
    "case2 = nerc_x3.groupby(['NERC_Region','Group','Region','Season','TOD'],as_index=False).agg(aggregations2)\n",
    "case2.columns = case2.columns.droplevel(0)\n",
    "case2.columns = ['NERC_Region','Group','Region','Season','TOD','Hour_Tot','Tot','Avg']\n",
    "print(case2.head())\n",
    "print('number of rows in dataset =',case2.shape[0])\n",
    "case2.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_NERC_region.csv')\n",
    "print()\n",
    "\n",
    "nerc_x4 = pd.merge(nerc_x3,case2,on=['NERC_Region','Group','Region','Season','TOD'],how='left')\n",
    "nerc_x4 = nerc_x4.sort_values(['Region',x_column]).reset_index(drop=True)\n",
    "print(nerc_x4)\n",
    "print('number of rows in dataset =',nerc_x4.shape[0])\n",
    "nerc_x4.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_NERC_region.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
