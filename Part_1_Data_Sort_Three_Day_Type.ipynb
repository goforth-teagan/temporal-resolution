{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Peak Load Days in Each Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Region R_Group R_Subgroup  Season  Month  Day  Hour  Hour_Counter  \\\n",
      "0       ERC_REST     ERC       REST  winter      1    1     1             1   \n",
      "1       ERC_REST     ERC       REST  winter      1    1     2             2   \n",
      "2       ERC_REST     ERC       REST  winter      1    1     3             3   \n",
      "3       ERC_REST     ERC       REST  winter      1    1     4             4   \n",
      "4       ERC_REST     ERC       REST  winter      1    1     5             5   \n",
      "...          ...     ...        ...     ...    ...  ...   ...           ...   \n",
      "551875  WEC_SDGE     WEC       SDGE  winter     12  365    20          8756   \n",
      "551876  WEC_SDGE     WEC       SDGE  winter     12  365    21          8757   \n",
      "551877  WEC_SDGE     WEC       SDGE  winter     12  365    22          8758   \n",
      "551878  WEC_SDGE     WEC       SDGE  winter     12  365    23          8759   \n",
      "551879  WEC_SDGE     WEC       SDGE  winter     12  365    24          8760   \n",
      "\n",
      "         Load  Load_Tot  Load_Max  \n",
      "0       34807    917588   1000605  \n",
      "1       34551    917588   1000605  \n",
      "2       34788    917588   1000605  \n",
      "3       35531    917588   1000605  \n",
      "4       36633    917588   1000605  \n",
      "...       ...       ...       ...  \n",
      "551875   2809     53949     63740  \n",
      "551876   2675     53949     63740  \n",
      "551877   2524     53949     63740  \n",
      "551878   2362     53949     63740  \n",
      "551879   2206     53949     63740  \n",
      "\n",
      "[551880 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "#importing packages needed for analysis\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from pandas import DataFrame\n",
    "\n",
    "path = os.getcwd()\n",
    "#print(path)\n",
    "\n",
    "#create temporary DF to find peak\n",
    "test = pd.read_csv('outputs/load_long_format.csv')\n",
    "\n",
    "#groupby region, month, and day to sum the total day load\n",
    "aggregations1 = {'Load':sum}\n",
    "test_sum = test.groupby(['Region','Month','Day'],as_index=False).agg(aggregations1)\n",
    "#test1.columns = test1.columns.droplevel(0)\n",
    "test_sum.columns = ['Region','Month','Day','Load_Tot']\n",
    "#print(test_sum.head())\n",
    "#print('number of rows in dataset =',test_sum.shape[0])\n",
    "\n",
    "test3 = pd.merge(test,test_sum,on=['Region','Month','Day'],how='left')\n",
    "#print(test3)\n",
    "\n",
    "#groupby region and month to find maximum load\n",
    "aggregations2 = {'Load_Tot':max}\n",
    "test_max = test_sum.groupby(['Region','Month'],as_index=False).agg(aggregations2)\n",
    "test_max.columns = ['Region','Month','Load_Max']\n",
    "#print(test_max.head())\n",
    "\n",
    "test4 = pd.merge(test3,test_max,on=['Region','Month'],how='left')\n",
    "test4 = test4.drop(test4.columns[0], axis=1)\n",
    "print(test4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Region R_Group R_Subgroup  Season  Month  Day  Hour  Hour_Counter  \\\n",
      "0       ERC_REST     ERC       REST  winter      1    1     1             1   \n",
      "1       ERC_REST     ERC       REST  winter      1    1     2             2   \n",
      "2       ERC_REST     ERC       REST  winter      1    1     3             3   \n",
      "3       ERC_REST     ERC       REST  winter      1    1     4             4   \n",
      "4       ERC_REST     ERC       REST  winter      1    1     5             5   \n",
      "...          ...     ...        ...     ...    ...  ...   ...           ...   \n",
      "551875  WEC_SDGE     WEC       SDGE  winter     12  365    20          8756   \n",
      "551876  WEC_SDGE     WEC       SDGE  winter     12  365    21          8757   \n",
      "551877  WEC_SDGE     WEC       SDGE  winter     12  365    22          8758   \n",
      "551878  WEC_SDGE     WEC       SDGE  winter     12  365    23          8759   \n",
      "551879  WEC_SDGE     WEC       SDGE  winter     12  365    24          8760   \n",
      "\n",
      "         Load       Date  DOW Day_Type  \n",
      "0       34807 2016-01-01    4  Weekday  \n",
      "1       34551 2016-01-01    4  Weekday  \n",
      "2       34788 2016-01-01    4  Weekday  \n",
      "3       35531 2016-01-01    4  Weekday  \n",
      "4       36633 2016-01-01    4  Weekday  \n",
      "...       ...        ...  ...      ...  \n",
      "551875   2809 2011-12-31    5  Weekend  \n",
      "551876   2675 2011-12-31    5  Weekend  \n",
      "551877   2524 2011-12-31    5  Weekend  \n",
      "551878   2362 2011-12-31    5  Weekend  \n",
      "551879   2206 2011-12-31    5  Weekend  \n",
      "\n",
      "[551880 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "load_dur1 = pd.read_csv('outputs/load_long_format.csv', index_col=0)\n",
    "load_years = pd.read_csv('inputs/load_years.csv').dropna()\n",
    "#print(load_dur1.head())\n",
    "#print(load_years)\n",
    "\n",
    "from itertools import cycle\n",
    "\n",
    "#repeat the date for 24 times for each hour in the day\n",
    "year_2016 = load_years['2016'].repeat(24).reset_index(drop=True)\n",
    "year_2011 = load_years['2011'].repeat(24).reset_index(drop=True)\n",
    "#print(year_2016)\n",
    "\n",
    "#use itertools.cycle to make the date repeat until it reaches the end of the dataframe\n",
    "year_2016 = cycle(year_2016)\n",
    "year_2011 = cycle(year_2011)\n",
    "\n",
    "#create temporary data frames where they will iterate through. Needed to achieve the correct length of the DF\n",
    "temp2016 = load_dur1.loc[load_dur1['R_Group'] == 'ERC'].copy()\n",
    "temp2011 = load_dur1.loc[load_dur1['R_Group'] != 'ERC'].copy()\n",
    "\n",
    "#iterate through the date until the end of DF \n",
    "temp2016['Date'] = [next(year_2016) for year in range(len(temp2016))]\n",
    "temp2011['Date'] = [next(year_2011) for year in range(len(temp2011))]\n",
    "#print(temp2016)\n",
    "#print(temp2011)\n",
    "\n",
    "load_dur2 = pd.concat([temp2016, temp2011], ignore_index=True)\n",
    "#print(load_dur2)\n",
    "\n",
    "#convert date to a datetime type \n",
    "load_dur2['Date'] = pd.to_datetime(load_dur2['Date'])\n",
    "load_dur2['DOW'] = load_dur2['Date'].dt.weekday\n",
    "\n",
    "#check if it is a weekday or not \n",
    "weekday = pd.read_csv('inputs/weekday.csv')\n",
    "load_dur2 = pd.merge(load_dur2,weekday,on='DOW',how='left')\n",
    "#print(load_dur2)\n",
    "\n",
    "load_dur3 = pd.merge(load_dur2,test4,on=['Region','R_Group','R_Subgroup','Season','Month','Day',\n",
    "                                         'Hour','Hour_Counter','Load'],how='left')\n",
    "load_dur3 = load_dur3.rename(columns={'Weekday':'Day_Type'})\n",
    "#print(load_dur3)\n",
    "\n",
    "#Return True if the load total equals the day identified as the max\n",
    "load_dur3.loc[load_dur3['Day_Type'] == True, 'Day_Type'] = 'Weekday'\n",
    "load_dur3.loc[load_dur3['Day_Type'] == False, 'Day_Type'] = 'Weekend'\n",
    "load_dur3.loc[load_dur3['Load_Tot'] == load_dur3['Load_Max'], 'Day_Type'] = 'Peak'\n",
    "load_dur3 = load_dur3.drop(['Load_Tot','Load_Max'], axis=1)\n",
    "print(load_dur3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1: Monthly, Weekend/weekday/peak day-types, 24 hours (864 segments)\n",
    "#### Methodology: similar to two day type, just adding in peak day types to sort by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Month Day_Type  Hour  Hour_Tot  Load_Tot  Load_Avg\n",
      "0  ERC_REST      1     Peak     1         1     38567   38567.0\n",
      "1  ERC_REST      1     Peak     2         1     39107   39107.0\n",
      "2  ERC_REST      1     Peak     3         1     40311   40311.0\n",
      "3  ERC_REST      1     Peak     4         1     43115   43115.0\n",
      "4  ERC_REST      1     Peak     5         1     47186   47186.0\n",
      "number of rows in dataset = 54432\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour  Hour_Counter  \\\n",
      "2042  ERC_REST     ERC       REST  shoulder      3   86     3          2043   \n",
      "2233  ERC_REST     ERC       REST  shoulder      4   94     2          2234   \n",
      "1706  ERC_REST     ERC       REST  shoulder      3   72     3          1707   \n",
      "\n",
      "       Load       Date  DOW Day_Type  Hour_Tot  Load_Tot   Load_Avg  \n",
      "2042  26989 2016-03-26    5  Weekend         8    226231  28278.875  \n",
      "2233  27006 2016-04-03    6  Weekend         8    228827  28603.375  \n",
      "1706  27025 2016-03-12    5  Weekend         8    226231  28278.875  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case1_load = load_dur3.copy()\n",
    "\n",
    "aggregations = {'Load':['count',sum,'mean']}\n",
    "case1 = case1_load.groupby(['Region','Month','Day_Type','Hour'],as_index=False).agg(aggregations)\n",
    "case1.columns = case1.columns.droplevel(0)\n",
    "case1.columns = ['Region','Month','Day_Type','Hour','Hour_Tot','Load_Tot','Load_Avg']\n",
    "print(case1.head())\n",
    "print('number of rows in dataset =',case1.shape[0])\n",
    "#case1.to_csv('outputs/load_segments_3daytype_monthly_24hr.csv')\n",
    "print()\n",
    "\n",
    "case1_load2 = pd.merge(case1_load,case1,on=['Region','Month','Day_Type','Hour'],how='left')\n",
    "case1_load2 = case1_load2.sort_values(['Region','Load'])\n",
    "print(case1_load2.head(3))\n",
    "print('number of rows in dataset =',case1_load2.shape[0])\n",
    "#case1_load2.to_csv('outputs/load_8760_3daytype_monthly_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2: Season, weekend/weekday/peak day-types, 24-hours (216 segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Season_Group Day_Type  Hour  Hour_Tot  Load_Tot      Load_Avg\n",
      "0  ERC_REST             1     Peak     1         3    121846  40615.333333\n",
      "1  ERC_REST             1     Peak     2         3    123092  41030.666667\n",
      "2  ERC_REST             1     Peak     3         3    126145  42048.333333\n",
      "3  ERC_REST             1     Peak     4         3    133193  44397.666667\n",
      "4  ERC_REST             1     Peak     5         3    144044  48014.666667\n",
      "number of rows in dataset = 22680\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour  Hour_Counter  \\\n",
      "2042  ERC_REST     ERC       REST  shoulder      3   86     3          2043   \n",
      "2233  ERC_REST     ERC       REST  shoulder      4   94     2          2234   \n",
      "1706  ERC_REST     ERC       REST  shoulder      3   72     3          1707   \n",
      "\n",
      "       Load       Date  DOW Day_Type  Hour_Tot  Load_Tot   Load_Avg  \n",
      "2042  26989 2016-03-26    5  Weekend         8    226231  28278.875  \n",
      "2233  27006 2016-04-03    6  Weekend         8    228827  28603.375  \n",
      "1706  27025 2016-03-12    5  Weekend         8    226231  28278.875  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case2_load = load_dur3.copy()\n",
    "case2_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case2_seasons = case2_seasons.drop(['bimonthly'], axis=1)\n",
    "case2_seasons = case2_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "\n",
    "case2_load = pd.merge(case2_load, case2_seasons, on='Month', how='left')\n",
    "#print(case2_load)\n",
    "\n",
    "aggregations = {'Load':['count',sum,'mean']}\n",
    "case2 = case2_load.groupby(['Region','Season_Group','Day_Type','Hour'],as_index=False).agg(aggregations)\n",
    "case2.columns = case2.columns.droplevel(0)\n",
    "case2.columns = ['Region','Season_Group','Day_Type','Hour','Hour_Tot','Load_Tot','Load_Avg']\n",
    "print(case2.head())\n",
    "print('number of rows in dataset =',case2.shape[0])\n",
    "#case2.to_csv('outputs/load_segments_3daytype_season_24hr.csv')\n",
    "print()\n",
    "\n",
    "case2_load2 = pd.merge(case2_load,case2,on=['Region','Season_Group','Day_Type','Hour'],how='left')\n",
    "case2_load2 = case2_load2.sort_values(['Region','Load'])\n",
    "print(case1_load2.head(3))\n",
    "print('number of rows in dataset =',case1_load2.shape[0])\n",
    "#case2_load2.to_csv('outputs/load_8760_3daytype_season_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 3: Annual, weekend/weekday/peak day-types, 24-hours (72 segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region Day_Type  Hour  Hour_Tot  Load_Tot      Load_Avg\n",
      "0  ERC_REST     Peak     1        12    455893  37991.083333\n",
      "1  ERC_REST     Peak     2        12    450804  37567.000000\n",
      "2  ERC_REST     Peak     3        12    454384  37865.333333\n",
      "3  ERC_REST     Peak     4        12    473371  39447.583333\n",
      "4  ERC_REST     Peak     5        12    505999  42166.583333\n",
      "number of rows in dataset = 4536\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour  Hour_Counter  \\\n",
      "2042  ERC_REST     ERC       REST  shoulder      3   86     3          2043   \n",
      "2233  ERC_REST     ERC       REST  shoulder      4   94     2          2234   \n",
      "1706  ERC_REST     ERC       REST  shoulder      3   72     3          1707   \n",
      "\n",
      "       Load       Date  DOW Day_Type  Hour_Tot  Load_Tot      Load_Avg  \n",
      "2042  26989 2016-03-26    5  Weekend       101   3270128  32377.504950  \n",
      "2233  27006 2016-04-03    6  Weekend       101   3269439  32370.683168  \n",
      "1706  27025 2016-03-12    5  Weekend       101   3270128  32377.504950  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case3_load = load_dur3.copy()\n",
    "\n",
    "aggregations = {'Load':['count',sum,'mean']}\n",
    "case3 = case3_load.groupby(['Region','Day_Type','Hour'],as_index=False).agg(aggregations)\n",
    "case3.columns = case3.columns.droplevel(0)\n",
    "case3.columns = ['Region','Day_Type','Hour','Hour_Tot','Load_Tot','Load_Avg']\n",
    "print(case3.head())\n",
    "print('number of rows in dataset =',case3.shape[0])\n",
    "case3.to_csv('outputs/load_segments_3daytype_annual_24hr.csv')\n",
    "print()\n",
    "\n",
    "case3_load2 = pd.merge(case3_load,case3,on=['Region','Day_Type','Hour'],how='left')\n",
    "case3_load2 = case3_load2.sort_values(['Region','Load'])\n",
    "print(case3_load2.head(3))\n",
    "print('number of rows in dataset =',case3_load2.shape[0])\n",
    "case3_load2.to_csv('outputs/load_8760_3daytype_annual_24hr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
