{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis on 8760 profiles (22 profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output files are written out in parent directory: C:\\Users\\tgoforth\\Documents\\IPM temporal resolution project\\outputs\\error_analysis\n"
     ]
    }
   ],
   "source": [
    "#importing packages needed for analysis\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from pandas import DataFrame\n",
    "\n",
    "path = os.getcwd()\n",
    "#print(path)\n",
    "\n",
    "#this code creates an output directory in the parent director, if one does not exist yet\n",
    "#Note: this is where all of the output files will be written, since outputs are large this saves space in git\n",
    "path = os.getcwd()\n",
    "parent = os.path.dirname(path)\n",
    "outputs_dir = parent+'\\outputs\\error_analysis'\n",
    "if not os.path.exists(outputs_dir):\n",
    "    os.makedirs(outputs_dir)\n",
    "print('output files are written out in parent directory: '+outputs_dir)\n",
    "\n",
    "##UNCOMMENT WHICH PROFILE BEING ANALYZED \n",
    "x = 'load'\n",
    "x2 = 'Load'\n",
    "\n",
    "#x = 'solar'\n",
    "#x2 = 'Solar_Gen'\n",
    "\n",
    "#x = 'wind'\n",
    "#x2 = 'Wind_Gen'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "difference in values \n",
    "root mean square for each region\n",
    "other analyses? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data \n",
    "## Choose the profile to use by commenting / uncommenting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### GENERAL DATA LOAD ###############\n",
    "# have to manually input CSV file name if not one of the 22 profiles\n",
    "#x_segments = pd.read_csv('')\n",
    "#x_hours = pd.read_csv('')\n",
    "#x_name = ''\n",
    "\n",
    "############### IPM APPROACH (2 profiles) ###############\n",
    "# NORM\n",
    "#x_segments = pd.read_csv('../outputs/'+x+'/'+x+'_segments_NORM.csv')\n",
    "#x_hours = pd.read_csv('../outputs/'+x+'/'+x+'_8760_NORM.csv')\n",
    "#x_name = 'norm'\n",
    "\n",
    "# TOD before group\n",
    "#x_segments = pd.read_csv('../outputs/'+x+'/'+x+'_segments_timeofday.csv')\n",
    "#x_hours = pd.read_csv('../outputs/'+x+'/'+x+'_8760_timeofday.csv')\n",
    "#x_name = 'timeofday'\n",
    "\n",
    "############### SEQUENTIAL APPROACH (9 profiles) ###############\n",
    "# 1-hr\n",
    "x_segments = pd.read_csv('../outputs/'+x+'/'+x+'_segments_bind_8760hr.csv')\n",
    "x_hours = pd.read_csv('../outputs/'+x+'/'+x+'_8760_bind_8760hr.csv')\n",
    "x_name = 'seq_1hr'\n",
    "\n",
    "# 2-hr\n",
    "#x_segments = pd.read_csv('../outputs/'+x+'/'+x+'_segments_seq_2hr.csv')\n",
    "#x_hours = pd.read_csv('../outputs/'+x+'/'+x+'_8760_seq_2hr.csv')\n",
    "#x_name = 'seq_2hr'\n",
    "\n",
    "# 4-hr\n",
    "#x_segments = pd.read_csv('../outputs/'+x+'/'+x+'_segments_seq_4hr.csv')\n",
    "#x_hours = pd.read_csv('../outputs/'+x+'/'+x+'_8760_seq_4hr.csv')\n",
    "#x_name = 'seq_4hr'\n",
    "\n",
    "# 8-hr\n",
    "#x_segments = pd.read_csv('../outputs/'+x+'/'+x+'_segments_seq_8hr.csv')\n",
    "#x_hours = pd.read_csv('../outputs/'+x+'/'+x+'_8760_seq_8hr.csv')\n",
    "#x_name = 'seq_8hr'\n",
    "\n",
    "# 12-hr\n",
    "#x_segments = pd.read_csv('../outputs/'+x+'/'+x+'_segments_seq_12hr.csv')\n",
    "#x_hours = pd.read_csv('../outputs/'+x+'/'+x+'_8760_seq_12hr.csv')\n",
    "#x_name = 'seq_12hr'\n",
    "\n",
    "# 24-hr\n",
    "#x_segments = pd.read_csv('../outputs/'+x+'/'+x+'_segments_seq_24hr.csv')\n",
    "#x_hours = pd.read_csv('../outputs/'+x+'/'+x+'_8760_seq_24hr.csv')\n",
    "#x_name = 'seq_24hr'\n",
    "\n",
    "# 48-hr\n",
    "#x_segments = pd.read_csv('../outputs/'+x+'/'+x+'_segments_seq_48hr.csv')\n",
    "#x_hours = pd.read_csv('../outputs/'+x+'/'+x+'_8760_seq_48hr.csv')\n",
    "#x_name = 'seq_48hr'\n",
    "\n",
    "# 120-hr\n",
    "#x_segments = pd.read_csv('../outputs/'+x+'/'+x+'_segments_seq_120hr.csv')\n",
    "#x_hours = pd.read_csv('../outputs/'+x+'/'+x+'_8760_seq_120hr.csv')\n",
    "#x_name = 'seq_120hr'\n",
    "\n",
    "# 8760-hr\n",
    "#x_segments = pd.read_csv('../outputs/'+x+'/'+x+'_segments_bind_1hr.csv')\n",
    "#x_hours = pd.read_csv('../outputs/'+x+'/'+x+'_8760_bind_1hr.csv')\n",
    "#x_name = 'seq_8760hr'\n",
    "\n",
    "############### ONE/TWO DAY APPROACH (7 profiles) ###############\n",
    "# One day type, monthly, 24 hrs\n",
    "#x_segments = pd.read_csv('../outputs/'+x+'/'+x+'_segments_1daytype_monthly_24hr.csv')\n",
    "#x_hours = pd.read_csv('../outputs/'+x+'/'+x+'_8760_1daytype_monthly_24hr.csv')\n",
    "#x_name = '1daytype_month_24hr'\n",
    "\n",
    "# One day type, season, 24 hrs\n",
    "#x_segments = pd.read_csv('../outputs/'+x+'/'+x+'_segments_1daytype_season_24hr.csv')\n",
    "#x_hours = pd.read_csv('../outputs/'+x+'/'+x+'_8760_1daytype_season_24hr.csv')\n",
    "#x_name = '1daytype_season_24hr'\n",
    "\n",
    "# Two day type, monthly, 24 hrs\n",
    "#x_segments = pd.read_csv('../outputs/'+x+'/'+x+'_segments_2daytype_monthly_24hr.csv')\n",
    "#x_hours = pd.read_csv('../outputs/'+x+'/'+x+'_8760_2daytype_monthly_24hr.csv')\n",
    "#x_name = '2daytype_month_24hr'\n",
    "\n",
    "# Two day type, season, 24 hrs#\n",
    "#x_segments = pd.read_csv('../outputs/'+x+'/'+x+'_segments_2daytype_season_24hr.csv')\n",
    "#x_hours = pd.read_csv('../outputs/'+x+'/'+x+'_8760_2daytype_season_24hr.csv')\n",
    "#x_name = '2daytype_season_24hr'\n",
    "\n",
    "# Two day type, monthly, 4 hrs\n",
    "#x_segments = pd.read_csv('../outputs/'+x+'/'+x+'_segments_2daytype_month_4hr.csv')\n",
    "#x_hours = pd.read_csv('../outputs/'+x+'/'+x+'_8760_2daytype_month_4hr.csv')\n",
    "#x_name = '2daytype_month_4hr'\n",
    "\n",
    "# Two day type, bimonthly, 4 hrs\n",
    "#x_segments = pd.read_csv('../outputs/'+x+'/'+x+'_segments_2daytype_bimonth_4hr.csv')\n",
    "#x_hours = pd.read_csv('../outputs/'+x+'/'+x+'_8760_2daytype_bimonth_4hr.csv')\n",
    "#x_name = '2daytype_bimonth_4hr'\n",
    "\n",
    "# Two day type, season, 4 hrs\n",
    "#x_segments = pd.read_csv('../outputs/'+x+'/'+x+'_segments_2daytype_season_4hr.csv')\n",
    "#x_hours = pd.read_csv('../outputs/'+x+'/'+x+'_8760_2daytype_season_4hr.csv')\n",
    "#x_name = '2daytype_season_4hr'\n",
    "\n",
    "############### THREE DAY TYPE APPROACH (6 profiles) ###############\n",
    "# Three day type, monthly, 24 hrs\n",
    "#x_segments = pd.read_csv('../outputs/'+x+'/'+x+'_segments_3daytype_monthly_24hr.csv')\n",
    "#x_hours = pd.read_csv('../outputs/'+x+'/'+x+'_8760_3daytype_monthly_24hr.csv')\n",
    "#x_name = '3daytype_month_24hr'\n",
    "\n",
    "# Three day type, season, 24 hrs\n",
    "#x_segments = pd.read_csv('../outputs/'+x+'/'+x+'_segments_3daytype_season_24hr.csv')\n",
    "#x_hours = pd.read_csv('../outputs/'+x+'/'+x+'_8760_3daytype_season_24hr.csv')\n",
    "#x_name = '3daytype_season_24hr'\n",
    "\n",
    "# Three day type, annual, 24 hrs\n",
    "#x_segments = pd.read_csv('../outputs/'+x+'/'+x+'_segments_3daytype_annual_24hr.csv')\n",
    "#x_hours = pd.read_csv('../outputs/'+x+'/'+x+'_8760_3daytype_annual_24hr.csv')\n",
    "#x_name = '3daytype_annual_24hr'\n",
    "\n",
    "# Three day type, bimonthly, 4 hrs\n",
    "#x_segments = pd.read_csv('../outputs/'+x+'/'+x+'_segments_3daytype_bimonth_4hr.csv')\n",
    "#x_hours = pd.read_csv('../outputs/'+x+'/'+x+'_8760_3daytype_bimonth_4hr.csv')\n",
    "#x_name = '3daytype_bimonth_4hr'\n",
    "\n",
    "# Three day type, season based, 4 hrs\n",
    "#x_segments = pd.read_csv('../outputs/'+x+'/'+x+'_segments_3daytype_seasonbased_4hr.csv')\n",
    "#x_hours = pd.read_csv('../outputs/'+x+'/'+x+'_8760_3daytype_seasonbased_4hr.csv')\n",
    "#x_name = '3daytype_seasonbased_4hr'\n",
    "\n",
    "# Three day type, season, 4 hrs\n",
    "#x_segments = pd.read_csv('../outputs/'+x+'/'+x+'_segments_3daytype_season_4hr.csv')\n",
    "#x_hours = pd.read_csv('../outputs/'+x+'/'+x+'_8760_3daytype_season_4hr.csv')\n",
    "#x_name = '3daytype_season_4hr'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create outputs: total number of representative hours, RMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Region  Rep_Count\n",
      "0   ERC_REST       8760\n",
      "1   ERC_WEST       8760\n",
      "2       FRCC       8760\n",
      "3   MIS_AMSO       8760\n",
      "4     MIS_AR       8760\n",
      "..       ...        ...\n",
      "58   WECC_WY       8760\n",
      "59  WEC_BANC       8760\n",
      "60  WEC_CALN       8760\n",
      "61  WEC_LADW       8760\n",
      "62  WEC_SDGE       8760\n",
      "\n",
      "[63 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#print(x_hours)\n",
    "\n",
    "# use groupby region to count number of representative hours in each region\n",
    "aggregations = {'Avg':'count'}\n",
    "stat1 = x_segments.groupby(['Region'],as_index=False).agg(aggregations)\n",
    "stat1 = stat1.rename(columns={'Avg':'Rep_Count'})\n",
    "#stat1 = stat1[['Region','Count']].copy()\n",
    "\n",
    "print(stat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Region    Avg  Hour_Tot  RMSE\n",
      "0       ERC_REST  26989         1   0.0\n",
      "1       ERC_REST  27006         1   0.0\n",
      "2       ERC_REST  27025         1   0.0\n",
      "3       ERC_REST  27059         1   0.0\n",
      "4       ERC_REST  27069         1   0.0\n",
      "...          ...    ...       ...   ...\n",
      "190326  WEC_SDGE   4434         1   0.0\n",
      "190327  WEC_SDGE   4446         1   0.0\n",
      "190328  WEC_SDGE   4465         1   0.0\n",
      "190329  WEC_SDGE   4470         1   0.0\n",
      "190330  WEC_SDGE   4479         1   0.0\n",
      "\n",
      "[190331 rows x 4 columns]\n",
      "          Region R_Group  Rep_Count  Hour_Tot   Load    Avg  Diff  Square  \\\n",
      "0       ERC_REST     ERC       8760         1  26989  26989     0       0   \n",
      "1       ERC_REST     ERC       8760         1  27006  27006     0       0   \n",
      "2       ERC_REST     ERC       8760         1  27025  27025     0       0   \n",
      "3       ERC_REST     ERC       8760         1  27059  27059     0       0   \n",
      "4       ERC_REST     ERC       8760         1  27069  27069     0       0   \n",
      "...          ...     ...        ...       ...    ...    ...   ...     ...   \n",
      "551875  WEC_SDGE     WEC       8760         1   4434   4434     0       0   \n",
      "551876  WEC_SDGE     WEC       8760         1   4446   4446     0       0   \n",
      "551877  WEC_SDGE     WEC       8760         1   4465   4465     0       0   \n",
      "551878  WEC_SDGE     WEC       8760         1   4470   4470     0       0   \n",
      "551879  WEC_SDGE     WEC       8760         1   4479   4479     0       0   \n",
      "\n",
      "        RMSE  \n",
      "0        0.0  \n",
      "1        0.0  \n",
      "2        0.0  \n",
      "3        0.0  \n",
      "4        0.0  \n",
      "...      ...  \n",
      "551875   0.0  \n",
      "551876   0.0  \n",
      "551877   0.0  \n",
      "551878   0.0  \n",
      "551879   0.0  \n",
      "\n",
      "[551880 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# calculate RMSE \n",
    "stat2 = x_hours[['Region','R_Group','Hour_Tot',x2,'Avg']].copy()\n",
    "stat2 = pd.merge(stat2,stat1,on='Region',how='left')\n",
    "stat2 = stat2[['Region','R_Group','Rep_Count','Hour_Tot',x2,'Avg']]\n",
    "\n",
    "#print(stat2)\n",
    "\n",
    "stat2['Diff'] = stat2[x2] - stat2['Avg'] \n",
    "stat2['Square'] = stat2['Diff']**2\n",
    "stat_reg = stat2.groupby(['Region','Avg','Hour_Tot'],as_index=False).agg({'Square' : sum})\n",
    "stat_reg = stat_reg.rename(columns = {'Square':'Square_Sum'}) \n",
    "stat_reg['Mean'] = stat_reg['Square_Sum'] / stat_reg['Hour_Tot'] \n",
    "stat_reg['RMSE'] = stat_reg['Mean']**(1/2)\n",
    "stat_reg = stat_reg.drop(columns={'Square_Sum','Mean'})\n",
    "print(stat_reg)\n",
    "\n",
    "stat3 = pd.merge(stat2,stat_reg,on=['Region','Avg','Hour_Tot'],how='left')\n",
    "print(stat3)\n",
    "\n",
    "stat3.to_csv('../outputs/error_analysis/'+x+'_'+x_name+'_error.csv')\n",
    "stat_reg.to_csv('../outputs/error_analysis/'+x+'_'+x_name+'region_error.csv')\n",
    "\n",
    "# RMSE tells us how spread out the data is over a line of best fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    551880.0\n",
       "mean          0.0\n",
       "std           0.0\n",
       "min           0.0\n",
       "25%           0.0\n",
       "50%           0.0\n",
       "75%           0.0\n",
       "max           0.0\n",
       "Name: Diff, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# descriptive statistics of entire data set \n",
    "stat2['Diff'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows in dataset for testing = 8760\n",
      "count    8760.0\n",
      "mean        0.0\n",
      "std         0.0\n",
      "min         0.0\n",
      "25%         0.0\n",
      "50%         0.0\n",
      "75%         0.0\n",
      "max         0.0\n",
      "Name: Diff, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# descriptive statistics of only a single region\n",
    "x_FRCC = stat2[stat2['R_Group']=='FRCC']\n",
    "print('number of rows in dataset for testing =',x_FRCC.shape[0])\n",
    "\n",
    "x_FRCC_stat = x_FRCC['Diff'].describe()\n",
    "print(x_FRCC_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
