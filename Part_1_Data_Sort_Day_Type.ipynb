{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data here: \n",
    "## One and two day type \n",
    "## Three day type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day Type: One/two\n",
    "## Create Date and Day of Week columns in the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output files are written out in parent directory: C:\\Users\\tgoforth\\Documents\\IPM temporal resolution project\\outputs\n",
      "           2016        2011\n",
      "0    2016-01-01  2011-01-01\n",
      "1    2016-01-02  2011-01-02\n",
      "2    2016-01-03  2011-01-03\n",
      "3    2016-01-04  2011-01-04\n",
      "4    2016-01-05  2011-01-05\n",
      "..          ...         ...\n",
      "360  2016-12-26  2011-12-27\n",
      "361  2016-12-27  2011-12-28\n",
      "362  2016-12-28  2011-12-29\n",
      "363  2016-12-29  2011-12-30\n",
      "364  2016-12-30  2011-12-31\n",
      "\n",
      "[365 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#importing packages needed for analysis\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from pandas import DataFrame\n",
    "\n",
    "path = os.getcwd()\n",
    "#print(path)\n",
    "\n",
    "#this code creates an output directory in the parent director, if one does not exist yet\n",
    "#Note: this is where all of the output files will be written, since outputs are large this saves space in git\n",
    "path = os.getcwd()\n",
    "parent = os.path.dirname(path)\n",
    "outputs_dir = parent+'\\outputs'\n",
    "if not os.path.exists(outputs_dir):\n",
    "    os.makedirs(outputs_dir)\n",
    "print('output files are written out in parent directory: '+outputs_dir)\n",
    "\n",
    "load_dur1 = pd.read_csv('outputs/load_long_format.csv', index_col=0)\n",
    "load_years = pd.read_csv('inputs/load_years.csv').dropna()\n",
    "#print(load_dur1.head())\n",
    "print(load_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "#repeat the date for 24 times for each hour in the day\n",
    "year_2016 = load_years['2016'].repeat(24).reset_index(drop=True)\n",
    "year_2011 = load_years['2011'].repeat(24).reset_index(drop=True)\n",
    "#print(year_2016)\n",
    "\n",
    "#use itertools.cycle to make the date repeat until it reaches the end of the dataframe\n",
    "year_2016 = cycle(year_2016)\n",
    "year_2011 = cycle(year_2011)\n",
    "\n",
    "#create temporary data frames where they will iterate through. Needed to achieve the correct length of the DF\n",
    "temp2016 = load_dur1.loc[load_dur1['R_Group'] == 'ERC'].copy()\n",
    "temp2011 = load_dur1.loc[load_dur1['R_Group'] != 'ERC'].copy()\n",
    "\n",
    "#iterate through the date until the end of DF \n",
    "temp2016['Date'] = [next(year_2016) for year in range(len(temp2016))]\n",
    "temp2011['Date'] = [next(year_2011) for year in range(len(temp2011))]\n",
    "#print(temp2016)\n",
    "#print(temp2011)\n",
    "\n",
    "load_dur2 = pd.concat([temp2016, temp2011], ignore_index=True)\n",
    "#print(load_dur2)\n",
    "\n",
    "#convert date to a datetime type \n",
    "load_dur2['Date'] = pd.to_datetime(load_dur2['Date'])\n",
    "load_dur2['DOW'] = load_dur2['Date'].dt.weekday\n",
    "\n",
    "#check if it is a weekday or not \n",
    "weekday = pd.read_csv('inputs/weekday.csv')\n",
    "load_dur2 = pd.merge(load_dur2,weekday,on='DOW',how='left')\n",
    "#print(load_dur2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Monthly, single day type, 24 hours (288 segments)\n",
    "#### Methodology: Using groupby function to group first by month, then by 24 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Month  Hour  Hour_Tot  Load_Tot      Load_Avg\n",
      "0  ERC_REST      1     1        31   1042446  33627.290323\n",
      "1  ERC_REST      1     2        31   1051661  33924.548387\n",
      "2  ERC_REST      1     3        31   1079739  34830.290323\n",
      "3  ERC_REST      1     4        31   1144492  36919.096774\n",
      "4  ERC_REST      1     5        31   1239385  39980.161290\n",
      "number of rows in dataset = 18144\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "\n",
      "           Date  DOW  Weekday  Hour_Tot  Load_Tot      Load_Avg  \n",
      "1545 2016-03-05    5    False        31    886708  28603.483871  \n",
      "823  2016-02-04    3     True        30    870591  29019.700000  \n",
      "1531 2016-03-04    4     True        31    886708  28603.483871  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case1_load = load_dur2.copy()\n",
    "\n",
    "aggregations = {'Load':['count',sum,'mean']}\n",
    "case1 = case1_load.groupby(['Region','Month','Hour'],as_index=False).agg(aggregations)\n",
    "case1.columns = case1.columns.droplevel(0)\n",
    "case1.columns = ['Region','Month','Hour','Hour_Tot','Load_Tot','Load_Avg']\n",
    "print(case1.head())\n",
    "print('number of rows in dataset =',case1.shape[0])\n",
    "#case1.to_csv('../outputs/load_segments_2daytype_monthly_24hr.csv')\n",
    "print()\n",
    "\n",
    "case1_load2 = pd.merge(case1_load,case1,on=['Region','Month','Hour'],how='left')\n",
    "case1_load2 = case1_load2.sort_values(['Region','Load'])\n",
    "print(case1_load2.head(3))\n",
    "print('number of rows in dataset =',case1_load2.shape[0])\n",
    "#case1_load2.to_csv('../outputs/load_8760_2daytype_monthly_24hr.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: Season, single day-type, 24 hours (72 segments)\n",
    "#### Methodology: Use groupby function to group by season and hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Season_Group  Hour  Hour_Tot  Load_Tot      Load_Avg\n",
      "0  ERC_REST             1     1        90   2928629  32540.322222\n",
      "1  ERC_REST             1     2        90   2929331  32548.122222\n",
      "2  ERC_REST             1     3        90   2978696  33096.622222\n",
      "3  ERC_REST             1     4        90   3116896  34632.177778\n",
      "4  ERC_REST             1     5        90   3346197  37179.966667\n",
      "number of rows in dataset = 7560\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "\n",
      "           Date  DOW  Weekday  Season_Group  Hour_Tot  Load_Tot      Load_Avg  \n",
      "1545 2016-03-05    5    False             2       122   3603145  29533.975410  \n",
      "823  2016-02-04    3     True             2       122   3587428  29405.147541  \n",
      "1531 2016-03-04    4     True             2       122   3603145  29533.975410  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case2_load = load_dur2.copy()\n",
    "case2_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case2_seasons = case2_seasons.drop(['bimonthly'], axis=1)\n",
    "case2_seasons = case2_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "\n",
    "case2_load = pd.merge(case2_load, case2_seasons, on='Month', how='left')\n",
    "#print(case4_load)\n",
    "\n",
    "aggregations = {'Load':['count',sum,'mean']}\n",
    "case2 = case2_load.groupby(['Region','Season_Group','Hour'],as_index=False).agg(aggregations)\n",
    "case2.columns = case2.columns.droplevel(0)\n",
    "case2.columns = ['Region','Season_Group','Hour','Hour_Tot','Load_Tot','Load_Avg']\n",
    "print(case2.head())\n",
    "print('number of rows in dataset =',case2.shape[0])\n",
    "case2.to_csv('../outputs/load_segments_2daytype_season_24hr.csv')\n",
    "print()\n",
    "\n",
    "case2_load2 = pd.merge(case2_load,case2,on=['Region','Season_Group','Hour'],how='left')\n",
    "case2_load2 = case2_load2.sort_values(['Region','Load'])\n",
    "print(case2_load2.head(3))\n",
    "print('number of rows in dataset =',case2_load2.shape[0])\n",
    "case2_load2.to_csv('../outputs/load_8760_2daytype_season_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3: Monthly, weekend/weekday, 24 hours (576 segments)\n",
    "#### Metholodogy: Use groupby function to group by month, then weekend/weekday, then by 24 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Month  Hour  Hour_Tot  Load_Tot      Load_Avg\n",
      "0  ERC_REST      1     1        31   1042446  33627.290323\n",
      "1  ERC_REST      1     2        31   1051661  33924.548387\n",
      "2  ERC_REST      1     3        31   1079739  34830.290323\n",
      "3  ERC_REST      1     4        31   1144492  36919.096774\n",
      "4  ERC_REST      1     5        31   1239385  39980.161290\n",
      "number of rows in dataset = 24507\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1524  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "794   ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW  Weekday  Hour_Tot  Load_Tot      Load_Avg  \n",
      "1545 2016-03-05    5    False        14    406333  29023.785714  \n",
      "823  2016-02-04    3     True        30    870591  29019.700000  \n",
      "1531 2016-03-04    4     True        17    480375  28257.352941  \n",
      "1524 2016-03-04    4     True        17    480375  28257.352941  \n",
      "794  2016-02-03    2     True        31    875126  28229.870968  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case3_load = load_dur2.copy()\n",
    "\n",
    "aggregations = {'Load':['count',sum,'mean']}\n",
    "case3 = case3_load.groupby(['Region','Month','Weekday','Hour'],as_index=False).agg(aggregations)\n",
    "case3.columns = case3.columns.droplevel(0)\n",
    "case3.columns = ['Region','Month','Weekday','Hour','Hour_Tot','Load_Tot','Load_Avg']\n",
    "print(case1.head())\n",
    "print('number of rows in dataset =',case3.shape[0])\n",
    "#case3.to_csv('../outputs/load_segments_2daytype_monthly_wkd.csv')\n",
    "print()\n",
    "\n",
    "case3_load2 = pd.merge(case3_load,case3,on=['Region','Month','Weekday','Hour'],how='left')\n",
    "case3_load2 = case3_load2.sort_values(['Region','Load'])\n",
    "print(case3_load2.head())\n",
    "print('number of rows in dataset =',case3_load2.shape[0])\n",
    "#case3_load2.to_csv('../outputs/load_8760_2daytype_monthly_wkd.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 4: Season, weekend/weekday, 24 hours (144 segments)\n",
    "#### Methodology: groupby season, weekday, hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Season_Group  Weekday  Hour  Hour_Tot  Load_Tot      Load_Avg\n",
      "0  ERC_REST             1    False     1        40   1226650  30666.250000\n",
      "1  ERC_REST             1    False     2        30   1003087  33436.233333\n",
      "2  ERC_REST             1    False     4        52   1741883  33497.750000\n",
      "3  ERC_REST             1    False     5        18    669258  37181.000000\n",
      "4  ERC_REST             1    False     7        64   2446079  38219.984375\n",
      "number of rows in dataset = 12478\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1524  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "794   ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW  Weekday  Season_Group  Hour_Tot  Load_Tot      Load_Avg  \n",
      "1545 2016-03-05    5    False             2        63   1894236  30067.238095  \n",
      "823  2016-02-04    3     True             2        90   2653648  29484.977778  \n",
      "1531 2016-03-04    4     True             2        59   1708909  28964.559322  \n",
      "1524 2016-03-04    4     True             2        59   1708909  28964.559322  \n",
      "794  2016-02-03    2     True             2        90   2653648  29484.977778  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case4_load = load_dur2.copy()\n",
    "case4_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case4_seasons = case4_seasons.drop(['bimonthly'], axis=1)\n",
    "case4_seasons = case4_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "\n",
    "case4_load = pd.merge(case4_load, case4_seasons, on='Month', how='left')\n",
    "#print(case4_load)\n",
    "\n",
    "aggregations = {'Load':['count',sum,'mean']}\n",
    "case4 = case4_load.groupby(['Region','Season_Group','Weekday','Hour'],as_index=False).agg(aggregations)\n",
    "case4.columns = case4.columns.droplevel(0)\n",
    "case4.columns = ['Region','Season_Group','Weekday','Hour','Hour_Tot','Load_Tot','Load_Avg']\n",
    "print(case4.head())\n",
    "print('number of rows in dataset =',case4.shape[0])\n",
    "#case4.to_csv('../outputs/load_segments_2daytype_season_wkd.csv')\n",
    "print()\n",
    "\n",
    "case4_load2 = pd.merge(case4_load,case4,on=['Region','Season_Group','Weekday','Hour'],how='left')\n",
    "case4_load2 = case4_load2.sort_values(['Region','Load'])\n",
    "print(case4_load2.head())\n",
    "print('number of rows in dataset =',case4_load2.shape[0])\n",
    "#case4_load2.to_csv('../outputs/load_8760_2daytype_season_wkd.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 hour interval day types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Hour_Counter  4-hr\n",
      "0                1     1\n",
      "1                2     1\n",
      "2                3     1\n",
      "3                4     1\n",
      "4                5     2\n",
      "...            ...   ...\n",
      "8755          8756  2189\n",
      "8756          8757  2190\n",
      "8757          8758  2190\n",
      "8758          8759  2190\n",
      "8759          8760  2190\n",
      "\n",
      "[8760 rows x 2 columns]\n",
      "          Region R_Group R_Subgroup  Season  Month  Day  Hour   Load  \\\n",
      "0       ERC_REST     ERC       REST  winter      1    1     1  34807   \n",
      "1       ERC_REST     ERC       REST  winter      1    1     2  34551   \n",
      "2       ERC_REST     ERC       REST  winter      1    1     3  34788   \n",
      "3       ERC_REST     ERC       REST  winter      1    1     4  35531   \n",
      "4       ERC_REST     ERC       REST  winter      1    1     5  36633   \n",
      "...          ...     ...        ...     ...    ...  ...   ...    ...   \n",
      "551875  WEC_SDGE     WEC       SDGE  winter     12  365    20   2809   \n",
      "551876  WEC_SDGE     WEC       SDGE  winter     12  365    21   2675   \n",
      "551877  WEC_SDGE     WEC       SDGE  winter     12  365    22   2524   \n",
      "551878  WEC_SDGE     WEC       SDGE  winter     12  365    23   2362   \n",
      "551879  WEC_SDGE     WEC       SDGE  winter     12  365    24   2206   \n",
      "\n",
      "             Date  DOW  Weekday  Hour_Counter  4-hr  \n",
      "0      2016-01-01    4     True             1     1  \n",
      "1      2016-01-31    6    False             2     1  \n",
      "2      2016-03-01    1     True             3     1  \n",
      "3      2016-04-01    4     True             4     1  \n",
      "4      2016-05-01    6    False             5     2  \n",
      "...           ...  ...      ...           ...   ...  \n",
      "551875 2011-05-02    0     True          8756  2189  \n",
      "551876 2011-11-16    2     True          8757  2190  \n",
      "551877 2011-06-02    3     True          8758  2190  \n",
      "551878 2011-12-16    4     True          8759  2190  \n",
      "551879 2011-07-02    5    False          8760  2190  \n",
      "\n",
      "[551880 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "#read in 4 hour interval counter\n",
    "interval_4hr = pd.read_csv('inputs/sequential_hours.csv')\n",
    "interval_4hr = interval_4hr.drop(columns=['2-hr','8-hr','12-hr','24-hr','48-hr','120-hr'])\n",
    "print(interval_4hr)\n",
    "\n",
    "#add an hour counter\n",
    "load_dur2['Hour_Counter'] = (load_dur2['Hour']) + (load_dur2['Day'] - 1) * 24\n",
    "load_dur2 = load_dur2.sort_values(by=['Region','Hour_Counter'])\n",
    "unique_hc = pd.Series(load_dur2['Hour_Counter'].unique()).dropna()\n",
    "#print(unique_hc.tail(2))\n",
    "\n",
    "load_dur3 = pd.merge(load_dur2,interval_4hr,on='Hour_Counter',how='left')\n",
    "print(load_dur3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 5: Monthly, single day type, 4 hour intervals (72 segments)\n",
    "#### Methodology: use groupby by month, 4 hour intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case5_load = load_dur3.copy()\n",
    "\n",
    "aggregations = {'Load':['count',sum,'mean']}\n",
    "case5 = case5_load.groupby(['Region','Month','4-hr'],as_index=False).agg(aggregations)\n",
    "case5.columns = case5.columns.droplevel(0)\n",
    "case5.columns = ['Region','Month','4-hr','Hour_Tot','Load_Tot','Load_Avg']\n",
    "print(case5.head())\n",
    "print('number of rows in dataset =',case5.shape[0])\n",
    "#case5.to_csv('../outputs/load_segments_2daytype_month_4hr.csv')\n",
    "print()\n",
    "\n",
    "case5_load2 = pd.merge(case5_load,case5,on=['Region','Month','4-hr'],how='left')\n",
    "case5_load2 = case5_load2.sort_values(['Region','Load'])\n",
    "print(case5_load2.head())\n",
    "print('number of rows in dataset =',case5_load2.shape[0])\n",
    "#case5_load2.to_csv('../outputs/load_8760_2daytype_month_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 6: Bi-monthly weekend/weekday day-types, 4-hour intervals\n",
    "#### Methodology: use groupby function and bimonthly groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case6_load = load_dur3.copy()\n",
    "case6_bimonth = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case6_bimonth = case6_bimonth.drop(['seasonal'], axis=1)\n",
    "case6_bimonth = case6_bimonth.rename(columns={'bimonthly':'Bimonth','month':'Month'})\n",
    "\n",
    "case6_load = pd.merge(case6_load, case6_bimonth, on='Month', how='left')\n",
    "#print(case4_load)\n",
    "\n",
    "aggregations = {'Load':['count',sum,'mean']}\n",
    "case6 = case6_load.groupby(['Region','Bimonth','4-hr'],as_index=False).agg(aggregations)\n",
    "case6.columns = case6.columns.droplevel(0)\n",
    "case6.columns = ['Region','Bimonth','4-hr','Hour_Tot','Load_Tot','Load_Avg']\n",
    "print(case6.head())\n",
    "print('number of rows in dataset =',case6.shape[0])\n",
    "#case6.to_csv('../outputs/load_segments_2daytype_bimonth_4hr.csv')\n",
    "print()\n",
    "\n",
    "case6_load2 = pd.merge(case6_load,case6,on=['Region','Bimonth','4-hr'],how='left')\n",
    "case6_load2 = case6_load2.sort_values(['Region','Load'])\n",
    "print(case6_load2.head())\n",
    "print('number of rows in dataset =',case6_load2.shape[0])\n",
    "#case6_load2.to_csv('../outputs/load_8760_2daytype_bimonth_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 7: Season-based months, weekend/weekday day-types, 4-hour intervals\n",
    "#### Methodology: groupby function and applied season groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case7_load = load_dur3.copy()\n",
    "case7_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case7_seasons = case7_seasons.drop(['bimonthly'], axis=1)\n",
    "case7_seasons = case7_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "\n",
    "case7_load = pd.merge(case7_load, case7_seasons, on='Month', how='left')\n",
    "#print(case4_load)\n",
    "\n",
    "aggregations = {'Load':['count',sum,'mean']}\n",
    "case7 = case7_load.groupby(['Region','Season','Weekday','4-hr'],as_index=False).agg(aggregations)\n",
    "case7.columns = case7.columns.droplevel(0)\n",
    "case7.columns = ['Region','Season','Weekday','4-hr','Hour_Tot','Load_Tot','Load_Avg']\n",
    "print(case7.head())\n",
    "print('number of rows in dataset =',case7.shape[0])\n",
    "case7.to_csv('../outputs/load_segments_2daytype_season_wkd_4hr.csv')\n",
    "print()\n",
    "\n",
    "case7_load2 = pd.merge(case7_load,case7,on=['Region','Season','Weekday','4-hr'],how='left')\n",
    "case7_load2 = case7_load2.sort_values(['Region','Load'])\n",
    "print(case7_load2.head())\n",
    "print('number of rows in dataset =',case7_load2.shape[0])\n",
    "case7_load2.to_csv('../outputs/load_8760_2daytype_season_wkd_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day type: Three (Weekday, Weekend, Peak Load)\n",
    "## Find Peak Load Days in Each Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Region R_Group R_Subgroup  Season  Month  Day  Hour   Load  \\\n",
      "0       ERC_REST     ERC       REST  winter      1    1     1  34807   \n",
      "1       ERC_REST     ERC       REST  winter      1    2     1  34716   \n",
      "2       ERC_REST     ERC       REST  winter      1    3     1  34736   \n",
      "3       ERC_REST     ERC       REST  winter      1    4     1  35914   \n",
      "4       ERC_REST     ERC       REST  winter      1    5     1  33845   \n",
      "...          ...     ...        ...     ...    ...  ...   ...    ...   \n",
      "551875   WECC_WY     WEC         WY  winter     12  361    24   1767   \n",
      "551876   WECC_WY     WEC         WY  winter     12  362    24   1744   \n",
      "551877   WECC_WY     WEC         WY  winter     12  363    24   1805   \n",
      "551878   WECC_WY     WEC         WY  winter     12  364    24   1791   \n",
      "551879   WECC_WY     WEC         WY  winter     12  365    24   1834   \n",
      "\n",
      "        Load_Tot  Load_Max  \n",
      "0         917588   1000605  \n",
      "1         863132   1000605  \n",
      "2         934513   1000605  \n",
      "3         960576   1000605  \n",
      "4         916216   1000605  \n",
      "...          ...       ...  \n",
      "551875     51381     56554  \n",
      "551876     49456     56554  \n",
      "551877     48194     56554  \n",
      "551878     47653     56554  \n",
      "551879     47420     56554  \n",
      "\n",
      "[551880 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "#create temporary DF to find peak\n",
    "test = pd.read_csv('outputs/load_long_format.csv')\n",
    "\n",
    "#groupby region, month, and day to sum the total day load\n",
    "aggregations1 = {'Load':sum}\n",
    "test_sum = test.groupby(['Region','Month','Day'],as_index=False).agg(aggregations1)\n",
    "#test1.columns = test1.columns.droplevel(0)\n",
    "test_sum.columns = ['Region','Month','Day','Load_Tot']\n",
    "#print(test_sum.head())\n",
    "#print('number of rows in dataset =',test_sum.shape[0])\n",
    "\n",
    "test3 = pd.merge(test,test_sum,on=['Region','Month','Day'],how='left')\n",
    "#print(test3)\n",
    "\n",
    "#groupby region and month to find maximum load\n",
    "aggregations2 = {'Load_Tot':max}\n",
    "test_max = test_sum.groupby(['Region','Month'],as_index=False).agg(aggregations2)\n",
    "test_max.columns = ['Region','Month','Load_Max']\n",
    "#print(test_max.head())\n",
    "\n",
    "test4 = pd.merge(test3,test_max,on=['Region','Month'],how='left')\n",
    "test4 = test4.drop(test4.columns[0], axis=1)\n",
    "print(test4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Region R_Group R_Subgroup  Season  Month  Day  Hour   Load  \\\n",
      "0       ERC_REST     ERC       REST  winter      1    1     1  34807   \n",
      "1       ERC_REST     ERC       REST  winter      1    2     1  34716   \n",
      "2       ERC_REST     ERC       REST  winter      1    3     1  34736   \n",
      "3       ERC_REST     ERC       REST  winter      1    4     1  35914   \n",
      "4       ERC_REST     ERC       REST  winter      1    5     1  33845   \n",
      "...          ...     ...        ...     ...    ...  ...   ...    ...   \n",
      "551875   WECC_WY     WEC         WY  winter     12  361    24   1767   \n",
      "551876   WECC_WY     WEC         WY  winter     12  362    24   1744   \n",
      "551877   WECC_WY     WEC         WY  winter     12  363    24   1805   \n",
      "551878   WECC_WY     WEC         WY  winter     12  364    24   1791   \n",
      "551879   WECC_WY     WEC         WY  winter     12  365    24   1834   \n",
      "\n",
      "             Date  DOW Day_Type  \n",
      "0      2016-01-01    4  Weekday  \n",
      "1      2016-01-01    4  Weekday  \n",
      "2      2016-01-01    4  Weekday  \n",
      "3      2016-01-01    4  Weekday  \n",
      "4      2016-01-01    4  Weekday  \n",
      "...           ...  ...      ...  \n",
      "551875 2011-12-31    5  Weekend  \n",
      "551876 2011-12-31    5  Weekend  \n",
      "551877 2011-12-31    5  Weekend  \n",
      "551878 2011-12-31    5  Weekend  \n",
      "551879 2011-12-31    5  Weekend  \n",
      "\n",
      "[551880 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "three_load = pd.read_csv('outputs/load_long_format.csv', index_col=0)\n",
    "load_years = pd.read_csv('inputs/load_years.csv').dropna()\n",
    "#print(load_dur1.head())\n",
    "#print(load_years)\n",
    "\n",
    "from itertools import cycle\n",
    "\n",
    "#repeat the date for 24 times for each hour in the day\n",
    "year_2016 = load_years['2016'].repeat(24).reset_index(drop=True)\n",
    "year_2011 = load_years['2011'].repeat(24).reset_index(drop=True)\n",
    "#print(year_2016)\n",
    "\n",
    "#use itertools.cycle to make the date repeat until it reaches the end of the dataframe\n",
    "year_2016 = cycle(year_2016)\n",
    "year_2011 = cycle(year_2011)\n",
    "\n",
    "#create temporary data frames where they will iterate through. Needed to achieve the correct length of the DF\n",
    "temp2016 = three_load.loc[three_load['R_Group'] == 'ERC'].copy()\n",
    "temp2011 = three_load.loc[three_load['R_Group'] != 'ERC'].copy()\n",
    "\n",
    "#iterate through the date until the end of DF \n",
    "temp2016['Date'] = [next(year_2016) for year in range(len(temp2016))]\n",
    "temp2011['Date'] = [next(year_2011) for year in range(len(temp2011))]\n",
    "#print(temp2016)\n",
    "#print(temp2011)\n",
    "\n",
    "three_load2 = pd.concat([temp2016, temp2011], ignore_index=True)\n",
    "#print(load_dur2)\n",
    "\n",
    "#convert date to a datetime type \n",
    "three_load2['Date'] = pd.to_datetime(three_load2['Date'])\n",
    "three_load2['DOW'] = three_load2['Date'].dt.weekday\n",
    "\n",
    "#check if it is a weekday or not \n",
    "weekday = pd.read_csv('inputs/weekday.csv')\n",
    "three_load2 = pd.merge(three_load2,weekday,on='DOW',how='left')\n",
    "#print(load_dur2)\n",
    "\n",
    "three_load3 = pd.merge(three_load2,test4,on=['Region','R_Group','R_Subgroup','Season','Month','Day',\n",
    "                                         'Hour','Load'],how='left')\n",
    "three_load3 = three_load3.rename(columns={'Weekday':'Day_Type'})\n",
    "#print(three_load3)\n",
    "\n",
    "#Return True if the load total equals the day identified as the max\n",
    "three_load3.loc[three_load3['Day_Type'] == True, 'Day_Type'] = 'Weekday'\n",
    "three_load3.loc[three_load3['Day_Type'] == False, 'Day_Type'] = 'Weekend'\n",
    "three_load3.loc[three_load3['Load_Tot'] == three_load3['Load_Max'], 'Day_Type'] = 'Peak'\n",
    "three_load3 = three_load3.drop(['Load_Tot','Load_Max'], axis=1)\n",
    "print(three_load3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Monthly, Weekend/weekday/peak day-types, 24 hours (864 segments)\n",
    "#### Methodology: similar to two day type, just adding in peak day types to sort by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case1_load = three_load3.copy()\n",
    "\n",
    "aggregations = {'Load':['count',sum,'mean']}\n",
    "case1 = case1_load.groupby(['Region','Month','Day_Type','Hour'],as_index=False).agg(aggregations)\n",
    "case1.columns = case1.columns.droplevel(0)\n",
    "case1.columns = ['Region','Month','Day_Type','Hour','Hour_Tot','Load_Tot','Load_Avg']\n",
    "print(case1.head())\n",
    "print('number of rows in dataset =',case1.shape[0])\n",
    "case1.to_csv('../outputs/load_segments_3daytype_monthly_24hr.csv')\n",
    "print()\n",
    "\n",
    "case1_load2 = pd.merge(case1_load,case1,on=['Region','Month','Day_Type','Hour'],how='left')\n",
    "case1_load2 = case1_load2.sort_values(['Region','Load'])\n",
    "print(case1_load2.head(3))\n",
    "print('number of rows in dataset =',case1_load2.shape[0])\n",
    "case1_load2.to_csv('../outputs/load_8760_3daytype_monthly_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: Season, weekend/weekday/peak day-types, 24-hours (216 segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case2_load = three_load3.copy()\n",
    "\n",
    "aggregations = {'Load':['count',sum,'mean']}\n",
    "case2 = case2_load.groupby(['Region','Season','Day_Type','Hour'],as_index=False).agg(aggregations)\n",
    "case2.columns = case2.columns.droplevel(0)\n",
    "case2.columns = ['Region','Season','Day_Type','Hour','Hour_Tot','Load_Tot','Load_Avg']\n",
    "print(case2.head())\n",
    "print('number of rows in dataset =',case2.shape[0])\n",
    "case2.to_csv('../outputs/load_segments_3daytype_season_24hr.csv')\n",
    "print()\n",
    "\n",
    "case2_load2 = pd.merge(case2_load,case2,on=['Region','Season','Day_Type','Hour'],how='left')\n",
    "case2_load2 = case2_load2.sort_values(['Region','Load'])\n",
    "print(case1_load2.head(3))\n",
    "print('number of rows in dataset =',case1_load2.shape[0])\n",
    "case2_load2.to_csv('../outputs/load_8760_3daytype_season_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3: Annual, weekend/weekday/peak day-types, 24-hours (72 segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case3_load = three_load3.copy()\n",
    "\n",
    "aggregations = {'Load':['count',sum,'mean']}\n",
    "case3 = case3_load.groupby(['Region','Day_Type','Hour'],as_index=False).agg(aggregations)\n",
    "case3.columns = case3.columns.droplevel(0)\n",
    "case3.columns = ['Region','Day_Type','Hour','Hour_Tot','Load_Tot','Load_Avg']\n",
    "print(case3.head())\n",
    "print('number of rows in dataset =',case3.shape[0])\n",
    "case3.to_csv('../outputs/load_segments_3daytype_annual_24hr.csv')\n",
    "print()\n",
    "\n",
    "case3_load2 = pd.merge(case3_load,case3,on=['Region','Day_Type','Hour'],how='left')\n",
    "case3_load2 = case3_load2.sort_values(['Region','Load'])\n",
    "print(case3_load2.head(3))\n",
    "print('number of rows in dataset =',case3_load2.shape[0])\n",
    "case3_load2.to_csv('../outputs/load_8760_3daytype_annual_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 hour interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Hour_Counter  4-hr\n",
      "0                1     1\n",
      "1                2     1\n",
      "2                3     1\n",
      "3                4     1\n",
      "4                5     2\n",
      "...            ...   ...\n",
      "8755          8756  2189\n",
      "8756          8757  2190\n",
      "8757          8758  2190\n",
      "8758          8759  2190\n",
      "8759          8760  2190\n",
      "\n",
      "[8760 rows x 2 columns]\n",
      "          Region R_Group R_Subgroup  Season  Month  Day  Hour   Load  \\\n",
      "0       ERC_REST     ERC       REST  winter      1    1     1  34807   \n",
      "1       ERC_REST     ERC       REST  winter      1    1     2  34551   \n",
      "2       ERC_REST     ERC       REST  winter      1    1     3  34788   \n",
      "3       ERC_REST     ERC       REST  winter      1    1     4  35531   \n",
      "4       ERC_REST     ERC       REST  winter      1    1     5  36633   \n",
      "...          ...     ...        ...     ...    ...  ...   ...    ...   \n",
      "551875  WEC_SDGE     WEC       SDGE  winter     12  365    20   2809   \n",
      "551876  WEC_SDGE     WEC       SDGE  winter     12  365    21   2675   \n",
      "551877  WEC_SDGE     WEC       SDGE  winter     12  365    22   2524   \n",
      "551878  WEC_SDGE     WEC       SDGE  winter     12  365    23   2362   \n",
      "551879  WEC_SDGE     WEC       SDGE  winter     12  365    24   2206   \n",
      "\n",
      "             Date  DOW  Weekday  Hour_Counter  4-hr  \n",
      "0      2016-01-01    4     True             1     1  \n",
      "1      2016-01-31    6    False             2     1  \n",
      "2      2016-03-01    1     True             3     1  \n",
      "3      2016-04-01    4     True             4     1  \n",
      "4      2016-05-01    6    False             5     2  \n",
      "...           ...  ...      ...           ...   ...  \n",
      "551875 2011-05-02    0     True          8756  2189  \n",
      "551876 2011-11-16    2     True          8757  2190  \n",
      "551877 2011-06-02    3     True          8758  2190  \n",
      "551878 2011-12-16    4     True          8759  2190  \n",
      "551879 2011-07-02    5    False          8760  2190  \n",
      "\n",
      "[551880 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "#read in 4 hour interval counter\n",
    "interval_4hr = pd.read_csv('inputs/sequential_hours.csv')\n",
    "interval_4hr = interval_4hr.drop(columns=['2-hr','8-hr','12-hr','24-hr','48-hr','120-hr'])\n",
    "print(interval_4hr)\n",
    "\n",
    "#add an hour counter\n",
    "three_load3['Hour_Counter'] = (three_load3['Hour']) + (three_load3['Day'] - 1) * 24\n",
    "three_load3 = three_load3.sort_values(by=['Region','Hour_Counter'])\n",
    "unique_hc = pd.Series(three_load3['Hour_Counter'].unique()).dropna()\n",
    "#print(unique_hc.tail(2))\n",
    "\n",
    "three_load4 = pd.merge(three_load3,interval_4hr,on='Hour_Counter',how='left')\n",
    "print(load_dur3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 4: Bi-monthly, weekend/weekday/peak day-types, 4-hour intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Region R_Group R_Subgroup  Season  Month  Day  Hour   Load  \\\n",
      "0       ERC_REST     ERC       REST  winter      1    1     1  34807   \n",
      "1       ERC_REST     ERC       REST  winter      1    1     2  34551   \n",
      "2       ERC_REST     ERC       REST  winter      1    1     3  34788   \n",
      "3       ERC_REST     ERC       REST  winter      1    1     4  35531   \n",
      "4       ERC_REST     ERC       REST  winter      1    1     5  36633   \n",
      "...          ...     ...        ...     ...    ...  ...   ...    ...   \n",
      "551875  WEC_SDGE     WEC       SDGE  winter     12  365    20   2809   \n",
      "551876  WEC_SDGE     WEC       SDGE  winter     12  365    21   2675   \n",
      "551877  WEC_SDGE     WEC       SDGE  winter     12  365    22   2524   \n",
      "551878  WEC_SDGE     WEC       SDGE  winter     12  365    23   2362   \n",
      "551879  WEC_SDGE     WEC       SDGE  winter     12  365    24   2206   \n",
      "\n",
      "             Date  DOW Day_Type  Hour_Counter  4-hr  Bimonth  \n",
      "0      2016-01-01    4  Weekday             1     1        1  \n",
      "1      2016-01-31    6  Weekend             2     1        1  \n",
      "2      2016-03-01    1  Weekday             3     1        1  \n",
      "3      2016-04-01    4  Weekday             4     1        1  \n",
      "4      2016-05-01    6  Weekend             5     2        1  \n",
      "...           ...  ...      ...           ...   ...      ...  \n",
      "551875 2011-05-02    0  Weekday          8756  2189        6  \n",
      "551876 2011-11-16    2  Weekday          8757  2190        6  \n",
      "551877 2011-06-02    3  Weekday          8758  2190        6  \n",
      "551878 2011-12-16    4  Weekday          8759  2190        6  \n",
      "551879 2011-07-02    5  Weekend          8760  2190        6  \n",
      "\n",
      "[551880 rows x 14 columns]\n",
      "     Region  Bimonth Day_Type  4-hr  Hour_Tot  Load_Tot  Load_Avg\n",
      "0  ERC_REST        1     Peak    55         4    161100  40275.00\n",
      "1  ERC_REST        1     Peak    56         4    187905  46976.25\n",
      "2  ERC_REST        1     Peak    57         4    168812  42203.00\n",
      "3  ERC_REST        1     Peak    58         4    155705  38926.25\n",
      "4  ERC_REST        1     Peak    59         4    172847  43211.75\n",
      "number of rows in dataset = 243586\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "2042  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "2233  ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1706  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1538  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "1537  ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW Day_Type  Hour_Counter  4-hr  Bimonth  Hour_Tot  \\\n",
      "2042 2016-03-05    5  Weekend          2043   511        2         1   \n",
      "2233 2016-02-04    3  Weekday          2234   559        2         3   \n",
      "1706 2016-03-04    4  Weekday          1707   427        2         3   \n",
      "1538 2016-03-04    4  Weekday          1539   385        2         2   \n",
      "1537 2016-02-03    2  Weekday          1538   385        2         2   \n",
      "\n",
      "      Load_Tot      Load_Avg  \n",
      "2042     26989  26989.000000  \n",
      "2233     83494  27831.333333  \n",
      "1706     81445  27148.333333  \n",
      "1538     54128  27064.000000  \n",
      "1537     54128  27064.000000  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case4_load = three_load4.copy()\n",
    "case4_bimonth = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case4_bimonth = case4_bimonth.drop(['seasonal'], axis=1)\n",
    "case4_bimonth = case4_bimonth.rename(columns={'bimonthly':'Bimonth','month':'Month'})\n",
    "\n",
    "case4_load = pd.merge(case4_load, case4_bimonth, on='Month', how='left')\n",
    "print(case4_load)\n",
    "\n",
    "aggregations = {'Load':['count',sum,'mean']}\n",
    "case4 = case4_load.groupby(['Region','Bimonth','Day_Type','4-hr'],as_index=False).agg(aggregations)\n",
    "case4.columns = case4.columns.droplevel(0)\n",
    "case4.columns = ['Region','Bimonth','Day_Type','4-hr','Hour_Tot','Load_Tot','Load_Avg']\n",
    "print(case4.head())\n",
    "print('number of rows in dataset =',case4.shape[0])\n",
    "#case4.to_csv('../outputs/load_segments_3daytype_bimonth_4hr.csv')\n",
    "print()\n",
    "\n",
    "case4_load2 = pd.merge(case4_load,case4,on=['Region','Bimonth','Day_Type','4-hr'],how='left')\n",
    "case4_load2 = case4_load2.sort_values(['Region','Load'])\n",
    "print(case4_load2.head())\n",
    "print('number of rows in dataset =',case4_load2.shape[0])\n",
    "#case4_load2.to_csv('../outputs/load_8760_3daytype_bimonth_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 5: Season-based months, weekend/weekday/peak day-types, 4-hour intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Season_Group Day_Type  4-hr  Hour_Tot  Load_Tot  Load_Avg\n",
      "0  ERC_REST             1     Peak    55         4    161100  40275.00\n",
      "1  ERC_REST             1     Peak    56         4    187905  46976.25\n",
      "2  ERC_REST             1     Peak    57         4    168812  42203.00\n",
      "3  ERC_REST             1     Peak    58         4    155705  38926.25\n",
      "4  ERC_REST             1     Peak    59         4    172847  43211.75\n",
      "number of rows in dataset = 243586\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "2042  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "2233  ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1706  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1538  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "1537  ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW Day_Type  Hour_Counter  4-hr  Season_Group  Hour_Tot  \\\n",
      "2042 2016-03-05    5  Weekend          2043   511             2         1   \n",
      "2233 2016-02-04    3  Weekday          2234   559             2         3   \n",
      "1706 2016-03-04    4  Weekday          1707   427             2         3   \n",
      "1538 2016-03-04    4  Weekday          1539   385             2         2   \n",
      "1537 2016-02-03    2  Weekday          1538   385             2         2   \n",
      "\n",
      "      Load_Tot      Load_Avg  \n",
      "2042     26989  26989.000000  \n",
      "2233     83494  27831.333333  \n",
      "1706     81445  27148.333333  \n",
      "1538     54128  27064.000000  \n",
      "1537     54128  27064.000000  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case5_load = three_load4.copy()\n",
    "case5_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case5_seasons = case5_seasons.drop(['bimonthly'], axis=1)\n",
    "case5_seasons = case5_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "\n",
    "case5_load = pd.merge(case5_load, case5_seasons, on='Month', how='left')\n",
    "#print(case5_load)\n",
    "\n",
    "aggregations = {'Load':['count',sum,'mean']}\n",
    "case5 = case5_load.groupby(['Region','Season_Group','Day_Type','4-hr'],as_index=False).agg(aggregations)\n",
    "case5.columns = case5.columns.droplevel(0)\n",
    "case5.columns = ['Region','Season_Group','Day_Type','4-hr','Hour_Tot','Load_Tot','Load_Avg']\n",
    "print(case5.head())\n",
    "print('number of rows in dataset =',case5.shape[0])\n",
    "#case5.to_csv('../outputs/load_segments_3daytype_seasonbased_4hr.csv')\n",
    "print()\n",
    "\n",
    "case5_load2 = pd.merge(case5_load,case5,on=['Region','Season_Group','Day_Type','4-hr'],how='left')\n",
    "case5_load2 = case5_load2.sort_values(['Region','Load'])\n",
    "print(case5_load2.head())\n",
    "print('number of rows in dataset =',case5_load2.shape[0])\n",
    "#case5_load2.to_csv('../outputs/load_8760_3daytype_seasonbased_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 6: Season, weekend/weekday/peak day-types, 4-hour intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region    Season Day_Type  4-hr  Hour_Tot  Load_Tot  Load_Avg\n",
      "0  ERC_REST  shoulder     Peak   535         4    120877  30219.25\n",
      "1  ERC_REST  shoulder     Peak   536         4    142982  35745.50\n",
      "2  ERC_REST  shoulder     Peak   537         4    155141  38785.25\n",
      "3  ERC_REST  shoulder     Peak   538         4    170205  42551.25\n",
      "4  ERC_REST  shoulder     Peak   539         4    165142  41285.50\n",
      "number of rows in dataset = 243586\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "2042  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "2233  ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1706  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1538  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "1537  ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW Day_Type  Hour_Counter  4-hr  Hour_Tot  Load_Tot  \\\n",
      "2042 2016-03-05    5  Weekend          2043   511         1     26989   \n",
      "2233 2016-02-04    3  Weekday          2234   559         3     83494   \n",
      "1706 2016-03-04    4  Weekday          1707   427         3     81445   \n",
      "1538 2016-03-04    4  Weekday          1539   385         2     54128   \n",
      "1537 2016-02-03    2  Weekday          1538   385         2     54128   \n",
      "\n",
      "          Load_Avg  \n",
      "2042  26989.000000  \n",
      "2233  27831.333333  \n",
      "1706  27148.333333  \n",
      "1538  27064.000000  \n",
      "1537  27064.000000  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case6_load = three_load4.copy()\n",
    "\n",
    "aggregations = {'Load':['count',sum,'mean']}\n",
    "case6 = case6_load.groupby(['Region','Season','Day_Type','4-hr'],as_index=False).agg(aggregations)\n",
    "case6.columns = case6.columns.droplevel(0)\n",
    "case6.columns = ['Region','Season','Day_Type','4-hr','Hour_Tot','Load_Tot','Load_Avg']\n",
    "print(case6.head())\n",
    "print('number of rows in dataset =',case6.shape[0])\n",
    "#case6.to_csv('../outputs/load_segments_3daytype_season_4hr.csv')\n",
    "print()\n",
    "\n",
    "case6_load2 = pd.merge(case6_load,case6,on=['Region','Season','Day_Type','4-hr'],how='left')\n",
    "case6_load2 = case6_load2.sort_values(['Region','Load'])\n",
    "print(case6_load2.head())\n",
    "print('number of rows in dataset =',case6_load2.shape[0])\n",
    "#case6_load2.to_csv('../outputs/load_8760_3daytype_season_4hr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
