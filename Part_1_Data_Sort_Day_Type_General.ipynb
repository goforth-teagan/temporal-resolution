{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data here: \n",
    "## One and two day type \n",
    "## Three day type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day Type: One/two\n",
    "## Create Date and Day of Week columns in the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output files are written out in parent directory: C:\\Users\\cmarcy\\Desktop\\py_projects\\temporal\\temporal_working\\outputs\n"
     ]
    }
   ],
   "source": [
    "#importing packages needed for analysis\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from pandas import DataFrame\n",
    "\n",
    "path = os.getcwd()\n",
    "#print(path)\n",
    "\n",
    "#this code creates an output directory in the parent director, if one does not exist yet\n",
    "#Note: this is where all of the output files will be written, since outputs are large this saves space in git\n",
    "path = os.getcwd()\n",
    "parent = os.path.dirname(path)\n",
    "outputs_dir = parent+'\\outputs'\n",
    "if not os.path.exists(outputs_dir):\n",
    "    os.makedirs(outputs_dir)\n",
    "print('output files are written out in parent directory: '+outputs_dir)\n",
    "\n",
    "load_dur = pd.read_csv('../outputs/load_long_format.csv')\n",
    "solar_dur = pd.read_csv('../outputs/solar_long_format.csv')\n",
    "wind_dur = pd.read_csv('../outputs/wind_long_format.csv')\n",
    "\n",
    "## UNCOMMENT WHICH PROFILE TO BE USED\n",
    "x = load_dur\n",
    "x_name = 'load'\n",
    "x_name2 = 'Load'\n",
    "x_column = 'Load'\n",
    "\n",
    "#x = solar_dur\n",
    "#x_name = 'solar'\n",
    "#x_name2 = 'Solar_Gen'\n",
    "#choose TRG \n",
    "#x_column = 'TRG6'\n",
    "\n",
    "#x = wind_dur\n",
    "#x_name = 'wind'\n",
    "#x_name2 = 'Wind_Gen'\n",
    "#x_column = 'TRG4'\n",
    "\n",
    "x = x[['Region','R_Group','R_Subgroup','Season','Month','Day','Hour',x_column]]\n",
    "years = pd.read_csv('inputs/years.csv').dropna()\n",
    "\n",
    "#print(x.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region R_Group R_Subgroup  Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "0   FRCC    FRCC       FRCC  winter      1    1     1  17005    1  2011   \n",
      "1   FRCC    FRCC       FRCC  winter      1    2     1  15750    2  2011   \n",
      "2   FRCC    FRCC       FRCC  winter      1    3     1  16123    3  2011   \n",
      "3   FRCC    FRCC       FRCC  winter      1    4     1  18300    4  2011   \n",
      "4   FRCC    FRCC       FRCC  winter      1    5     1  17370    5  2011   \n",
      "\n",
      "        Date  DOW  Weekday  \n",
      "0 2011-01-01    5    False  \n",
      "1 2011-01-02    6    False  \n",
      "2 2011-01-03    0     True  \n",
      "3 2011-01-04    1     True  \n",
      "4 2011-01-05    2     True  \n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "daydata = pd.read_csv('inputs/days_365.csv')\n",
    "#print(daydata.tail())\n",
    "daydata = daydata.drop(columns='Month')\n",
    "x2 = pd.merge(x,daydata,on=['Day'],how='left')\n",
    "#print(x2.tail())\n",
    "\n",
    "#sets the year for each region\n",
    "x2['Year']=2011\n",
    "x2.loc[x2['R_Group'] == 'ERC', 'Year'] = 2016\n",
    "#print(x2.head())\n",
    "\n",
    "#Creates a date column\n",
    "x2 = x2.rename(columns={'Day':'DOY','DayofMo':'Day'})\n",
    "x2['Date']=pd.to_datetime(x2[['Year', 'Month', 'Day']], errors='coerce')\n",
    "#print(x2.tail())\n",
    "\n",
    "#convert date to a datetime type \n",
    "#x2['Date'] = pd.to_datetime(x2['Date'])\n",
    "x2['DOW'] = x2['Date'].dt.weekday\n",
    "\n",
    "#check if it is a weekday or not \n",
    "weekday = pd.read_csv('inputs/weekday.csv')\n",
    "x2 = pd.merge(x2,weekday,on='DOW',how='left')\n",
    "print(x2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Monthly, single day type, 24 hours (288 segments)\n",
    "#### Methodology: Using groupby function to group first by month, then by 24 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region  Month  Hour  Hour_Tot  Load_Tot      Load_Avg\n",
      "0   FRCC      1     1        31    626801  20219.387097\n",
      "1   FRCC      1     2        31    644817  20800.548387\n",
      "2   FRCC      1     3        31    694961  22418.096774\n",
      "3   FRCC      1     4        31    781019  25194.161290\n",
      "4   FRCC      1     5        31    836184  26973.677419\n",
      "number of rows in dataset = 288\n",
      "\n",
      "    R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "366    FRCC       FRCC    winter      1    2     2  15694    2  2011   \n",
      "73     FRCC       FRCC  shoulder      3   74     1  15738   15  2011   \n",
      "1      FRCC       FRCC    winter      1    2     1  15750    2  2011   \n",
      "\n",
      "          Date  DOW  Weekday  Hour_Tot  Load_Tot      Load_Avg  \n",
      "366 2011-01-02    6    False        31    644817  20800.548387  \n",
      "73  2011-03-15    1     True        31    519021  16742.612903  \n",
      "1   2011-01-02    6    False        31    626801  20219.387097  \n",
      "number of rows in dataset = 8760\n"
     ]
    }
   ],
   "source": [
    "case1_x = x2.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case1 = case1_x.groupby(['Region','Month','Hour'],as_index=False).agg(aggregations)\n",
    "case1.columns = case1.columns.droplevel(0)\n",
    "case1.columns = ['Region','Month','Hour','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case1.head())\n",
    "print('number of rows in dataset =',case1.shape[0])\n",
    "case1.to_csv('../outputs/'+x_name+'_segments_1daytype_monthly_24hr.csv')\n",
    "print()\n",
    "\n",
    "case1_x2 = pd.merge(case1_x,case1,on=['Region','Month','Hour'],how='left')\n",
    "case1_x2 = case1_x2.sort_values(['Region',x_column]).drop(case1_x2.columns[0], axis=1)\n",
    "print(case1_x2.head(3))\n",
    "print('number of rows in dataset =',case1_x2.shape[0])\n",
    "case1_x2.to_csv('../outputs/'+x_name+'_8760_1daytype_monthly_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: Season, single day-type, 24 hours (72 segments)\n",
    "#### Methodology: Use groupby function to group by season and hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region  Season_Group  Hour  Hour_Tot  Load_Tot      Load_Avg\n",
      "0   FRCC             1     1        90   1659148  18434.977778\n",
      "1   FRCC             1     2        90   1694864  18831.822222\n",
      "2   FRCC             1     3        90   1817817  20197.966667\n",
      "3   FRCC             1     4        90   2039870  22665.222222\n",
      "4   FRCC             1     5        90   2183989  24266.544444\n",
      "number of rows in dataset = 120\n",
      "\n",
      "    R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "366    FRCC       FRCC    winter      1    2     2  15694    2  2011   \n",
      "73     FRCC       FRCC  shoulder      3   74     1  15738   15  2011   \n",
      "1      FRCC       FRCC    winter      1    2     1  15750    2  2011   \n",
      "\n",
      "          Date  DOW  Weekday  Season_Group  Hour_Tot  Load_Tot      Load_Avg  \n",
      "366 2011-01-02    6    False             1        90   1694864  18831.822222  \n",
      "73  2011-03-15    1     True             2       122   2159594  17701.590164  \n",
      "1   2011-01-02    6    False             1        90   1659148  18434.977778  \n",
      "number of rows in dataset = 8760\n"
     ]
    }
   ],
   "source": [
    "case2_x = x2.copy()\n",
    "case2_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case2_seasons = case2_seasons.drop(['bimonthly'], axis=1)\n",
    "case2_seasons = case2_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "#print(case2_seasons)\n",
    "\n",
    "case2_x = pd.merge(case2_x, case2_seasons, on='Month', how='left')\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case2 = case2_x.groupby(['Region','Season_Group','Hour'],as_index=False).agg(aggregations)\n",
    "case2.columns = case2.columns.droplevel(0)\n",
    "case2.columns = ['Region','Season_Group','Hour','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case2.head())\n",
    "print('number of rows in dataset =',case2.shape[0])\n",
    "case2.to_csv('../outputs/'+x_name+'_segments_1daytype_season_24hr.csv')\n",
    "print()\n",
    "\n",
    "case2_x2 = pd.merge(case2_x,case2,on=['Region','Season_Group','Hour'],how='left')\n",
    "case2_x2 = case2_x2.sort_values(['Region',x_column]).drop(case2_x2.columns[0],axis=1)\n",
    "print(case2_x2.head(3))\n",
    "print('number of rows in dataset =',case2_x2.shape[0])\n",
    "case2_x2.to_csv('../outputs/'+x_name+'_8760_1daytype_season_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3: Monthly, weekend/weekday, 24 hours (576 segments)\n",
    "#### Metholodogy: Use groupby function to group by month, then weekend/weekday, then by 24 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region  Month  Hour  Hour_Tot  Load_Tot      Load_Avg\n",
      "0   FRCC      1     1        31    626801  20219.387097\n",
      "1   FRCC      1     2        31    644817  20800.548387\n",
      "2   FRCC      1     3        31    694961  22418.096774\n",
      "3   FRCC      1     4        31    781019  25194.161290\n",
      "4   FRCC      1     5        31    836184  26973.677419\n",
      "number of rows in dataset = 576\n",
      "\n",
      "    R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "366    FRCC       FRCC    winter      1    2     2  15694    2  2011   \n",
      "73     FRCC       FRCC  shoulder      3   74     1  15738   15  2011   \n",
      "1      FRCC       FRCC    winter      1    2     1  15750    2  2011   \n",
      "443    FRCC       FRCC  shoulder      3   79     2  15822   20  2011   \n",
      "72     FRCC       FRCC  shoulder      3   73     1  15831   14  2011   \n",
      "\n",
      "          Date  DOW  Weekday  Hour_Tot  Load_Tot      Load_Avg  \n",
      "366 2011-01-02    6    False        10    212142  21214.200000  \n",
      "73  2011-03-15    1     True        23    382919  16648.652174  \n",
      "1   2011-01-02    6    False        10    208342  20834.200000  \n",
      "443 2011-03-20    6    False         8    136110  17013.750000  \n",
      "72  2011-03-14    0     True        23    382919  16648.652174  \n",
      "number of rows in dataset = 8760\n"
     ]
    }
   ],
   "source": [
    "case3_x = x2.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case3 = case3_x.groupby(['Region','Month','Weekday','Hour'],as_index=False).agg(aggregations)\n",
    "case3.columns = case3.columns.droplevel(0)\n",
    "case3.columns = ['Region','Month','Weekday','Hour','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case1.head())\n",
    "print('number of rows in dataset =',case3.shape[0])\n",
    "case3.to_csv('../outputs/'+x_name+'_segments_2daytype_monthly_24hr.csv')\n",
    "print()\n",
    "\n",
    "case3_x2 = pd.merge(case3_x,case3,on=['Region','Month','Weekday','Hour'],how='left')\n",
    "case3_x2 = case3_x2.sort_values(['Region',x_column]).drop(case3_x2.columns[0],axis=1)\n",
    "print(case3_x2.head())\n",
    "print('number of rows in dataset =',case3_x2.shape[0])\n",
    "case3_x2.to_csv('../outputs/'+x_name+'_8760_2daytype_monthly_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 4: Season, weekend/weekday, 24 hours (144 segments)\n",
    "#### Methodology: groupby season, weekday, hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Region  Season_Group  Weekday  Hour  Hour_Tot  Load_Tot      Load_Avg\n",
      "235   FRCC             5     True    20        44   1435196  32618.090909\n",
      "236   FRCC             5     True    21        44   1307046  29705.590909\n",
      "237   FRCC             5     True    22        44   1191436  27078.090909\n",
      "238   FRCC             5     True    23        44   1111291  25256.613636\n",
      "239   FRCC             5     True    24        44   1057027  24023.340909\n",
      "number of rows in dataset = 240\n",
      "\n",
      "    R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "366    FRCC       FRCC    winter      1    2     2  15694    2  2011   \n",
      "73     FRCC       FRCC  shoulder      3   74     1  15738   15  2011   \n",
      "1      FRCC       FRCC    winter      1    2     1  15750    2  2011   \n",
      "443    FRCC       FRCC  shoulder      3   79     2  15822   20  2011   \n",
      "72     FRCC       FRCC  shoulder      3   73     1  15831   14  2011   \n",
      "\n",
      "          Date  DOW  Weekday  Season_Group  Hour_Tot  Load_Tot      Load_Avg  \n",
      "366 2011-01-02    6    False             1        27    512224  18971.259259  \n",
      "73  2011-03-15    1     True             2        87   1541095  17713.735632  \n",
      "1   2011-01-02    6    False             1        27    507490  18795.925926  \n",
      "443 2011-03-20    6    False             2        35    614008  17543.085714  \n",
      "72  2011-03-14    0     True             2        87   1541095  17713.735632  \n",
      "number of rows in dataset = 8760\n"
     ]
    }
   ],
   "source": [
    "case4_x = x2.copy()\n",
    "case4_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case4_seasons = case4_seasons.drop(['bimonthly'], axis=1)\n",
    "case4_seasons = case4_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "\n",
    "case4_x = pd.merge(case4_x, case4_seasons, on='Month', how='left')\n",
    "#print(case4_x)\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case4 = case4_x.groupby(['Region','Season_Group','Weekday','Hour'],as_index=False).agg(aggregations)\n",
    "case4.columns = case4.columns.droplevel(0)\n",
    "case4.columns = ['Region','Season_Group','Weekday','Hour','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case4.tail())\n",
    "print('number of rows in dataset =',case4.shape[0])\n",
    "case4.to_csv('../outputs/'+x_name+'_segments_2daytype_season_24hr.csv')\n",
    "print()\n",
    "\n",
    "case4_x2 = pd.merge(case4_x,case4,on=['Region','Season_Group','Weekday','Hour'],how='left')\n",
    "case4_x2 = case4_x2.sort_values(['Region',x_column]).drop(case4_x2.columns[0],axis=1)\n",
    "print(case4_x2.head())\n",
    "print('number of rows in dataset =',case4_x2.shape[0])\n",
    "case4_x2.to_csv('../outputs/'+x_name+'_8760_2daytype_season_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 hour interval day types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Region R_Group R_Subgroup  Season  Month  Day  Hour   Load  \\\n",
      "0       ERC_REST     ERC       REST  winter      1    1     1  34807   \n",
      "1       ERC_REST     ERC       REST  winter      1    2     1  34716   \n",
      "2       ERC_REST     ERC       REST  winter      1    3     1  34736   \n",
      "3       ERC_REST     ERC       REST  winter      1    4     1  35914   \n",
      "4       ERC_REST     ERC       REST  winter      1    5     1  33845   \n",
      "...          ...     ...        ...     ...    ...  ...   ...    ...   \n",
      "551875   WECC_WY     WEC         WY  winter     12  361    24   1767   \n",
      "551876   WECC_WY     WEC         WY  winter     12  362    24   1744   \n",
      "551877   WECC_WY     WEC         WY  winter     12  363    24   1805   \n",
      "551878   WECC_WY     WEC         WY  winter     12  364    24   1791   \n",
      "551879   WECC_WY     WEC         WY  winter     12  365    24   1834   \n",
      "\n",
      "             Date  DOW  Weekday  4-hr  \n",
      "0      2016-01-01    4     True     1  \n",
      "1      2016-01-01    4     True     1  \n",
      "2      2016-01-01    4     True     1  \n",
      "3      2016-01-01    4     True     1  \n",
      "4      2016-01-01    4     True     1  \n",
      "...           ...  ...      ...   ...  \n",
      "551875 2011-12-31    5    False     6  \n",
      "551876 2011-12-31    5    False     6  \n",
      "551877 2011-12-31    5    False     6  \n",
      "551878 2011-12-31    5    False     6  \n",
      "551879 2011-12-31    5    False     6  \n",
      "\n",
      "[551880 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "#read in 4 hour interval counter\n",
    "interval_4hr = pd.read_csv('inputs/daytype_4hr.csv')\n",
    "#print(interval_4hr)\n",
    "\n",
    "x3 = pd.merge(x2,interval_4hr,on='Hour',how='left').rename(columns={'4-hr_Day':'4-hr'})\n",
    "print(x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 5: Monthly, single day type, 4 hour intervals (72 segments)\n",
    "#### Methodology: use groupby by month, 4 hour intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Month  Day  4-hr  Hour_Tot     Tot       Avg\n",
      "0  ERC_REST      1    1     1         4  139677  34919.25\n",
      "1  ERC_REST      1    1     2         4  153285  38321.25\n",
      "2  ERC_REST      1    1     3         4  160446  40111.50\n",
      "3  ERC_REST      1    1     4         4  155314  38828.50\n",
      "4  ERC_REST      1    1     5         4  162472  40618.00\n",
      "number of rows in dataset = 137970\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1524  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "794   ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW  Weekday  4-hr  Hour_Tot     Tot       Avg  \n",
      "1545 2016-03-05    5    False     1         4  108862  27215.50  \n",
      "823  2016-02-04    3     True     1         4  111068  27767.00  \n",
      "1531 2016-03-04    4     True     1         4  109231  27307.75  \n",
      "1524 2016-03-04    4     True     1         4  108948  27237.00  \n",
      "794  2016-02-03    2     True     1         4  108948  27237.00  \n",
      "number of rows in dataset = 551880\n"

     ]
    }
   ],
   "source": [
    "case5_x = x3.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case5 = case5_x.groupby(['Region','Month','Day','4-hr'],as_index=False).agg(aggregations)\n",
    "case5.columns = case5.columns.droplevel(0)\n",
    "case5.columns = ['Region','Month','Day','4-hr','Hour_Tot','Tot','Avg']\n",
    "print(case5.head())\n",
    "print('number of rows in dataset =',case5.shape[0])\n",
    "case5.to_csv('../outputs/'+x_name+'_segments_2daytype_month_4hr.csv')\n",
    "print()\n",
    "\n",
    "case5_x2 = pd.merge(case5_x,case5,on=['Region','Month','Day','4-hr'],how='left')\n",
    "case5_x2 = case5_x2.sort_values(['Region',x_column])\n",
    "print(case5_x2.head())\n",
    "print('number of rows in dataset =',case5_x2.shape[0])\n",
    "case5_x2.to_csv('../outputs/'+x_name+'_8760_2daytype_month_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 6: Bi-monthly weekend/weekday day-types, 4-hour intervals\n",
    "#### Methodology: use groupby function and bimonthly groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Bimonth  Day  4-hr  Hour_Tot     Tot       Avg\n",
      "0  ERC_REST        1    1     1         4  139677  34919.25\n",
      "1  ERC_REST        1    1     2         4  153285  38321.25\n",
      "2  ERC_REST        1    1     3         4  160446  40111.50\n",
      "3  ERC_REST        1    1     4         4  155314  38828.50\n",
      "4  ERC_REST        1    1     5         4  162472  40618.00\n",
      "number of rows in dataset = 137970\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1524  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "794   ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW  Weekday  4-hr  Bimonth  Hour_Tot     Tot       Avg  \n",
      "1545 2016-03-05    5    False     1        2         4  108862  27215.50  \n",
      "823  2016-02-04    3     True     1        2         4  111068  27767.00  \n",
      "1531 2016-03-04    4     True     1        2         4  109231  27307.75  \n",
      "1524 2016-03-04    4     True     1        2         4  108948  27237.00  \n",
      "794  2016-02-03    2     True     1        2         4  108948  27237.00  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case6_x = x3.copy()\n",
    "case6_bimonth = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case6_bimonth = case6_bimonth.drop(['seasonal'], axis=1)\n",
    "case6_bimonth = case6_bimonth.rename(columns={'bimonthly':'Bimonth','month':'Month'})\n",
    "\n",
    "case6_x = pd.merge(case6_x, case6_bimonth, on='Month', how='left')\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case6 = case6_x.groupby(['Region','Bimonth','Day','4-hr'],as_index=False).agg(aggregations)\n",
    "case6.columns = case6.columns.droplevel(0)\n",
    "case6.columns = ['Region','Bimonth','Day','4-hr','Hour_Tot','Tot','Avg']\n",
    "print(case6.head())\n",
    "print('number of rows in dataset =',case6.shape[0])\n",
    "case6.to_csv('../outputs/'+x_name+'_segments_2daytype_bimonth_4hr.csv')\n",
    "print()\n",
    "\n",
    "case6_x2 = pd.merge(case6_x,case6,on=['Region','Bimonth','Day','4-hr'],how='left')\n",
    "case6_x2 = case6_x2.sort_values(['Region',x_column])\n",
    "print(case6_x2.head())\n",
    "print('number of rows in dataset =',case6_x2.shape[0])\n",
    "case6_x2.to_csv('../outputs/'+x_name+'_8760_2daytype_bimonth_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 7: Season-based months, weekend/weekday day-types, 4-hour intervals\n",
    "#### Methodology: groupby function and applied season groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region    Season  Weekday  4-hr  Hour_Tot      Tot           Avg\n",
      "0  ERC_REST  shoulder    False     1       115  3399710  29562.695652\n",
      "1  ERC_REST  shoulder    False     2       144  4872764  33838.638889\n",
      "2  ERC_REST  shoulder    False     3       155  5809365  37479.774194\n",
      "3  ERC_REST  shoulder    False     4       115  4632532  40282.886957\n",
      "4  ERC_REST  shoulder    False     5       144  5663089  39327.006944\n",
      "number of rows in dataset = 2268\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1524  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "794   ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW  Weekday  4-hr  Season_Group  Hour_Tot       Tot  \\\n",
      "1545 2016-03-05    5    False     1             2       115   3399710   \n",
      "823  2016-02-04    3     True     1             2       373  11158266   \n",
      "1531 2016-03-04    4     True     1             2       373  11158266   \n",
      "1524 2016-03-04    4     True     1             2       373  11158266   \n",
      "794  2016-02-03    2     True     1             2       373  11158266   \n",
      "\n",
      "               Avg  \n",
      "1545  29562.695652  \n",
      "823   29914.922252  \n",
      "1531  29914.922252  \n",
      "1524  29914.922252  \n",
      "794   29914.922252  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case7_x = x3.copy()\n",
    "case7_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case7_seasons = case7_seasons.drop(['bimonthly'], axis=1)\n",
    "case7_seasons = case7_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "\n",
    "case7_x = pd.merge(case7_x, case7_seasons, on='Month', how='left')\n",
    "#print(case4_load)\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case7 = case7_x.groupby(['Region','Season','Weekday','4-hr'],as_index=False).agg(aggregations)\n",
    "case7.columns = case7.columns.droplevel(0)\n",
    "case7.columns = ['Region','Season','Weekday','4-hr','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case7.head())\n",
    "print('number of rows in dataset =',case7.shape[0])\n",
    "case7.to_csv('../outputs/'+x_name+'_segments_2daytype_season_wkd_4hr.csv')\n",
    "print()\n",
    "\n",
    "case7_x2 = pd.merge(case7_x,case7,on=['Region','Season','Weekday','4-hr'],how='left')\n",
    "case7_x2 = case7_x2.sort_values(['Region',x_column]).drop(case7_x2.columns[0],axis=1)\n",
    "print(case7_x2.head())\n",
    "print('number of rows in dataset =',case7_x2.shape[0])\n",
    "case7_x2.to_csv('../outputs/'+x_name+'_8760_2daytype_season_wkd_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day type: Three (Weekday, Weekend, Peak Load)\n",
    "## Find Peak Load Days in Each Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Region  Month  Load_Max\n",
      "0    FRCC      1    839413\n",
      "1    FRCC      2    632009\n",
      "2    FRCC      3    629063\n",
      "3    FRCC      4    774706\n",
      "4    FRCC      5    761238\n",
      "5    FRCC      6    855573\n",
      "6    FRCC      7    848154\n",
      "7    FRCC      8    864191\n",
      "8    FRCC      9    798533\n",
      "9    FRCC     10    707155\n",
      "10   FRCC     11    657292\n",
      "11   FRCC     12    584382\n",
      "\n",
      "     Region R_Group R_Subgroup  Season  Month  Day  Hour   Load  Load_Tot  \\\n",
      "8755   FRCC    FRCC       FRCC  winter     12  361    24  17370    556346   \n",
      "8756   FRCC    FRCC       FRCC  winter     12  362    24  19366    546778   \n",
      "8757   FRCC    FRCC       FRCC  winter     12  363    24  19024    564643   \n",
      "8758   FRCC    FRCC       FRCC  winter     12  364    24  16730    542878   \n",
      "8759   FRCC    FRCC       FRCC  winter     12  365    24  17794    528679   \n",
      "\n",
      "      Load_Max  \n",
      "8755    584382  \n",
      "8756    584382  \n",
      "8757    584382  \n",
      "8758    584382  \n",
      "8759    584382  \n"
     ]
    }
   ],
   "source": [
    "#create temporary DF to find peak\n",
    "test = load_dur.copy()\n",
    "\n",
    "#groupby region, month, and day to sum the total day\n",
    "aggregations1 = {x_column:sum}\n",
    "md_sum = test.groupby(['Region','Month','Day'],as_index=False).agg(aggregations1)\n",
    "#test1.columns = test1.columns.droplevel(0)\n",
    "md_sum.columns = ['Region','Month','Day',x_name2+'_Tot']\n",
    "#print(test_sum.head())\n",
    "#print('number of rows in dataset =',test_sum.shape[0])\n",
    "\n",
    "md_sum2 = pd.merge(test,md_sum,on=['Region','Month','Day'],how='left')\n",
    "#print(test3)\n",
    "\n",
    "#groupby region and month to find maximum \n",
    "aggregations2 = {x_name2+'_Tot':max}\n",
    "md_max = md_sum2.groupby(['Region','Month'],as_index=False).agg(aggregations2)\n",
    "md_max.columns = ['Region','Month',x_name2+'_Max']\n",
    "print(md_max)\n",
    "print()\n",
    "\n",
    "peakd = pd.merge(test3,md_max,on=['Region','Month'],how='left')\n",
    "print(peakd.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region R_Group R_Subgroup  Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "0   FRCC    FRCC       FRCC  winter      1    1     1  17005    1  2011   \n",
      "1   FRCC    FRCC       FRCC  winter      1    2     1  15750    2  2011   \n",
      "2   FRCC    FRCC       FRCC  winter      1    3     1  16123    3  2011   \n",
      "3   FRCC    FRCC       FRCC  winter      1    4     1  18300    4  2011   \n",
      "4   FRCC    FRCC       FRCC  winter      1    5     1  17370    5  2011   \n",
      "\n",
      "        Date  DOW Day_Type  \n",
      "0 2011-01-01    5  Weekend  \n",
      "1 2011-01-02    6  Weekend  \n",
      "2 2011-01-03    0  Weekday  \n",
      "3 2011-01-04    1  Weekday  \n",
      "4 2011-01-05    2  Weekday  \n"
     ]
    }
   ],
   "source": [
    "x_peak = x2.copy()\n",
    "\n",
    "x_peak = pd.merge(x_peak,peakd,on=['Region','R_Group','R_Subgroup','Season','Month','Day','Hour',x_column],how='left')\n",
    "x_peak = x_peak.rename(columns={'Weekday':'Day_Type'})\n",
    "\n",
    "#Return True if the load total equals the day identified as the max\n",
    "x_peak.loc[x_peak['Day_Type'] == True, 'Day_Type'] = 'Weekday'\n",
    "x_peak.loc[x_peak['Day_Type'] == False, 'Day_Type'] = 'Weekend'\n",
    "x_peak.loc[x_peak[x_name2+'_Tot'] == x_peak[x_name2+'_Max'], 'Day_Type'] = 'Peak'\n",
    "x_peak = x_peak.drop([x_name2+'_Tot',x_name2+'_Max'], axis=1)\n",
    "print(x_peak.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Monthly, Weekend/weekday/peak day-types, 24 hours (864 segments)\n",
    "#### Methodology: similar to two day type, just adding in peak day types to sort by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region  Month Day_Type  Hour  Hour_Tot  Load_Tot  Load_Avg\n",
      "0   FRCC      1     Peak     1         1     32350   32350.0\n",
      "1   FRCC      1     Peak     2         1     33972   33972.0\n",
      "2   FRCC      1     Peak     3         1     37185   37185.0\n",
      "3   FRCC      1     Peak     4         1     41916   41916.0\n",
      "4   FRCC      1     Peak     5         1     44276   44276.0\n",
      "number of rows in dataset = 600\n",
      "\n",
      "     Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "300    FRCC    FRCC       FRCC    winter      1    2     2  15694    2  2011   \n",
      "4034   FRCC    FRCC       FRCC  shoulder      3   74     1  15738   15  2011   \n",
      "288    FRCC    FRCC       FRCC    winter      1    2     1  15750    2  2011   \n",
      "\n",
      "           Date  DOW Day_Type  Hour_Counter  Hour_Tot  Load_Tot      Load_Avg  \n",
      "300  2011-01-02    6  Weekend            26        10    212142  21214.200000  \n",
      "4034 2011-03-15    1  Weekday           337        23    382919  16648.652174  \n",
      "288  2011-01-02    6  Weekend            25        10    208342  20834.200000  \n",
      "number of rows in dataset = 8760\n"
     ]
    }
   ],
   "source": [
    "case1_x = x_peak.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case1 = case1_x.groupby(['Region','Month','Day_Type','Hour'],as_index=False).agg(aggregations)\n",
    "case1.columns = case1.columns.droplevel(0)\n",
    "case1.columns = ['Region','Month','Day_Type','Hour','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case1.head())\n",
    "print('number of rows in dataset =',case1.shape[0])\n",
    "case1.to_csv('../outputs/'+x_name+'_segments_3daytype_monthly_24hr.csv')\n",
    "print()\n",
    "\n",
    "case1_x2 = pd.merge(case1_x,case1,on=['Region','Month','Day_Type','Hour'],how='left')\n",
    "case1_x2 = case1_x2.sort_values(['Region',x_column])\n",
    "print(case1_x2.head(3))\n",
    "print('number of rows in dataset =',case1_x2.shape[0])\n",
    "case1_x2.to_csv('../outputs/'+x_name+'_8760_3daytype_monthly_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: Season, weekend/weekday/peak day-types, 24-hours (216 segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region    Season Day_Type  Hour  Hour_Tot  Load_Tot      Load_Avg\n",
      "0   FRCC  shoulder  Weekday     1        87   1541095  17713.735632\n",
      "1   FRCC  shoulder  Weekday     2        87   1553709  17858.724138\n",
      "2   FRCC  shoulder  Weekday     3        87   1661960  19102.988506\n",
      "3   FRCC  shoulder  Weekday     4        87   1886198  21680.436782\n",
      "4   FRCC  shoulder  Weekday     5        87   2013602  23144.850575\n",
      "number of rows in dataset = 168\n",
      "\n",
      "     Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "300    FRCC    FRCC       FRCC    winter      1    2     2  15694    2  2011   \n",
      "4034   FRCC    FRCC       FRCC  shoulder      3   74     1  15738   15  2011   \n",
      "288    FRCC    FRCC       FRCC    winter      1    2     1  15750    2  2011   \n",
      "\n",
      "           Date  DOW Day_Type  Hour_Counter  Hour_Tot  Load_Tot      Load_Avg  \n",
      "300  2011-01-02    6  Weekend            26        10    212142  21214.200000  \n",
      "4034 2011-03-15    1  Weekday           337        23    382919  16648.652174  \n",
      "288  2011-01-02    6  Weekend            25        10    208342  20834.200000  \n",
      "number of rows in dataset = 8760\n"
     ]
    }
   ],
   "source": [
    "case2_x = x_peak.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case2 = case2_x.groupby(['Region','Season','Day_Type','Hour'],as_index=False).agg(aggregations)\n",
    "case2.columns = case2.columns.droplevel(0)\n",
    "case2.columns = ['Region','Season','Day_Type','Hour','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case2.head())\n",
    "print('number of rows in dataset =',case2.shape[0])\n",
    "case2.to_csv('../outputs/'+x_name+'_segments_3daytype_season_24hr.csv')\n",
    "print()\n",
    "\n",
    "case2_x2 = pd.merge(case2_x,case2,on=['Region','Season','Day_Type','Hour'],how='left')\n",
    "case2_x2 = case2_x2.sort_values(['Region',x_column])\n",
    "print(case1_x2.head(3))\n",
    "print('number of rows in dataset =',case1_x2.shape[0])\n",
    "case2_x2.to_csv('../outputs/'+x_name+'_8760_3daytype_season_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3: Annual, weekend/weekday/peak day-types, 24-hours (72 segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region Day_Type  Hour  Hour_Tot  Load_Tot  Load_Avg\n",
      "0   FRCC     Peak     1         1     32350   32350.0\n",
      "1   FRCC     Peak     2         1     33972   33972.0\n",
      "2   FRCC     Peak     3         1     37185   37185.0\n",
      "3   FRCC     Peak     4         1     41916   41916.0\n",
      "4   FRCC     Peak     5         1     44276   44276.0\n",
      "number of rows in dataset = 72\n",
      "\n",
      "     Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "300    FRCC    FRCC       FRCC    winter      1    2     2  15694    2  2011   \n",
      "4034   FRCC    FRCC       FRCC  shoulder      3   74     1  15738   15  2011   \n",
      "288    FRCC    FRCC       FRCC    winter      1    2     1  15750    2  2011   \n",
      "\n",
      "           Date  DOW Day_Type  Hour_Counter  Hour_Tot  Load_Tot      Load_Avg  \n",
      "300  2011-01-02    6  Weekend            26       105   2051738  19540.361905  \n",
      "4034 2011-03-15    1  Weekday           337       259   5088736  19647.629344  \n",
      "288  2011-01-02    6  Weekend            25       105   2072468  19737.790476  \n",
      "number of rows in dataset = 8760\n"
     ]
    }
   ],
   "source": [
    "case3_x = x_peak.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case3 = case3_x.groupby(['Region','Day_Type','Hour'],as_index=False).agg(aggregations)\n",
    "case3.columns = case3.columns.droplevel(0)\n",
    "case3.columns = ['Region','Day_Type','Hour','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case3.head())\n",
    "print('number of rows in dataset =',case3.shape[0])\n",
    "case3.to_csv('../outputs/'+x_name+'_segments_3daytype_annual_24hr.csv')\n",
    "print()\n",
    "\n",
    "case3_x2 = pd.merge(case3_x,case3,on=['Region','Day_Type','Hour'],how='left')\n",
    "case3_x2 = case3_x2.sort_values(['Region',x_column])\n",
    "print(case3_x2.head(3))\n",
    "print('number of rows in dataset =',case3_x2.shape[0])\n",
    "case3_x2.to_csv('../outputs/'+x_name+'_8760_3daytype_annual_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 hour interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Region R_Group R_Subgroup  Season  Month  Day  Hour   Load  \\\n",
      "0       ERC_REST     ERC       REST  winter      1    1     1  34807   \n",
      "1       ERC_REST     ERC       REST  winter      1    2     1  34716   \n",
      "2       ERC_REST     ERC       REST  winter      1    3     1  34736   \n",
      "3       ERC_REST     ERC       REST  winter      1    4     1  35914   \n",
      "4       ERC_REST     ERC       REST  winter      1    5     1  33845   \n",
      "...          ...     ...        ...     ...    ...  ...   ...    ...   \n",
      "551875   WECC_WY     WEC         WY  winter     12  361    24   1767   \n",
      "551876   WECC_WY     WEC         WY  winter     12  362    24   1744   \n",
      "551877   WECC_WY     WEC         WY  winter     12  363    24   1805   \n",
      "551878   WECC_WY     WEC         WY  winter     12  364    24   1791   \n",
      "551879   WECC_WY     WEC         WY  winter     12  365    24   1834   \n",
      "\n",
      "             Date  DOW Day_Type  4-hr  \n",
      "0      2016-01-01    4  Weekday     1  \n",
      "1      2016-01-01    4  Weekday     1  \n",
      "2      2016-01-01    4  Weekday     1  \n",
      "3      2016-01-01    4  Weekday     1  \n",
      "4      2016-01-01    4  Weekday     1  \n",
      "...           ...  ...      ...   ...  \n",
      "551875 2011-12-31    5  Weekend     6  \n",
      "551876 2011-12-31    5  Weekend     6  \n",
      "551877 2011-12-31    5  Weekend     6  \n",
      "551878 2011-12-31    5  Weekend     6  \n",
      "551879 2011-12-31    5  Weekend     6  \n",
      "\n",
      "[551880 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "#read in 4 hour interval counter\n",
    "interval_4hr = pd.read_csv('inputs/daytype_4hr.csv')\n",
    "\n",
    "x_peak2 = pd.merge(x_peak,interval_4hr,on='Hour',how='left').rename(columns={'4-hr_Day':'4-hr'})\n",
    "print(x_peak2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 4: Bi-monthly, weekend/weekday/peak day-types, 4-hour intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Bimonth Day_Type  4-hr  Hour_Tot     Tot        Avg\n",
      "0  ERC_REST        1     Peak     1         8  312443  39055.375\n",
      "1  ERC_REST        1     Peak     2         8  366535  45816.875\n",
      "2  ERC_REST        1     Peak     3         8  322104  40263.000\n",
      "3  ERC_REST        1     Peak     4         8  297348  37168.500\n",
      "4  ERC_REST        1     Peak     5         8  332533  41566.625\n",
      "number of rows in dataset = 6804\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1524  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "794   ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW Day_Type  4-hr  Bimonth  Hour_Tot      Tot           Avg  \n",
      "1545 2016-03-05    5  Weekend     1        2        62  1790938  28886.096774  \n",
      "823  2016-02-04    3  Weekday     1        2       174  5066815  29119.626437  \n",
      "1531 2016-03-04    4  Weekday     1        2       174  5066815  29119.626437  \n",
      "1524 2016-03-04    4  Weekday     1        2       174  5066815  29119.626437  \n",
      "794  2016-02-03    2  Weekday     1        2       174  5066815  29119.626437  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case4_x = x_peak2.copy()\n",
    "case4_bimonth = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case4_bimonth = case4_bimonth.drop(['seasonal'], axis=1)\n",
    "case4_bimonth = case4_bimonth.rename(columns={'bimonthly':'Bimonth','month':'Month'})\n",
    "\n",
    "case4_x = pd.merge(case4_x, case4_bimonth, on='Month', how='left')\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case4 = case4_x.groupby(['Region','Bimonth','Day_Type','4-hr'],as_index=False).agg(aggregations)\n",
    "case4.columns = case4.columns.droplevel(0)\n",
    "case4.columns = ['Region','Bimonth','Day_Type','4-hr','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case4.head())\n",
    "print('number of rows in dataset =',case4.shape[0])\n",
    "case4.to_csv('../outputs/'+x_name+'_segments_3daytype_bimonth_4hr.csv')\n",
    "print()\n",
    "\n",
    "case4_x2 = pd.merge(case4_x,case4,on=['Region','Bimonth','Day_Type','4-hr'],how='left')\n",
    "case4_x2 = case4_x2.sort_values(['Region',x_column])\n",
    "print(case4_x2.head())\n",
    "print('number of rows in dataset =',case4_x2.shape[0])\n",
    "case4_x2.to_csv('../outputs/'+x_name+'_8760_3daytype_bimonth_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 5: Season-based months, weekend/weekday/peak day-types, 4-hour intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Season_Group Day_Type  4-hr  Hour_Tot     Tot           Avg\n",
      "0  ERC_REST             1     Peak     1        12  504276  42023.000000\n",
      "1  ERC_REST             1     Peak     2        12  583360  48613.333333\n",
      "2  ERC_REST             1     Peak     3        12  527061  43921.750000\n",
      "3  ERC_REST             1     Peak     4        12  478413  39867.750000\n",
      "4  ERC_REST             1     Peak     5        12  530620  44218.333333\n",
      "number of rows in dataset = 5639\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1524  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "794   ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW Day_Type  4-hr  Season_Group  Hour_Tot       Tot  \\\n",
      "1545 2016-03-05    5  Weekend     1             2       111   3268235   \n",
      "823  2016-02-04    3  Weekday     1             2       361  10756588   \n",
      "1531 2016-03-04    4  Weekday     1             2       361  10756588   \n",
      "1524 2016-03-04    4  Weekday     1             2       361  10756588   \n",
      "794  2016-02-03    2  Weekday     1             2       361  10756588   \n",
      "\n",
      "               Avg  \n",
      "1545  29443.558559  \n",
      "823   29796.642659  \n",
      "1531  29796.642659  \n",
      "1524  29796.642659  \n",
      "794   29796.642659  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case5_x = x_peak2.copy()\n",
    "case5_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case5_seasons = case5_seasons.drop(['bimonthly'], axis=1)\n",
    "case5_seasons = case5_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "\n",
    "case5_x = pd.merge(case5_x, case5_seasons, on='Month', how='left')\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case5 = case5_x.groupby(['Region','Season_Group','Day_Type','4-hr'],as_index=False).agg(aggregations)\n",
    "case5.columns = case5.columns.droplevel(0)\n",
    "case5.columns = ['Region','Season_Group','Day_Type','4-hr','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case5.head())\n",
    "print('number of rows in dataset =',case5.shape[0])\n",
    "case5.to_csv('../outputs/'+x_name+'_segments_3daytype_seasonbased_4hr.csv')\n",
    "print()\n",
    "\n",
    "case5_x2 = pd.merge(case5_x,case5,on=['Region','Season_Group','Day_Type','4-hr'],how='left')\n",
    "case5_x2 = case5_x2.sort_values(['Region',x_column])\n",
    "print(case5_x2.head())\n",
    "print('number of rows in dataset =',case5_x2.shape[0])\n",
    "case5_x2.to_csv('../outputs/'+x_name+'_8760_3daytype_seasonbased_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 6: Season, weekend/weekday/peak day-types, 4-hour intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region    Season Day_Type  4-hr  Hour_Tot     Tot         Avg\n",
      "0  ERC_REST  shoulder     Peak     1        16  533153  33322.0625\n",
      "1  ERC_REST  shoulder     Peak     2        16  619476  38717.2500\n",
      "2  ERC_REST  shoulder     Peak     3        16  707544  44221.5000\n",
      "3  ERC_REST  shoulder     Peak     4        16  785332  49083.2500\n",
      "4  ERC_REST  shoulder     Peak     5        16  743101  46443.8125\n",
      "number of rows in dataset = 3402\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1524  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "794   ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW Day_Type  4-hr  Hour_Tot       Tot           Avg  \n",
      "1545 2016-03-05    5  Weekend     1       111   3268235  29443.558559  \n",
      "823  2016-02-04    3  Weekday     1       361  10756588  29796.642659  \n",
      "1531 2016-03-04    4  Weekday     1       361  10756588  29796.642659  \n",
      "1524 2016-03-04    4  Weekday     1       361  10756588  29796.642659  \n",
      "794  2016-02-03    2  Weekday     1       361  10756588  29796.642659  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case6_x = x_peak2.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case6 = case6_x.groupby(['Region','Season','Day_Type','4-hr'],as_index=False).agg(aggregations)\n",
    "case6.columns = case6.columns.droplevel(0)\n",
    "case6.columns = ['Region','Season','Day_Type','4-hr','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case6.head())\n",
    "print('number of rows in dataset =',case6.shape[0])\n",
    "case6.to_csv('../outputs/'+x_name+'_segments_3daytype_season_4hr.csv')\n",
    "print()\n",
    "\n",
    "case6_x2 = pd.merge(case6_x,case6,on=['Region','Season','Day_Type','4-hr'],how='left')\n",
    "case6_x2 = case6_x2.sort_values(['Region',x_column])\n",
    "print(case6_x2.head())\n",
    "print('number of rows in dataset =',case6_x2.shape[0])\n",
    "case6_x2.to_csv('../outputs/'+x_name+'_8760_3daytype_season_4hr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
