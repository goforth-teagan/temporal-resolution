{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data here: \n",
    "## One and two day type \n",
    "## Three day type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day Type: One/two\n",
    "## Create Date and Day of Week columns in the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output files are written out in parent directory: C:\\Users\\tgoforth\\Documents\\IPM temporal resolution project\\outputs\n",
      "output files are written out in parent directory: C:\\Users\\tgoforth\\Documents\\IPM temporal resolution project\\outputs/load\n"
     ]
    }
   ],
   "source": [
    "#importing packages needed for analysis\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from pandas import DataFrame\n",
    "\n",
    "path = os.getcwd()\n",
    "#print(path)\n",
    "\n",
    "load_dur = pd.read_csv('../outputs/load_long_format.csv')\n",
    "solar_dur = pd.read_csv('../outputs/solar_long_format.csv')\n",
    "wind_dur = pd.read_csv('../outputs/wind_long_format.csv')\n",
    "\n",
    "## UNCOMMENT WHICH PROFILE TO BE USED\n",
    "x = load_dur\n",
    "x_name = 'load'\n",
    "x_name2 = 'Load'\n",
    "x_column = 'Load'\n",
    "\n",
    "#x = solar_dur\n",
    "#x_name = 'solar'\n",
    "#x_name2 = 'Solar_Gen'\n",
    "#choose TRG \n",
    "#x_column = 'TRG6'\n",
    "\n",
    "#x = wind_dur\n",
    "#x_name = 'wind'\n",
    "#x_name2 = 'Wind_Gen'\n",
    "#x_column = 'TRG4'\n",
    "\n",
    "#this code creates an output directory in the parent director, if one does not exist yet\n",
    "#Note: this is where all of the output files will be written, since outputs are large this saves space in git\n",
    "path = os.getcwd()\n",
    "parent = os.path.dirname(path)\n",
    "outputs_dir = parent+'\\outputs'\n",
    "if not os.path.exists(outputs_dir):\n",
    "    os.makedirs(outputs_dir)\n",
    "print('output files are written out in parent directory: '+outputs_dir)\n",
    "\n",
    "outputs_x = outputs_dir+'/'+x_name\n",
    "if not os.path.exists(outputs_x):\n",
    "    os.makedirs(outputs_x)\n",
    "print('output files are written out in parent directory: '+outputs_x)\n",
    "\n",
    "x = x[['Region','R_Group','R_Subgroup','Season','Month','Day','Hour',x_column]]\n",
    "years = pd.read_csv('inputs/years.csv').dropna()\n",
    "\n",
    "#print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "#repeat the date for 24 times for each hour in the day\n",
    "year_2016 = years['2016'].repeat(24).reset_index(drop=True)\n",
    "year_2011 = years['2011'].repeat(24).reset_index(drop=True)\n",
    "#print(year_2016)\n",
    "\n",
    "#use itertools.cycle to make the date repeat until it reaches the end of the dataframe\n",
    "year_2016 = cycle(year_2016)\n",
    "year_2011 = cycle(year_2011)\n",
    "\n",
    "#create temporary data frames where they will iterate through. Needed to achieve the correct length of the DF\n",
    "temp2016 = x.loc[x['R_Group'] == 'ERC'].copy()\n",
    "temp2011 = x.loc[x['R_Group'] != 'ERC'].copy()\n",
    "\n",
    "#iterate through the date until the end of DF \n",
    "temp2016['Date'] = [next(year_2016) for year in range(len(temp2016))]\n",
    "temp2011['Date'] = [next(year_2011) for year in range(len(temp2011))]\n",
    "#print(temp2016)\n",
    "#print(temp2011)\n",
    "\n",
    "x2 = pd.concat([temp2016, temp2011], ignore_index=True)\n",
    "#print(x2)\n",
    "\n",
    "#convert date to a datetime type \n",
    "x2['Date'] = pd.to_datetime(x2['Date'])\n",
    "x2['DOW'] = x2['Date'].dt.weekday\n",
    "\n",
    "#check if it is a weekday or not \n",
    "weekday = pd.read_csv('inputs/weekday.csv')\n",
    "x2 = pd.merge(x2,weekday,on='DOW',how='left')\n",
    "#print(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Monthly, single day type, 24 hours (288 segments)\n",
    "#### Methodology: Using groupby function to group first by month, then by 24 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Month  Hour  Hour_Tot      Tot           Avg\n",
      "0  ERC_REST      1     1        31  1042446  33627.290323\n",
      "1  ERC_REST      1     2        31  1051661  33924.548387\n",
      "2  ERC_REST      1     3        31  1079739  34830.290323\n",
      "3  ERC_REST      1     4        31  1144492  36919.096774\n",
      "4  ERC_REST      1     5        31  1239385  39980.161290\n",
      "number of rows in dataset = 18144\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "\n",
      "           Date  DOW  Weekday  Hour_Tot     Tot           Avg  \n",
      "1545 2016-03-05    5    False        31  886708  28603.483871  \n",
      "823  2016-02-04    3     True        30  870591  29019.700000  \n",
      "1531 2016-03-04    4     True        31  886708  28603.483871  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case1_x = x2.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case1 = case1_x.groupby(['Region','Month','Hour'],as_index=False).agg(aggregations)\n",
    "case1.columns = case1.columns.droplevel(0)\n",
    "case1.columns = ['Region','Month','Hour','Hour_Tot','Tot','Avg']\n",
    "print(case1.head())\n",
    "print('number of rows in dataset =',case1.shape[0])\n",
    "case1.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_1daytype_monthly_24hr.csv')\n",
    "print()\n",
    "\n",
    "case1_x2 = pd.merge(case1_x,case1,on=['Region','Month','Hour'],how='left')\n",
    "case1_x2 = case1_x2.sort_values(['Region',x_column])\n",
    "print(case1_x2.head(3))\n",
    "print('number of rows in dataset =',case1_x2.shape[0])\n",
    "case1_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_1daytype_monthly_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: Season, single day-type, 24 hours (72 segments)\n",
    "#### Methodology: Use groupby function to group by season and hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Season_Group  Hour  Hour_Tot      Tot           Avg\n",
      "0  ERC_REST             1     1        90  2928629  32540.322222\n",
      "1  ERC_REST             1     2        90  2929331  32548.122222\n",
      "2  ERC_REST             1     3        90  2978696  33096.622222\n",
      "3  ERC_REST             1     4        90  3116896  34632.177778\n",
      "4  ERC_REST             1     5        90  3346197  37179.966667\n",
      "number of rows in dataset = 7560\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "\n",
      "           Date  DOW  Weekday  Season_Group  Hour_Tot      Tot           Avg  \n",
      "1545 2016-03-05    5    False             2       122  3603145  29533.975410  \n",
      "823  2016-02-04    3     True             2       122  3587428  29405.147541  \n",
      "1531 2016-03-04    4     True             2       122  3603145  29533.975410  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case2_x = x2.copy()\n",
    "case2_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case2_seasons = case2_seasons.drop(['bimonthly'], axis=1)\n",
    "case2_seasons = case2_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "\n",
    "case2_x = pd.merge(case2_x, case2_seasons, on='Month', how='left')\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case2 = case2_x.groupby(['Region','Season_Group','Hour'],as_index=False).agg(aggregations)\n",
    "case2.columns = case2.columns.droplevel(0)\n",
    "case2.columns = ['Region','Season_Group','Hour','Hour_Tot','Tot','Avg']\n",
    "print(case2.head())\n",
    "print('number of rows in dataset =',case2.shape[0])\n",
    "case2.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_1daytype_season_24hr.csv')\n",
    "print()\n",
    "\n",
    "case2_x2 = pd.merge(case2_x,case2,on=['Region','Season_Group','Hour'],how='left')\n",
    "case2_x2 = case2_x2.sort_values(['Region',x_column])\n",
    "print(case2_x2.head(3))\n",
    "print('number of rows in dataset =',case2_x2.shape[0])\n",
    "case2_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_1daytype_season_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3: Monthly, weekend/weekday, 24 hours (576 segments)\n",
    "#### Metholodogy: Use groupby function to group by month, then weekend/weekday, then by 24 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Month  Hour  Hour_Tot      Tot           Avg\n",
      "0  ERC_REST      1     1        31  1042446  33627.290323\n",
      "1  ERC_REST      1     2        31  1051661  33924.548387\n",
      "2  ERC_REST      1     3        31  1079739  34830.290323\n",
      "3  ERC_REST      1     4        31  1144492  36919.096774\n",
      "4  ERC_REST      1     5        31  1239385  39980.161290\n",
      "number of rows in dataset = 24507\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1524  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "794   ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW  Weekday  Hour_Tot     Tot           Avg  \n",
      "1545 2016-03-05    5    False        14  406333  29023.785714  \n",
      "823  2016-02-04    3     True        30  870591  29019.700000  \n",
      "1531 2016-03-04    4     True        17  480375  28257.352941  \n",
      "1524 2016-03-04    4     True        17  480375  28257.352941  \n",
      "794  2016-02-03    2     True        31  875126  28229.870968  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case3_x = x2.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case3 = case3_x.groupby(['Region','Month','Weekday','Hour'],as_index=False).agg(aggregations)\n",
    "case3.columns = case3.columns.droplevel(0)\n",
    "case3.columns = ['Region','Month','Weekday','Hour','Hour_Tot','Tot','Avg']\n",
    "print(case1.head())\n",
    "print('number of rows in dataset =',case3.shape[0])\n",
    "case3.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_2daytype_monthly_24hr.csv')\n",
    "print()\n",
    "\n",
    "case3_x2 = pd.merge(case3_x,case3,on=['Region','Month','Weekday','Hour'],how='left')\n",
    "case3_x2 = case3_x2.sort_values(['Region',x_column])\n",
    "print(case3_x2.head())\n",
    "print('number of rows in dataset =',case3_x2.shape[0])\n",
    "case3_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_2daytype_monthly_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 4: Season, weekend/weekday, 24 hours (144 segments)\n",
    "#### Methodology: groupby season, weekday, hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Season_Group  Weekday  Hour  Hour_Tot      Tot           Avg\n",
      "0  ERC_REST             1    False     1        40  1226650  30666.250000\n",
      "1  ERC_REST             1    False     2        30  1003087  33436.233333\n",
      "2  ERC_REST             1    False     4        52  1741883  33497.750000\n",
      "3  ERC_REST             1    False     5        18   669258  37181.000000\n",
      "4  ERC_REST             1    False     7        64  2446079  38219.984375\n",
      "number of rows in dataset = 12478\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1524  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "794   ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW  Weekday  Season_Group  Hour_Tot      Tot           Avg  \n",
      "1545 2016-03-05    5    False             2        63  1894236  30067.238095  \n",
      "823  2016-02-04    3     True             2        90  2653648  29484.977778  \n",
      "1531 2016-03-04    4     True             2        59  1708909  28964.559322  \n",
      "1524 2016-03-04    4     True             2        59  1708909  28964.559322  \n",
      "794  2016-02-03    2     True             2        90  2653648  29484.977778  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case4_x = x2.copy()\n",
    "case4_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case4_seasons = case4_seasons.drop(['bimonthly'], axis=1)\n",
    "case4_seasons = case4_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "\n",
    "case4_x = pd.merge(case4_x, case4_seasons, on='Month', how='left')\n",
    "#print(case4_x)\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case4 = case4_x.groupby(['Region','Season_Group','Weekday','Hour'],as_index=False).agg(aggregations)\n",
    "case4.columns = case4.columns.droplevel(0)\n",
    "case4.columns = ['Region','Season_Group','Weekday','Hour','Hour_Tot','Tot','Avg']\n",
    "print(case4.head())\n",
    "print('number of rows in dataset =',case4.shape[0])\n",
    "case4.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_2daytype_season_24hr.csv')\n",
    "print()\n",
    "\n",
    "case4_x2 = pd.merge(case4_x,case4,on=['Region','Season_Group','Weekday','Hour'],how='left')\n",
    "case4_x2 = case4_x2.sort_values(['Region',x_column])\n",
    "print(case4_x2.head())\n",
    "print('number of rows in dataset =',case4_x2.shape[0])\n",
    "case4_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_2daytype_season_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 hour interval day types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Region R_Group R_Subgroup  Season  Month  Day  Hour   Load  \\\n",
      "0       ERC_REST     ERC       REST  winter      1    1     1  34807   \n",
      "1       ERC_REST     ERC       REST  winter      1    2     1  34716   \n",
      "2       ERC_REST     ERC       REST  winter      1    3     1  34736   \n",
      "3       ERC_REST     ERC       REST  winter      1    4     1  35914   \n",
      "4       ERC_REST     ERC       REST  winter      1    5     1  33845   \n",
      "...          ...     ...        ...     ...    ...  ...   ...    ...   \n",
      "551875   WECC_WY     WEC         WY  winter     12  361    24   1767   \n",
      "551876   WECC_WY     WEC         WY  winter     12  362    24   1744   \n",
      "551877   WECC_WY     WEC         WY  winter     12  363    24   1805   \n",
      "551878   WECC_WY     WEC         WY  winter     12  364    24   1791   \n",
      "551879   WECC_WY     WEC         WY  winter     12  365    24   1834   \n",
      "\n",
      "             Date  DOW  Weekday  4-hr  \n",
      "0      2016-01-01    4     True     1  \n",
      "1      2016-01-01    4     True     1  \n",
      "2      2016-01-01    4     True     1  \n",
      "3      2016-01-01    4     True     1  \n",
      "4      2016-01-01    4     True     1  \n",
      "...           ...  ...      ...   ...  \n",
      "551875 2011-12-31    5    False     6  \n",
      "551876 2011-12-31    5    False     6  \n",
      "551877 2011-12-31    5    False     6  \n",
      "551878 2011-12-31    5    False     6  \n",
      "551879 2011-12-31    5    False     6  \n",
      "\n",
      "[551880 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "#read in 4 hour interval counter\n",
    "interval_4hr = pd.read_csv('inputs/daytype_4hr.csv')\n",
    "#print(interval_4hr)\n",
    "\n",
    "x3 = pd.merge(x2,interval_4hr,on='Hour',how='left').rename(columns={'4-hr_Day':'4-hr'})\n",
    "print(x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 5: Monthly, single day type, 4 hour intervals (72 segments)\n",
    "#### Methodology: use groupby by month, 4 hour intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Month  Day  4-hr  Hour_Tot     Tot       Avg\n",
      "0  ERC_REST      1    1     1         4  139677  34919.25\n",
      "1  ERC_REST      1    1     2         4  153285  38321.25\n",
      "2  ERC_REST      1    1     3         4  160446  40111.50\n",
      "3  ERC_REST      1    1     4         4  155314  38828.50\n",
      "4  ERC_REST      1    1     5         4  162472  40618.00\n",
      "number of rows in dataset = 137970\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1524  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "794   ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW  Weekday  4-hr  Hour_Tot     Tot       Avg  \n",
      "1545 2016-03-05    5    False     1         4  108862  27215.50  \n",
      "823  2016-02-04    3     True     1         4  111068  27767.00  \n",
      "1531 2016-03-04    4     True     1         4  109231  27307.75  \n",
      "1524 2016-03-04    4     True     1         4  108948  27237.00  \n",
      "794  2016-02-03    2     True     1         4  108948  27237.00  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case5_x = x3.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case5 = case5_x.groupby(['Region','Month','Day','4-hr'],as_index=False).agg(aggregations)\n",
    "case5.columns = case5.columns.droplevel(0)\n",
    "case5.columns = ['Region','Month','Day','4-hr','Hour_Tot','Tot','Avg']\n",
    "print(case5.head())\n",
    "print('number of rows in dataset =',case5.shape[0])\n",
    "case5.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_2daytype_month_4hr.csv')\n",
    "print()\n",
    "\n",
    "case5_x2 = pd.merge(case5_x,case5,on=['Region','Month','Day','4-hr'],how='left')\n",
    "case5_x2 = case5_x2.sort_values(['Region',x_column])\n",
    "print(case5_x2.head())\n",
    "print('number of rows in dataset =',case5_x2.shape[0])\n",
    "case5_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_2daytype_month_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 6: Bi-monthly weekend/weekday day-types, 4-hour intervals\n",
    "#### Methodology: use groupby function and bimonthly groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Bimonth  Day  4-hr  Hour_Tot     Tot       Avg\n",
      "0  ERC_REST        1    1     1         4  139677  34919.25\n",
      "1  ERC_REST        1    1     2         4  153285  38321.25\n",
      "2  ERC_REST        1    1     3         4  160446  40111.50\n",
      "3  ERC_REST        1    1     4         4  155314  38828.50\n",
      "4  ERC_REST        1    1     5         4  162472  40618.00\n",
      "number of rows in dataset = 137970\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1524  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "794   ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW  Weekday  4-hr  Bimonth  Hour_Tot     Tot       Avg  \n",
      "1545 2016-03-05    5    False     1        2         4  108862  27215.50  \n",
      "823  2016-02-04    3     True     1        2         4  111068  27767.00  \n",
      "1531 2016-03-04    4     True     1        2         4  109231  27307.75  \n",
      "1524 2016-03-04    4     True     1        2         4  108948  27237.00  \n",
      "794  2016-02-03    2     True     1        2         4  108948  27237.00  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case6_x = x3.copy()\n",
    "case6_bimonth = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case6_bimonth = case6_bimonth.drop(['seasonal'], axis=1)\n",
    "case6_bimonth = case6_bimonth.rename(columns={'bimonthly':'Bimonth','month':'Month'})\n",
    "\n",
    "case6_x = pd.merge(case6_x, case6_bimonth, on='Month', how='left')\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case6 = case6_x.groupby(['Region','Bimonth','Day','4-hr'],as_index=False).agg(aggregations)\n",
    "case6.columns = case6.columns.droplevel(0)\n",
    "case6.columns = ['Region','Bimonth','Day','4-hr','Hour_Tot','Tot','Avg']\n",
    "print(case6.head())\n",
    "print('number of rows in dataset =',case6.shape[0])\n",
    "case6.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_2daytype_bimonth_4hr.csv')\n",
    "print()\n",
    "\n",
    "case6_x2 = pd.merge(case6_x,case6,on=['Region','Bimonth','Day','4-hr'],how='left')\n",
    "case6_x2 = case6_x2.sort_values(['Region',x_column])\n",
    "print(case6_x2.head())\n",
    "print('number of rows in dataset =',case6_x2.shape[0])\n",
    "case6_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_2daytype_bimonth_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 7: Season-based months, weekend/weekday day-types, 4-hour intervals\n",
    "#### Methodology: groupby function and applied season groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region    Season  Weekday  4-hr  Hour_Tot      Tot           Avg\n",
      "0  ERC_REST  shoulder    False     1       115  3399710  29562.695652\n",
      "1  ERC_REST  shoulder    False     2       144  4872764  33838.638889\n",
      "2  ERC_REST  shoulder    False     3       155  5809365  37479.774194\n",
      "3  ERC_REST  shoulder    False     4       115  4632532  40282.886957\n",
      "4  ERC_REST  shoulder    False     5       144  5663089  39327.006944\n",
      "number of rows in dataset = 2268\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1524  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "794   ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW  Weekday  4-hr  Season_Group  Hour_Tot       Tot  \\\n",
      "1545 2016-03-05    5    False     1             2       115   3399710   \n",
      "823  2016-02-04    3     True     1             2       373  11158266   \n",
      "1531 2016-03-04    4     True     1             2       373  11158266   \n",
      "1524 2016-03-04    4     True     1             2       373  11158266   \n",
      "794  2016-02-03    2     True     1             2       373  11158266   \n",
      "\n",
      "               Avg  \n",
      "1545  29562.695652  \n",
      "823   29914.922252  \n",
      "1531  29914.922252  \n",
      "1524  29914.922252  \n",
      "794   29914.922252  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case7_x = x3.copy()\n",
    "case7_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case7_seasons = case7_seasons.drop(['bimonthly'], axis=1)\n",
    "case7_seasons = case7_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "\n",
    "case7_x = pd.merge(case7_x, case7_seasons, on='Month', how='left')\n",
    "#print(case4_load)\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case7 = case7_x.groupby(['Region','Season','Weekday','4-hr'],as_index=False).agg(aggregations)\n",
    "case7.columns = case7.columns.droplevel(0)\n",
    "case7.columns = ['Region','Season','Weekday','4-hr','Hour_Tot','Tot','Avg']\n",
    "print(case7.head())\n",
    "print('number of rows in dataset =',case7.shape[0])\n",
    "case7.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_2daytype_season_4hr.csv')\n",
    "print()\n",
    "\n",
    "case7_x2 = pd.merge(case7_x,case7,on=['Region','Season','Weekday','4-hr'],how='left')\n",
    "case7_x2 = case7_x2.sort_values(['Region',x_column])\n",
    "print(case7_x2.head())\n",
    "print('number of rows in dataset =',case7_x2.shape[0])\n",
    "case7_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_2daytype_season_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day type: Three (Weekday, Weekend, Peak Load)\n",
    "## Find Peak Load Days in Each Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Region R_Group R_Subgroup  Season  Month  Day  Hour   Load     Tot  \\\n",
      "0       ERC_REST     ERC       REST  winter      1    1     1  34807  917588   \n",
      "1       ERC_REST     ERC       REST  winter      1    2     1  34716  863132   \n",
      "2       ERC_REST     ERC       REST  winter      1    3     1  34736  934513   \n",
      "3       ERC_REST     ERC       REST  winter      1    4     1  35914  960576   \n",
      "4       ERC_REST     ERC       REST  winter      1    5     1  33845  916216   \n",
      "...          ...     ...        ...     ...    ...  ...   ...    ...     ...   \n",
      "551875   WECC_WY     WEC         WY  winter     12  361    24   1767   51381   \n",
      "551876   WECC_WY     WEC         WY  winter     12  362    24   1744   49456   \n",
      "551877   WECC_WY     WEC         WY  winter     12  363    24   1805   48194   \n",
      "551878   WECC_WY     WEC         WY  winter     12  364    24   1791   47653   \n",
      "551879   WECC_WY     WEC         WY  winter     12  365    24   1834   47420   \n",
      "\n",
      "            Max  \n",
      "0       1000605  \n",
      "1       1000605  \n",
      "2       1000605  \n",
      "3       1000605  \n",
      "4       1000605  \n",
      "...         ...  \n",
      "551875    56554  \n",
      "551876    56554  \n",
      "551877    56554  \n",
      "551878    56554  \n",
      "551879    56554  \n",
      "\n",
      "[551880 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "#create temporary DF to find peak\n",
    "test = x.copy()\n",
    "\n",
    "#groupby region, month, and day to sum the total day\n",
    "aggregations1 = {x_column:sum}\n",
    "test_sum = test.groupby(['Region','Month','Day'],as_index=False).agg(aggregations1)\n",
    "#test1.columns = test1.columns.droplevel(0)\n",
    "test_sum.columns = ['Region','Month','Day','Tot']\n",
    "#print(test_sum.head())\n",
    "#print('number of rows in dataset =',test_sum.shape[0])\n",
    "\n",
    "test3 = pd.merge(test,test_sum,on=['Region','Month','Day'],how='left')\n",
    "#print(test3)\n",
    "\n",
    "#groupby region and month to find maximum \n",
    "aggregations2 = {'Tot':max}\n",
    "test_max = test_sum.groupby(['Region','Month'],as_index=False).agg(aggregations2)\n",
    "test_max.columns = ['Region','Month','Max']\n",
    "#print(test_max.head())\n",
    "\n",
    "test4 = pd.merge(test3,test_max,on=['Region','Month'],how='left')\n",
    "print(test4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Region R_Group R_Subgroup  Season  Month  Day  Hour   Load  \\\n",
      "0       ERC_REST     ERC       REST  winter      1    1     1  34807   \n",
      "1       ERC_REST     ERC       REST  winter      1    2     1  34716   \n",
      "2       ERC_REST     ERC       REST  winter      1    3     1  34736   \n",
      "3       ERC_REST     ERC       REST  winter      1    4     1  35914   \n",
      "4       ERC_REST     ERC       REST  winter      1    5     1  33845   \n",
      "...          ...     ...        ...     ...    ...  ...   ...    ...   \n",
      "551875   WECC_WY     WEC         WY  winter     12  361    24   1767   \n",
      "551876   WECC_WY     WEC         WY  winter     12  362    24   1744   \n",
      "551877   WECC_WY     WEC         WY  winter     12  363    24   1805   \n",
      "551878   WECC_WY     WEC         WY  winter     12  364    24   1791   \n",
      "551879   WECC_WY     WEC         WY  winter     12  365    24   1834   \n",
      "\n",
      "             Date  DOW Day_Type  \n",
      "0      2016-01-01    4  Weekday  \n",
      "1      2016-01-01    4  Weekday  \n",
      "2      2016-01-01    4  Weekday  \n",
      "3      2016-01-01    4  Weekday  \n",
      "4      2016-01-01    4  Weekday  \n",
      "...           ...  ...      ...  \n",
      "551875 2011-12-31    5  Weekend  \n",
      "551876 2011-12-31    5  Weekend  \n",
      "551877 2011-12-31    5  Weekend  \n",
      "551878 2011-12-31    5  Weekend  \n",
      "551879 2011-12-31    5  Weekend  \n",
      "\n",
      "[551880 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "x_peak = x2.copy()\n",
    "\n",
    "x_peak = pd.merge(x_peak,test4,on=['Region','R_Group','R_Subgroup','Season','Month','Day','Hour',x_column],how='left')\n",
    "x_peak = x_peak.rename(columns={'Weekday':'Day_Type'})\n",
    "\n",
    "#Return True if the load total equals the day identified as the max\n",
    "x_peak.loc[x_peak['Day_Type'] == True, 'Day_Type'] = 'Weekday'\n",
    "x_peak.loc[x_peak['Day_Type'] == False, 'Day_Type'] = 'Weekend'\n",
    "x_peak.loc[x_peak['Tot'] == x_peak['Max'], 'Day_Type'] = 'Peak'\n",
    "x_peak = x_peak.drop(['Tot','Max'], axis=1)\n",
    "print(x_peak)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Monthly, Weekend/weekday/peak day-types, 24 hours (864 segments)\n",
    "#### Methodology: similar to two day type, just adding in peak day types to sort by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Month Day_Type  Hour  Hour_Tot    Tot      Avg\n",
      "0  ERC_REST      1     Peak     1         1  38567  38567.0\n",
      "1  ERC_REST      1     Peak     2         1  39107  39107.0\n",
      "2  ERC_REST      1     Peak     3         1  40311  40311.0\n",
      "3  ERC_REST      1     Peak     4         1  43115  43115.0\n",
      "4  ERC_REST      1     Peak     5         1  47186  47186.0\n",
      "number of rows in dataset = 42612\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "2042  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "2233  ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1706  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "\n",
      "           Date  DOW Day_Type  Hour_Counter  Hour_Tot     Tot           Avg  \n",
      "2042 2016-03-05    5  Weekend          2043        13  376380  28952.307692  \n",
      "2233 2016-02-04    3  Weekday          2234        29  837721  28886.931034  \n",
      "1706 2016-03-04    4  Weekday          1707        17  480375  28257.352941  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case1_x = x_peak.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case1 = case1_x.groupby(['Region','Month','Day_Type','Hour'],as_index=False).agg(aggregations)\n",
    "case1.columns = case1.columns.droplevel(0)\n",
    "case1.columns = ['Region','Month','Day_Type','Hour','Hour_Tot','Tot','Avg']\n",
    "print(case1.head())\n",
    "print('number of rows in dataset =',case1.shape[0])\n",
    "case1.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_3daytype_monthly_24hr.csv')\n",
    "print()\n",
    "\n",
    "case1_x2 = pd.merge(case1_x,case1,on=['Region','Month','Day_Type','Hour'],how='left')\n",
    "case1_x2 = case1_x2.sort_values(['Region',x_column])\n",
    "print(case1_x2.head(3))\n",
    "print('number of rows in dataset =',case1_x2.shape[0])\n",
    "case1_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_3daytype_monthly_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: Season, weekend/weekday/peak day-types, 24-hours (216 segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region    Season Day_Type  Hour  Hour_Tot     Tot       Avg\n",
      "0  ERC_REST  shoulder     Peak     1         4  132690  33172.50\n",
      "1  ERC_REST  shoulder     Peak     2         4  130802  32700.50\n",
      "2  ERC_REST  shoulder     Peak     3         4  131751  32937.75\n",
      "3  ERC_REST  shoulder     Peak     4         4  137910  34477.50\n",
      "4  ERC_REST  shoulder     Peak     5         4  150015  37503.75\n",
      "number of rows in dataset = 12919\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "2042  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "2233  ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1706  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "\n",
      "           Date  DOW Day_Type  Hour_Counter  Hour_Tot     Tot           Avg  \n",
      "2042 2016-03-05    5  Weekend          2043        13  376380  28952.307692  \n",
      "2233 2016-02-04    3  Weekday          2234        29  837721  28886.931034  \n",
      "1706 2016-03-04    4  Weekday          1707        17  480375  28257.352941  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case2_x = x_peak.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case2 = case2_x.groupby(['Region','Season','Day_Type','Hour'],as_index=False).agg(aggregations)\n",
    "case2.columns = case2.columns.droplevel(0)\n",
    "case2.columns = ['Region','Season','Day_Type','Hour','Hour_Tot','Tot','Avg']\n",
    "print(case2.head())\n",
    "print('number of rows in dataset =',case2.shape[0])\n",
    "case2.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_3daytype_season_24hr.csv')\n",
    "print()\n",
    "\n",
    "case2_x2 = pd.merge(case2_x,case2,on=['Region','Season','Day_Type','Hour'],how='left')\n",
    "case2_x2 = case2_x2.sort_values(['Region',x_column])\n",
    "print(case1_x2.head(3))\n",
    "print('number of rows in dataset =',case1_x2.shape[0])\n",
    "case2_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_3daytype_season_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3: Annual, weekend/weekday/peak day-types, 24-hours (72 segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region Day_Type  Hour  Hour_Tot     Tot           Avg\n",
      "0  ERC_REST     Peak     1        12  455893  37991.083333\n",
      "1  ERC_REST     Peak     2        12  450804  37567.000000\n",
      "2  ERC_REST     Peak     3        12  454384  37865.333333\n",
      "3  ERC_REST     Peak     4        12  473371  39447.583333\n",
      "4  ERC_REST     Peak     5        12  505999  42166.583333\n",
      "number of rows in dataset = 4536\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "2042  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "2233  ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1706  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "\n",
      "           Date  DOW Day_Type  Hour_Counter  Hour_Tot      Tot           Avg  \n",
      "2042 2016-03-05    5  Weekend          2043        92  2899625  31517.663043  \n",
      "2233 2016-02-04    3  Weekday          2234       247  8133375  32928.643725  \n",
      "1706 2016-03-04    4  Weekday          1707       261  8694501  33312.264368  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case3_x = x_peak.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case3 = case3_x.groupby(['Region','Day_Type','Hour'],as_index=False).agg(aggregations)\n",
    "case3.columns = case3.columns.droplevel(0)\n",
    "case3.columns = ['Region','Day_Type','Hour','Hour_Tot','Tot','Avg']\n",
    "print(case3.head())\n",
    "print('number of rows in dataset =',case3.shape[0])\n",
    "case3.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_3daytype_annual_24hr.csv')\n",
    "print()\n",
    "\n",
    "case3_x2 = pd.merge(case3_x,case3,on=['Region','Day_Type','Hour'],how='left')\n",
    "case3_x2 = case3_x2.sort_values(['Region',x_column])\n",
    "print(case3_x2.head(3))\n",
    "print('number of rows in dataset =',case3_x2.shape[0])\n",
    "case3_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_3daytype_annual_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 hour interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Region R_Group R_Subgroup  Season  Month  Day  Hour   Load  \\\n",
      "0       ERC_REST     ERC       REST  winter      1    1     1  34807   \n",
      "1       ERC_REST     ERC       REST  winter      1    2     1  34716   \n",
      "2       ERC_REST     ERC       REST  winter      1    3     1  34736   \n",
      "3       ERC_REST     ERC       REST  winter      1    4     1  35914   \n",
      "4       ERC_REST     ERC       REST  winter      1    5     1  33845   \n",
      "...          ...     ...        ...     ...    ...  ...   ...    ...   \n",
      "551875   WECC_WY     WEC         WY  winter     12  361    24   1767   \n",
      "551876   WECC_WY     WEC         WY  winter     12  362    24   1744   \n",
      "551877   WECC_WY     WEC         WY  winter     12  363    24   1805   \n",
      "551878   WECC_WY     WEC         WY  winter     12  364    24   1791   \n",
      "551879   WECC_WY     WEC         WY  winter     12  365    24   1834   \n",
      "\n",
      "             Date  DOW Day_Type  4-hr  \n",
      "0      2016-01-01    4  Weekday     1  \n",
      "1      2016-01-01    4  Weekday     1  \n",
      "2      2016-01-01    4  Weekday     1  \n",
      "3      2016-01-01    4  Weekday     1  \n",
      "4      2016-01-01    4  Weekday     1  \n",
      "...           ...  ...      ...   ...  \n",
      "551875 2011-12-31    5  Weekend     6  \n",
      "551876 2011-12-31    5  Weekend     6  \n",
      "551877 2011-12-31    5  Weekend     6  \n",
      "551878 2011-12-31    5  Weekend     6  \n",
      "551879 2011-12-31    5  Weekend     6  \n",
      "\n",
      "[551880 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "#read in 4 hour interval counter\n",
    "interval_4hr = pd.read_csv('inputs/daytype_4hr.csv')\n",
    "\n",
    "x_peak2 = pd.merge(x_peak,interval_4hr,on='Hour',how='left').rename(columns={'4-hr_Day':'4-hr'})\n",
    "print(x_peak2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 4: Bi-monthly, weekend/weekday/peak day-types, 4-hour intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Bimonth Day_Type  4-hr  Hour_Tot     Tot        Avg\n",
      "0  ERC_REST        1     Peak     1         8  312443  39055.375\n",
      "1  ERC_REST        1     Peak     2         8  366535  45816.875\n",
      "2  ERC_REST        1     Peak     3         8  322104  40263.000\n",
      "3  ERC_REST        1     Peak     4         8  297348  37168.500\n",
      "4  ERC_REST        1     Peak     5         8  332533  41566.625\n",
      "number of rows in dataset = 6804\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1524  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "794   ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW Day_Type  4-hr  Bimonth  Hour_Tot      Tot           Avg  \n",
      "1545 2016-03-05    5  Weekend     1        2        62  1790938  28886.096774  \n",
      "823  2016-02-04    3  Weekday     1        2       174  5066815  29119.626437  \n",
      "1531 2016-03-04    4  Weekday     1        2       174  5066815  29119.626437  \n",
      "1524 2016-03-04    4  Weekday     1        2       174  5066815  29119.626437  \n",
      "794  2016-02-03    2  Weekday     1        2       174  5066815  29119.626437  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case4_x = x_peak2.copy()\n",
    "case4_bimonth = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case4_bimonth = case4_bimonth.drop(['seasonal'], axis=1)\n",
    "case4_bimonth = case4_bimonth.rename(columns={'bimonthly':'Bimonth','month':'Month'})\n",
    "\n",
    "case4_x = pd.merge(case4_x, case4_bimonth, on='Month', how='left')\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case4 = case4_x.groupby(['Region','Bimonth','Day_Type','4-hr'],as_index=False).agg(aggregations)\n",
    "case4.columns = case4.columns.droplevel(0)\n",
    "case4.columns = ['Region','Bimonth','Day_Type','4-hr','Hour_Tot','Tot','Avg']\n",
    "print(case4.head())\n",
    "print('number of rows in dataset =',case4.shape[0])\n",
    "case4.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_3daytype_bimonth_4hr.csv')\n",
    "print()\n",
    "\n",
    "case4_x2 = pd.merge(case4_x,case4,on=['Region','Bimonth','Day_Type','4-hr'],how='left')\n",
    "case4_x2 = case4_x2.sort_values(['Region',x_column])\n",
    "print(case4_x2.head())\n",
    "print('number of rows in dataset =',case4_x2.shape[0])\n",
    "case4_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_3daytype_bimonth_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 5: Season-based months, weekend/weekday/peak day-types, 4-hour intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Season_Group Day_Type  4-hr  Hour_Tot     Tot           Avg\n",
      "0  ERC_REST             1     Peak     1        12  504276  42023.000000\n",
      "1  ERC_REST             1     Peak     2        12  583360  48613.333333\n",
      "2  ERC_REST             1     Peak     3        12  527061  43921.750000\n",
      "3  ERC_REST             1     Peak     4        12  478413  39867.750000\n",
      "4  ERC_REST             1     Peak     5        12  530620  44218.333333\n",
      "number of rows in dataset = 5639\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1524  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "794   ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW Day_Type  4-hr  Season_Group  Hour_Tot       Tot  \\\n",
      "1545 2016-03-05    5  Weekend     1             2       111   3268235   \n",
      "823  2016-02-04    3  Weekday     1             2       361  10756588   \n",
      "1531 2016-03-04    4  Weekday     1             2       361  10756588   \n",
      "1524 2016-03-04    4  Weekday     1             2       361  10756588   \n",
      "794  2016-02-03    2  Weekday     1             2       361  10756588   \n",
      "\n",
      "               Avg  \n",
      "1545  29443.558559  \n",
      "823   29796.642659  \n",
      "1531  29796.642659  \n",
      "1524  29796.642659  \n",
      "794   29796.642659  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case5_x = x_peak2.copy()\n",
    "case5_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case5_seasons = case5_seasons.drop(['bimonthly'], axis=1)\n",
    "case5_seasons = case5_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "\n",
    "case5_x = pd.merge(case5_x, case5_seasons, on='Month', how='left')\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case5 = case5_x.groupby(['Region','Season_Group','Day_Type','4-hr'],as_index=False).agg(aggregations)\n",
    "case5.columns = case5.columns.droplevel(0)\n",
    "case5.columns = ['Region','Season_Group','Day_Type','4-hr','Hour_Tot','Tot','Avg']\n",
    "print(case5.head())\n",
    "print('number of rows in dataset =',case5.shape[0])\n",
    "case5.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_3daytype_seasonbased_4hr.csv')\n",
    "print()\n",
    "\n",
    "case5_x2 = pd.merge(case5_x,case5,on=['Region','Season_Group','Day_Type','4-hr'],how='left')\n",
    "case5_x2 = case5_x2.sort_values(['Region',x_column])\n",
    "print(case5_x2.head())\n",
    "print('number of rows in dataset =',case5_x2.shape[0])\n",
    "case5_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_3daytype_seasonbased_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 6: Season, weekend/weekday/peak day-types, 4-hour intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region    Season Day_Type  4-hr  Hour_Tot     Tot         Avg\n",
      "0  ERC_REST  shoulder     Peak     1        16  533153  33322.0625\n",
      "1  ERC_REST  shoulder     Peak     2        16  619476  38717.2500\n",
      "2  ERC_REST  shoulder     Peak     3        16  707544  44221.5000\n",
      "3  ERC_REST  shoulder     Peak     4        16  785332  49083.2500\n",
      "4  ERC_REST  shoulder     Peak     5        16  743101  46443.8125\n",
      "number of rows in dataset = 3402\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1524  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "794   ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW Day_Type  4-hr  Hour_Tot       Tot           Avg  \n",
      "1545 2016-03-05    5  Weekend     1       111   3268235  29443.558559  \n",
      "823  2016-02-04    3  Weekday     1       361  10756588  29796.642659  \n",
      "1531 2016-03-04    4  Weekday     1       361  10756588  29796.642659  \n",
      "1524 2016-03-04    4  Weekday     1       361  10756588  29796.642659  \n",
      "794  2016-02-03    2  Weekday     1       361  10756588  29796.642659  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case6_x = x_peak2.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case6 = case6_x.groupby(['Region','Season','Day_Type','4-hr'],as_index=False).agg(aggregations)\n",
    "case6.columns = case6.columns.droplevel(0)\n",
    "case6.columns = ['Region','Season','Day_Type','4-hr','Hour_Tot','Tot','Avg']\n",
    "print(case6.head())\n",
    "print('number of rows in dataset =',case6.shape[0])\n",
    "case6.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_3daytype_season_4hr.csv')\n",
    "print()\n",
    "\n",
    "case6_x2 = pd.merge(case6_x,case6,on=['Region','Season','Day_Type','4-hr'],how='left')\n",
    "case6_x2 = case6_x2.sort_values(['Region',x_column])\n",
    "print(case6_x2.head())\n",
    "print('number of rows in dataset =',case6_x2.shape[0])\n",
    "case6_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_3daytype_season_4hr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
