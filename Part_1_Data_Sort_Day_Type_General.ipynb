{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data here: \n",
    "## One and two day type \n",
    "## Three day type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day Type: One/two\n",
    "## Create Date and Day of Week columns in the DF"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
||||||| merged common ancestors
   "execution_count": 1,
=======
   "execution_count": 208,
>>>>>>> master
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output files are written out in parent directory: C:\\Users\\cmarcy\\Desktop\\py_projects\\temporal\\temporal_working\\outputs\n"
     ]
    }
   ],
   "source": [
    "#importing packages needed for analysis\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from pandas import DataFrame\n",
    "\n",
    "path = os.getcwd()\n",
    "#print(path)\n",
    "\n",
    "#this code creates an output directory in the parent director, if one does not exist yet\n",
    "#Note: this is where all of the output files will be written, since outputs are large this saves space in git\n",
    "path = os.getcwd()\n",
    "parent = os.path.dirname(path)\n",
    "outputs_dir = parent+'\\outputs'\n",
    "if not os.path.exists(outputs_dir):\n",
    "    os.makedirs(outputs_dir)\n",
    "print('output files are written out in parent directory: '+outputs_dir)\n",
    "\n",
    "load_dur = pd.read_csv('../outputs/load_long_format.csv')\n",
    "solar_dur = pd.read_csv('../outputs/solar_long_format.csv')\n",
    "wind_dur = pd.read_csv('../outputs/wind_long_format.csv')\n",
    "\n",
    "## UNCOMMENT WHICH PROFILE TO BE USED\n",
    "x = load_dur\n",
    "x_name = 'load'\n",
    "x_name2 = 'Load'\n",
    "x_column = 'Load'\n",
    "\n",
    "#x = solar_dur\n",
    "#x_name = 'solar'\n",
    "#x_name2 = 'Solar_Gen'\n",
    "#choose TRG \n",
    "#x_column = 'TRG6'\n",
    "\n",
    "#x = wind_dur\n",
    "#x_name = 'wind'\n",
    "#x_name2 = 'Wind_Gen'\n",
    "#x_column = 'TRG4'\n",
    "\n",
    "x = x[['Region','R_Group','R_Subgroup','Season','Month','Day','Hour',x_column]]\n",
    "years = pd.read_csv('inputs/years.csv').dropna()\n",
    "#print(x.head())\n",
    "\n",
<<<<<<< HEAD
    "unique_r = pd.Series(load_dur['Region'].unique()).dropna()\n",
    "#print(unique_r)\n",
    "reg_count = unique_r.shape[0]"
||||||| merged common ancestors
    "#print(x)"
=======
    "#print(x.head())"
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
||||||| merged common ancestors
   "execution_count": 2,
=======
   "execution_count": 188,
>>>>>>> master
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region R_Group R_Subgroup  Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "0  ERC_REST     ERC       REST  winter      1    1     1  34807    1  2016   \n",
      "1  ERC_REST     ERC       REST  winter      1    2     1  34716    2  2016   \n",
      "2  ERC_REST     ERC       REST  winter      1    3     1  34736    3  2016   \n",
      "3  ERC_REST     ERC       REST  winter      1    4     1  35914    4  2016   \n",
      "4  ERC_REST     ERC       REST  winter      1    5     1  33845    5  2016   \n",
      "\n",
      "        Date  DOW  Weekday  \n",
      "0 2016-01-01    4     True  \n",
      "1 2016-01-02    5    False  \n",
      "2 2016-01-03    6    False  \n",
      "3 2016-01-04    0     True  \n",
      "4 2016-01-05    1     True  \n"
     ]
    }
   ],
||||||| merged common ancestors
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region R_Group R_Subgroup  Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "0   FRCC    FRCC       FRCC  winter      1    1     1  17005    1  2011   \n",
      "1   FRCC    FRCC       FRCC  winter      1    2     1  15750    2  2011   \n",
      "2   FRCC    FRCC       FRCC  winter      1    3     1  16123    3  2011   \n",
      "3   FRCC    FRCC       FRCC  winter      1    4     1  18300    4  2011   \n",
      "4   FRCC    FRCC       FRCC  winter      1    5     1  17370    5  2011   \n",
      "\n",
      "        Date  DOW  Weekday  \n",
      "0 2011-01-01    5    False  \n",
      "1 2011-01-02    6    False  \n",
      "2 2011-01-03    0     True  \n",
      "3 2011-01-04    1     True  \n",
      "4 2011-01-05    2     True  \n"
     ]
    }
   ],
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
   "source": [
<<<<<<< HEAD
    "import datetime\n",
    "daydata = pd.read_csv('inputs/days_365.csv')\n",
    "#print(daydata.tail())\n",
    "daydata = daydata.drop(columns='Month')\n",
    "x2 = pd.merge(x,daydata,on=['Day'],how='left')\n",
    "#print(x2.tail())\n",
||||||| merged common ancestors
    "from itertools import cycle\n",
    "\n",
    "#repeat the date for 24 times for each hour in the day\n",
    "year_2016 = years['2016'].repeat(24).reset_index(drop=True)\n",
    "year_2011 = years['2011'].repeat(24).reset_index(drop=True)\n",
    "#print(year_2016)\n",
    "\n",
    "#use itertools.cycle to make the date repeat until it reaches the end of the dataframe\n",
    "year_2016 = cycle(year_2016)\n",
    "year_2011 = cycle(year_2011)\n",
=======
    "import datetime\n",
    "daydata = pd.read_csv('inputs/days_365.csv')\n",
    "#print(daydata.tail())\n",
    "daydata = daydata.drop(columns='Month')\n",
    "x2 = pd.merge(x,daydata,on=['Day'],how='left')\n",
    "#print(x2.tail())\n",
    "\n",
    "#sets the year for each region\n",
    "x2['Year']=2011\n",
    "x2.loc[x2['R_Group'] == 'ERC', 'Year'] = 2016\n",
    "#print(x2.head())\n",
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
    "\n",
<<<<<<< HEAD
    "#sets the year for each region\n",
    "x2['Year']=2011\n",
    "x2.loc[x2['R_Group'] == 'ERC', 'Year'] = 2016\n",
    "#print(x2.head())\n",
    "\n",
    "#Creates a date column\n",
    "x2 = x2.rename(columns={'Day':'DOY','DayofMo':'Day'})\n",
    "x2['Date']=pd.to_datetime(x2[['Year', 'Month', 'Day']], errors='coerce')\n",
    "#print(x2.tail())\n",
||||||| merged common ancestors
    "#create temporary data frames where they will iterate through. Needed to achieve the correct length of the DF\n",
    "temp2016 = x.loc[x['R_Group'] == 'ERC'].copy()\n",
    "temp2011 = x.loc[x['R_Group'] != 'ERC'].copy()\n",
    "\n",
    "#iterate through the date until the end of DF \n",
    "temp2016['Date'] = [next(year_2016) for year in range(len(temp2016))]\n",
    "temp2011['Date'] = [next(year_2011) for year in range(len(temp2011))]\n",
    "#print(temp2016)\n",
    "#print(temp2011)\n",
    "\n",
    "x2 = pd.concat([temp2016, temp2011], ignore_index=True)\n",
    "#print(x2)\n",
=======
    "#Creates a date column\n",
    "x2 = x2.rename(columns={'Day':'DOY','DayofMo':'Day'})\n",
    "x2['Date']=pd.to_datetime(x2[['Year', 'Month', 'Day']], errors='coerce')\n",
    "#print(x2.tail())\n",
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
    "\n",
    "#convert date to a datetime type \n",
    "#x2['Date'] = pd.to_datetime(x2['Date'])\n",
    "x2['DOW'] = x2['Date'].dt.weekday\n",
    "\n",
    "#check if it is a weekday or not \n",
    "weekday = pd.read_csv('inputs/weekday.csv')\n",
    "x2 = pd.merge(x2,weekday,on='DOW',how='left')\n",
    "print(x2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Monthly, single day type, 24 hours (288 segments)\n",
    "#### Methodology: Using groupby function to group first by month, then by 24 hours"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
||||||| merged common ancestors
   "execution_count": 3,
=======
   "execution_count": 4,
>>>>>>> master
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "     Region  Month  Hour  Hour_Tot  Load_Tot      Load_Avg\n",
      "0  ERC_REST      1     1        31   1042446  33627.290323\n",
      "1  ERC_REST      1     2        31   1051661  33924.548387\n",
      "2  ERC_REST      1     3        31   1079739  34830.290323\n",
      "3  ERC_REST      1     4        31   1144492  36919.096774\n",
      "4  ERC_REST      1     5        31   1239385  39980.161290\n",
      "number of segments in dataset = 288.0\n",
      "\n",
      "     R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "1545     ERC       REST  shoulder      3   86     3  26989   27  2016   \n",
      "823      ERC       REST  shoulder      4   94     2  27006    4  2016   \n",
      "1531     ERC       REST  shoulder      3   72     3  27025   13  2016   \n",
      "\n",
      "           Date  DOW  Weekday  Hour_Tot  Load_Tot      Load_Avg  \n",
      "1545 2016-03-27    6    False        31    886708  28603.483871  \n",
      "823  2016-04-04    0     True        30    870591  29019.700000  \n",
      "1531 2016-03-13    6    False        31    886708  28603.483871  \n",
      "number of rows in dataset = 17520\n"
||||||| merged common ancestors
      "     Region  Month  Hour  Hour_Tot  Wind_Gen_Tot  Wind_Gen_Avg\n",
      "0  ERC_PHDL      1     1        31       17461.0    563.258065\n",
      "1  ERC_PHDL      1     2        31       17975.0    579.838710\n",
      "2  ERC_PHDL      1     3        31       18119.0    584.483871\n",
      "3  ERC_PHDL      1     4        31       18400.0    593.548387\n",
      "4  ERC_PHDL      1     5        31       18581.0    599.387097\n",
      "number of rows in dataset = 16704\n",
      "\n",
      "     R_Group R_Subgroup  Season  Month  Day  Hour  TRG4       Date  DOW  \\\n",
      "5913     ERC       PHDL  summer      9  247    10  33.0 2016-09-03    5   \n",
      "5914     ERC       PHDL  summer      9  247    11  49.0 2016-09-03    5   \n",
      "4883     ERC       PHDL  summer      7  204    12  51.0 2016-07-22    4   \n",
      "\n",
      "      Weekday  Hour_Tot  Wind_Gen_Tot  Wind_Gen_Avg  \n",
      "5913    False        30        8956.0    298.533333  \n",
      "5914    False        30        8981.0    299.366667  \n",
      "4883     True        31        6916.0    223.096774  \n",
      "number of rows in dataset = 867240\n"
=======
      "  Region  Month  Hour  Hour_Tot  Load_Tot      Load_Avg\n",
      "0   FRCC      1     1        31    626801  20219.387097\n",
      "1   FRCC      1     2        31    644817  20800.548387\n",
      "2   FRCC      1     3        31    694961  22418.096774\n",
      "3   FRCC      1     4        31    781019  25194.161290\n",
      "4   FRCC      1     5        31    836184  26973.677419\n",
      "number of rows in dataset = 288\n",
      "\n",
      "    R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "366    FRCC       FRCC    winter      1    2     2  15694    2  2011   \n",
      "73     FRCC       FRCC  shoulder      3   74     1  15738   15  2011   \n",
      "1      FRCC       FRCC    winter      1    2     1  15750    2  2011   \n",
      "\n",
      "          Date  DOW  Weekday  Hour_Tot  Load_Tot      Load_Avg  \n",
      "366 2011-01-02    6    False        31    644817  20800.548387  \n",
      "73  2011-03-15    1     True        31    519021  16742.612903  \n",
      "1   2011-01-02    6    False        31    626801  20219.387097  \n",
      "number of rows in dataset = 8760\n"
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
     ]
    }
   ],
   "source": [
    "case1_x = x2.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case1 = case1_x.groupby(['Region','Month','Hour'],as_index=False).agg(aggregations)\n",
    "case1.columns = case1.columns.droplevel(0)\n",
    "case1.columns = ['Region','Month','Hour','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case1.head())\n",
<<<<<<< HEAD
    "print('number of segments in dataset =',case1.shape[0]/reg_count)\n",
    "case1.to_csv('../outputs/'+x_name+'_segments_1daytype_monthly_24hr.csv')\n",
||||||| merged common ancestors
    "print('number of rows in dataset =',case1.shape[0])\n",
    "case1.to_csv('../outputs/'+x_name+'_segments_2daytype_monthly_24hr.csv')\n",
=======
    "print('number of rows in dataset =',case1.shape[0])\n",
    "case1.to_csv('../outputs/'+x_name+'_segments_1daytype_monthly_24hr.csv')\n",
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
    "print()\n",
    "\n",
    "case1_x2 = pd.merge(case1_x,case1,on=['Region','Month','Hour'],how='left')\n",
    "case1_x2 = case1_x2.sort_values(['Region',x_column]).drop(case1_x2.columns[0], axis=1)\n",
    "print(case1_x2.head(3))\n",
    "print('number of rows in dataset =',case1_x2.shape[0])\n",
    "case1_x2.to_csv('../outputs/'+x_name+'_8760_1daytype_monthly_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: Season, single day-type, 24 hours (72 segments)\n",
    "#### Methodology: Use groupby function to group by season and hour"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
||||||| merged common ancestors
   "execution_count": 4,
=======
   "execution_count": 5,
>>>>>>> master
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "     Region    Season  Hour  Hour_Tot  Load_Tot      Load_Avg\n",
      "0  ERC_REST  shoulder     1       122   3635975  29803.073770\n",
      "1  ERC_REST  shoulder     2       122   3587428  29405.147541\n",
      "2  ERC_REST  shoulder     3       122   3603145  29533.975410\n",
      "3  ERC_REST  shoulder     4       122   3731428  30585.475410\n",
      "4  ERC_REST  shoulder     5       122   3992001  32721.319672\n",
      "number of segments in dataset = 72.0\n",
      "\n",
      "     R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "1545     ERC       REST  shoulder      3   86     3  26989   27  2016   \n",
      "823      ERC       REST  shoulder      4   94     2  27006    4  2016   \n",
      "1531     ERC       REST  shoulder      3   72     3  27025   13  2016   \n",
      "\n",
      "           Date  DOW  Weekday  Hour_Tot  Load_Tot      Load_Avg  \n",
      "1545 2016-03-27    6    False       122   3603145  29533.975410  \n",
      "823  2016-04-04    0     True       122   3587428  29405.147541  \n",
      "1531 2016-03-13    6    False       122   3603145  29533.975410  \n",
      "number of rows in dataset = 17520\n"
||||||| merged common ancestors
      "     Region  Season_Group  Hour  Hour_Tot  Wind_Gen_Tot  Wind_Gen_Avg\n",
      "0  ERC_PHDL             1     1        90       50327.0    559.188889\n",
      "1  ERC_PHDL             1     2        90       51509.0    572.322222\n",
      "2  ERC_PHDL             1     3        90       52196.0    579.955556\n",
      "3  ERC_PHDL             1     4        90       52972.0    588.577778\n",
      "4  ERC_PHDL             1     5        90       53882.0    598.688889\n",
      "number of rows in dataset = 6960\n",
      "\n",
      "     R_Group R_Subgroup  Season  Month  Day  Hour  TRG4       Date  DOW  \\\n",
      "5913     ERC       PHDL  summer      9  247    10  33.0 2016-09-03    5   \n",
      "5914     ERC       PHDL  summer      9  247    11  49.0 2016-09-03    5   \n",
      "4883     ERC       PHDL  summer      7  204    12  51.0 2016-07-22    4   \n",
      "\n",
      "      Weekday  Season_Group  Hour_Tot  Wind_Gen_Tot  Wind_Gen_Avg  \n",
      "5913    False             3        61       20009.0    328.016393  \n",
      "5914    False             3        61       20059.0    328.836066  \n",
      "4883     True             5        62       13256.0    213.806452  \n",
      "number of rows in dataset = 867240\n"
=======
      "  Region  Season_Group  Hour  Hour_Tot  Load_Tot      Load_Avg\n",
      "0   FRCC             1     1        90   1659148  18434.977778\n",
      "1   FRCC             1     2        90   1694864  18831.822222\n",
      "2   FRCC             1     3        90   1817817  20197.966667\n",
      "3   FRCC             1     4        90   2039870  22665.222222\n",
      "4   FRCC             1     5        90   2183989  24266.544444\n",
      "number of rows in dataset = 120\n",
      "\n",
      "    R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "366    FRCC       FRCC    winter      1    2     2  15694    2  2011   \n",
      "73     FRCC       FRCC  shoulder      3   74     1  15738   15  2011   \n",
      "1      FRCC       FRCC    winter      1    2     1  15750    2  2011   \n",
      "\n",
      "          Date  DOW  Weekday  Season_Group  Hour_Tot  Load_Tot      Load_Avg  \n",
      "366 2011-01-02    6    False             1        90   1694864  18831.822222  \n",
      "73  2011-03-15    1     True             2       122   2159594  17701.590164  \n",
      "1   2011-01-02    6    False             1        90   1659148  18434.977778  \n",
      "number of rows in dataset = 8760\n"
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
     ]
    }
   ],
   "source": [
    "case2_x = x2.copy()\n",
<<<<<<< HEAD
    "#case2_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "#case2_seasons = case2_seasons.drop(['bimonthly'], axis=1)\n",
    "#case2_seasons = case2_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "#print(case2_x.head())\n",
    "#case2_x = pd.merge(case2_x, case2_seasons, on='Month', how='left')\n",
||||||| merged common ancestors
    "case2_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case2_seasons = case2_seasons.drop(['bimonthly'], axis=1)\n",
    "case2_seasons = case2_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "\n",
    "case2_x = pd.merge(case2_x, case2_seasons, on='Month', how='left')\n",
=======
    "case2_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case2_seasons = case2_seasons.drop(['bimonthly'], axis=1)\n",
    "case2_seasons = case2_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "#print(case2_seasons)\n",
    "\n",
    "case2_x = pd.merge(case2_x, case2_seasons, on='Month', how='left')\n",
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case2 = case2_x.groupby(['Region','Season','Hour'],as_index=False).agg(aggregations)\n",
    "case2.columns = case2.columns.droplevel(0)\n",
    "case2.columns = ['Region','Season','Hour','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case2.head())\n",
<<<<<<< HEAD
    "print('number of segments in dataset =',case2.shape[0]/reg_count)\n",
    "case2.to_csv('../outputs/'+x_name+'_segments_1daytype_season_24hr.csv')\n",
||||||| merged common ancestors
    "print('number of rows in dataset =',case2.shape[0])\n",
    "case2.to_csv('../outputs/'+x_name+'_segments_2daytype_season_24hr.csv')\n",
=======
    "print('number of rows in dataset =',case2.shape[0])\n",
    "case2.to_csv('../outputs/'+x_name+'_segments_1daytype_season_24hr.csv')\n",
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
    "print()\n",
    "\n",
    "case2_x2 = pd.merge(case2_x,case2,on=['Region','Season','Hour'],how='left')\n",
    "case2_x2 = case2_x2.sort_values(['Region',x_column]).drop(case2_x2.columns[0],axis=1)\n",
    "print(case2_x2.head(3))\n",
    "print('number of rows in dataset =',case2_x2.shape[0])\n",
    "case2_x2.to_csv('../outputs/'+x_name+'_8760_1daytype_season_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3: Monthly, weekend/weekday, 24 hours (576 segments)\n",
    "#### Metholodogy: Use groupby function to group by month, then weekend/weekday, then by 24 hours"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
||||||| merged common ancestors
   "execution_count": 5,
=======
   "execution_count": 6,
>>>>>>> master
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "     Region  Month  Hour  Hour_Tot  Load_Tot      Load_Avg\n",
      "0  ERC_REST      1     1        31   1042446  33627.290323\n",
      "1  ERC_REST      1     2        31   1051661  33924.548387\n",
      "2  ERC_REST      1     3        31   1079739  34830.290323\n",
      "3  ERC_REST      1     4        31   1144492  36919.096774\n",
      "4  ERC_REST      1     5        31   1239385  39980.161290\n",
      "number of segments in dataset = 576.0\n",
      "\n",
      "     R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "1545     ERC       REST  shoulder      3   86     3  26989   27  2016   \n",
      "823      ERC       REST  shoulder      4   94     2  27006    4  2016   \n",
      "1531     ERC       REST  shoulder      3   72     3  27025   13  2016   \n",
      "1524     ERC       REST  shoulder      3   65     3  27059    6  2016   \n",
      "794      ERC       REST  shoulder      3   65     2  27069    6  2016   \n",
      "\n",
      "           Date  DOW  Weekday  Hour_Tot  Load_Tot      Load_Avg  \n",
      "1545 2016-03-27    6    False         8    222011  27751.375000  \n",
      "823  2016-04-04    0     True        21    612083  29146.809524  \n",
      "1531 2016-03-13    6    False         8    222011  27751.375000  \n",
      "1524 2016-03-06    6    False         8    222011  27751.375000  \n",
      "794  2016-03-06    6    False         8    221751  27718.875000  \n",
      "number of rows in dataset = 17520\n"
||||||| merged common ancestors
      "     Region  Month  Hour  Hour_Tot  Wind_Gen_Tot  Wind_Gen_Avg\n",
      "0  ERC_PHDL      1     1        31       17461.0    563.258065\n",
      "1  ERC_PHDL      1     2        31       17975.0    579.838710\n",
      "2  ERC_PHDL      1     3        31       18119.0    584.483871\n",
      "3  ERC_PHDL      1     4        31       18400.0    593.548387\n",
      "4  ERC_PHDL      1     5        31       18581.0    599.387097\n",
      "number of rows in dataset = 29418\n",
      "\n",
      "     R_Group R_Subgroup  Season  Month  Day  Hour  TRG4       Date  DOW  \\\n",
      "5913     ERC       PHDL  summer      9  247    10  33.0 2016-09-03    5   \n",
      "5914     ERC       PHDL  summer      9  247    11  49.0 2016-09-03    5   \n",
      "4883     ERC       PHDL  summer      7  204    12  51.0 2016-07-22    4   \n",
      "4880     ERC       PHDL  summer      7  204     9  53.0 2016-07-22    4   \n",
      "5028     ERC       PHDL  summer      7  210    13  55.0 2016-07-28    3   \n",
      "\n",
      "      Weekday  Hour_Tot  Wind_Gen_Tot  Wind_Gen_Avg  \n",
      "5913    False         8        2380.0    297.500000  \n",
      "5914    False         8        2360.0    295.000000  \n",
      "4883     True        22        4361.0    198.227273  \n",
      "4880     True        22        5147.0    233.954545  \n",
      "5028     True        22        4497.0    204.409091  \n",
      "number of rows in dataset = 867240\n"
=======
      "  Region  Month  Hour  Hour_Tot  Load_Tot      Load_Avg\n",
      "0   FRCC      1     1        31    626801  20219.387097\n",
      "1   FRCC      1     2        31    644817  20800.548387\n",
      "2   FRCC      1     3        31    694961  22418.096774\n",
      "3   FRCC      1     4        31    781019  25194.161290\n",
      "4   FRCC      1     5        31    836184  26973.677419\n",
      "number of rows in dataset = 576\n",
      "\n",
      "    R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "366    FRCC       FRCC    winter      1    2     2  15694    2  2011   \n",
      "73     FRCC       FRCC  shoulder      3   74     1  15738   15  2011   \n",
      "1      FRCC       FRCC    winter      1    2     1  15750    2  2011   \n",
      "443    FRCC       FRCC  shoulder      3   79     2  15822   20  2011   \n",
      "72     FRCC       FRCC  shoulder      3   73     1  15831   14  2011   \n",
      "\n",
      "          Date  DOW  Weekday  Hour_Tot  Load_Tot      Load_Avg  \n",
      "366 2011-01-02    6    False        10    212142  21214.200000  \n",
      "73  2011-03-15    1     True        23    382919  16648.652174  \n",
      "1   2011-01-02    6    False        10    208342  20834.200000  \n",
      "443 2011-03-20    6    False         8    136110  17013.750000  \n",
      "72  2011-03-14    0     True        23    382919  16648.652174  \n",
      "number of rows in dataset = 8760\n"
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
     ]
    }
   ],
   "source": [
    "case3_x = x2.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case3 = case3_x.groupby(['Region','Month','Weekday','Hour'],as_index=False).agg(aggregations)\n",
    "case3.columns = case3.columns.droplevel(0)\n",
    "case3.columns = ['Region','Month','Weekday','Hour','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case1.head())\n",
<<<<<<< HEAD
    "print('number of segments in dataset =',case3.shape[0]/reg_count)\n",
    "case3.to_csv('../outputs/'+x_name+'_segments_2daytype_monthly_24hr.csv')\n",
||||||| merged common ancestors
    "print('number of rows in dataset =',case3.shape[0])\n",
    "case3.to_csv('../outputs/'+x_name+'_segments_2daytype_monthly_wkd.csv')\n",
=======
    "print('number of rows in dataset =',case3.shape[0])\n",
    "case3.to_csv('../outputs/'+x_name+'_segments_2daytype_monthly_24hr.csv')\n",
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
    "print()\n",
    "\n",
    "case3_x2 = pd.merge(case3_x,case3,on=['Region','Month','Weekday','Hour'],how='left')\n",
    "case3_x2 = case3_x2.sort_values(['Region',x_column]).drop(case3_x2.columns[0],axis=1)\n",
    "print(case3_x2.head())\n",
    "print('number of rows in dataset =',case3_x2.shape[0])\n",
    "case3_x2.to_csv('../outputs/'+x_name+'_8760_2daytype_monthly_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 4: Season, weekend/weekday, 24 hours (144 segments)\n",
    "#### Methodology: groupby season, weekday, hour"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
||||||| merged common ancestors
   "execution_count": 6,
=======
   "execution_count": 7,
>>>>>>> master
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "       Region  Season  Weekday  Hour  Hour_Tot  Load_Tot     Load_Avg\n",
      "283  ERC_WEST  winter     True    20        63    205537  3262.492063\n",
      "284  ERC_WEST  winter     True    21        63    199198  3161.873016\n",
      "285  ERC_WEST  winter     True    22        63    191477  3039.317460\n",
      "286  ERC_WEST  winter     True    23        63    184976  2936.126984\n",
      "287  ERC_WEST  winter     True    24        63    182272  2893.206349\n",
      "number of segments in dataset = 144.0\n",
      "\n",
      "     R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "1545     ERC       REST  shoulder      3   86     3  26989   27  2016   \n",
      "823      ERC       REST  shoulder      4   94     2  27006    4  2016   \n",
      "1531     ERC       REST  shoulder      3   72     3  27025   13  2016   \n",
      "1524     ERC       REST  shoulder      3   65     3  27059    6  2016   \n",
      "794      ERC       REST  shoulder      3   65     2  27069    6  2016   \n",
      "\n",
      "           Date  DOW  Weekday  Hour_Tot  Load_Tot      Load_Avg  \n",
      "1545 2016-03-27    6    False        35   1003593  28674.085714  \n",
      "823  2016-04-04    0     True        87   2578430  29637.126437  \n",
      "1531 2016-03-13    6    False        35   1003593  28674.085714  \n",
      "1524 2016-03-06    6    False        35   1003593  28674.085714  \n",
      "794  2016-03-06    6    False        35   1008998  28828.514286  \n",
      "number of rows in dataset = 17520\n"
||||||| merged common ancestors
      "     Region  Season_Group  Weekday  Hour  Hour_Tot  Wind_Gen_Tot  Wind_Gen_Avg\n",
      "0  ERC_PHDL             1    False     1        26       14093.0    542.038462\n",
      "1  ERC_PHDL             1    False     2        26       14612.0    562.000000\n",
      "2  ERC_PHDL             1    False     3        26       15178.0    583.769231\n",
      "3  ERC_PHDL             1    False     4        26       15584.0    599.384615\n",
      "4  ERC_PHDL             1    False     5        26       16131.0    620.423077\n",
      "number of rows in dataset = 13036\n",
      "\n",
      "     R_Group R_Subgroup  Season  Month  Day  Hour  TRG4       Date  DOW  \\\n",
      "5913     ERC       PHDL  summer      9  247    10  33.0 2016-09-03    5   \n",
      "5914     ERC       PHDL  summer      9  247    11  49.0 2016-09-03    5   \n",
      "4883     ERC       PHDL  summer      7  204    12  51.0 2016-07-22    4   \n",
      "4880     ERC       PHDL  summer      7  204     9  53.0 2016-07-22    4   \n",
      "5028     ERC       PHDL  summer      7  210    13  55.0 2016-07-28    3   \n",
      "\n",
      "      Weekday  Season_Group  Hour_Tot  Wind_Gen_Tot  Wind_Gen_Avg  \n",
      "5913    False             3        18        5958.0    331.000000  \n",
      "5914    False             3        18        5983.0    332.388889  \n",
      "4883     True             5        44        8956.0    203.545455  \n",
      "4880     True             5        44       10407.0    236.522727  \n",
      "5028     True             5        44        9287.0    211.068182  \n",
      "number of rows in dataset = 867240\n"
=======
      "    Region  Season_Group  Weekday  Hour  Hour_Tot  Load_Tot      Load_Avg\n",
      "235   FRCC             5     True    20        44   1435196  32618.090909\n",
      "236   FRCC             5     True    21        44   1307046  29705.590909\n",
      "237   FRCC             5     True    22        44   1191436  27078.090909\n",
      "238   FRCC             5     True    23        44   1111291  25256.613636\n",
      "239   FRCC             5     True    24        44   1057027  24023.340909\n",
      "number of rows in dataset = 240\n",
      "\n",
      "    R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "366    FRCC       FRCC    winter      1    2     2  15694    2  2011   \n",
      "73     FRCC       FRCC  shoulder      3   74     1  15738   15  2011   \n",
      "1      FRCC       FRCC    winter      1    2     1  15750    2  2011   \n",
      "443    FRCC       FRCC  shoulder      3   79     2  15822   20  2011   \n",
      "72     FRCC       FRCC  shoulder      3   73     1  15831   14  2011   \n",
      "\n",
      "          Date  DOW  Weekday  Season_Group  Hour_Tot  Load_Tot      Load_Avg  \n",
      "366 2011-01-02    6    False             1        27    512224  18971.259259  \n",
      "73  2011-03-15    1     True             2        87   1541095  17713.735632  \n",
      "1   2011-01-02    6    False             1        27    507490  18795.925926  \n",
      "443 2011-03-20    6    False             2        35    614008  17543.085714  \n",
      "72  2011-03-14    0     True             2        87   1541095  17713.735632  \n",
      "number of rows in dataset = 8760\n"
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
     ]
    }
   ],
   "source": [
    "case4_x = x2.copy()\n",
    "#case4_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "#case4_seasons = case4_seasons.drop(['bimonthly'], axis=1)\n",
    "#case4_seasons = case4_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "#case4_x = pd.merge(case4_x, case4_seasons, on='Month', how='left')\n",
    "#print(case4_x)\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case4 = case4_x.groupby(['Region','Season','Weekday','Hour'],as_index=False).agg(aggregations)\n",
    "case4.columns = case4.columns.droplevel(0)\n",
<<<<<<< HEAD
    "case4.columns = ['Region','Season','Weekday','Hour','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case4.tail())\n",
    "print('number of segments in dataset =',case4.shape[0]/reg_count)\n",
    "case4.to_csv('../outputs/'+x_name+'_segments_2daytype_season_24hr.csv')\n",
||||||| merged common ancestors
    "case4.columns = ['Region','Season_Group','Weekday','Hour','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case4.head())\n",
    "print('number of rows in dataset =',case4.shape[0])\n",
    "case4.to_csv('../outputs/'+x_name+'_segments_2daytype_season_wkd.csv')\n",
=======
    "case4.columns = ['Region','Season_Group','Weekday','Hour','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case4.tail())\n",
    "print('number of rows in dataset =',case4.shape[0])\n",
    "case4.to_csv('../outputs/'+x_name+'_segments_2daytype_season_24hr.csv')\n",
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
    "print()\n",
    "\n",
    "case4_x2 = pd.merge(case4_x,case4,on=['Region','Season','Weekday','Hour'],how='left')\n",
    "case4_x2 = case4_x2.sort_values(['Region',x_column]).drop(case4_x2.columns[0],axis=1)\n",
    "print(case4_x2.head())\n",
    "print('number of rows in dataset =',case4_x2.shape[0])\n",
    "case4_x2.to_csv('../outputs/'+x_name+'_8760_2daytype_season_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 hour interval day types"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
||||||| merged common ancestors
   "execution_count": 7,
=======
   "execution_count": 3,
>>>>>>> master
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "         Region R_Group R_Subgroup  Season  Month  DOY  Hour  Load  Day  Year  \\\n",
      "17515  ERC_WEST     ERC       WEST  winter     12  361    24  2674   27  2016   \n",
      "17516  ERC_WEST     ERC       WEST  winter     12  362    24  2662   28  2016   \n",
      "17517  ERC_WEST     ERC       WEST  winter     12  363    24  3059   29  2016   \n",
      "17518  ERC_WEST     ERC       WEST  winter     12  364    24  2893   30  2016   \n",
      "17519  ERC_WEST     ERC       WEST  winter     12  365    24  2594   31  2016   \n",
      "\n",
      "            Date  DOW  Weekday  4-hr  \n",
      "17515 2016-12-27    1     True     6  \n",
      "17516 2016-12-28    2     True     6  \n",
      "17517 2016-12-29    3     True     6  \n",
      "17518 2016-12-30    4     True     6  \n",
      "17519 2016-12-31    5    False     6  \n"
||||||| merged common ancestors
      "      Hour_Counter  4-hr\n",
      "0                1     1\n",
      "1                2     1\n",
      "2                3     1\n",
      "3                4     1\n",
      "4                5     2\n",
      "...            ...   ...\n",
      "8755          8756  2189\n",
      "8756          8757  2190\n",
      "8757          8758  2190\n",
      "8758          8759  2190\n",
      "8759          8760  2190\n",
      "\n",
      "[8760 rows x 2 columns]\n",
      "          Region R_Group R_Subgroup  Season  Month  Day  Hour   TRG4  \\\n",
      "0       ERC_PHDL     ERC       PHDL  winter      1    1     1  653.0   \n",
      "1       ERC_PHDL     ERC       PHDL  winter      1    1     2  687.0   \n",
      "2       ERC_PHDL     ERC       PHDL  winter      1    1     3  605.0   \n",
      "3       ERC_PHDL     ERC       PHDL  winter      1    1     4  579.0   \n",
      "4       ERC_PHDL     ERC       PHDL  winter      1    1     5  538.0   \n",
      "...          ...     ...        ...     ...    ...  ...   ...    ...   \n",
      "867235  WEC_SDGE     WEC       SDGE  winter     12  365    20   71.0   \n",
      "867236  WEC_SDGE     WEC       SDGE  winter     12  365    21   69.0   \n",
      "867237  WEC_SDGE     WEC       SDGE  winter     12  365    22  103.0   \n",
      "867238  WEC_SDGE     WEC       SDGE  winter     12  365    23   85.0   \n",
      "867239  WEC_SDGE     WEC       SDGE  winter     12  365    24  117.0   \n",
      "\n",
      "             Date  DOW  Weekday  Hour_Counter  4-hr  \n",
      "0      2016-01-01    4     True             1     1  \n",
      "1      2016-01-01    4     True             2     1  \n",
      "2      2016-01-01    4     True             3     1  \n",
      "3      2016-01-01    4     True             4     1  \n",
      "4      2016-01-01    4     True             5     2  \n",
      "...           ...  ...      ...           ...   ...  \n",
      "867235 2011-12-31    5    False          8756  2189  \n",
      "867236 2011-12-31    5    False          8757  2190  \n",
      "867237 2011-12-31    5    False          8758  2190  \n",
      "867238 2011-12-31    5    False          8759  2190  \n",
      "867239 2011-12-31    5    False          8760  2190  \n",
      "\n",
      "[867240 rows x 13 columns]\n"
=======
      "          Region R_Group R_Subgroup  Season  Month  Day  Hour   Load  \\\n",
      "0       ERC_REST     ERC       REST  winter      1    1     1  34807   \n",
      "1       ERC_REST     ERC       REST  winter      1    2     1  34716   \n",
      "2       ERC_REST     ERC       REST  winter      1    3     1  34736   \n",
      "3       ERC_REST     ERC       REST  winter      1    4     1  35914   \n",
      "4       ERC_REST     ERC       REST  winter      1    5     1  33845   \n",
      "...          ...     ...        ...     ...    ...  ...   ...    ...   \n",
      "551875   WECC_WY     WEC         WY  winter     12  361    24   1767   \n",
      "551876   WECC_WY     WEC         WY  winter     12  362    24   1744   \n",
      "551877   WECC_WY     WEC         WY  winter     12  363    24   1805   \n",
      "551878   WECC_WY     WEC         WY  winter     12  364    24   1791   \n",
      "551879   WECC_WY     WEC         WY  winter     12  365    24   1834   \n",
      "\n",
      "             Date  DOW  Weekday  4-hr  \n",
      "0      2016-01-01    4     True     1  \n",
      "1      2016-01-01    4     True     1  \n",
      "2      2016-01-01    4     True     1  \n",
      "3      2016-01-01    4     True     1  \n",
      "4      2016-01-01    4     True     1  \n",
      "...           ...  ...      ...   ...  \n",
      "551875 2011-12-31    5    False     6  \n",
      "551876 2011-12-31    5    False     6  \n",
      "551877 2011-12-31    5    False     6  \n",
      "551878 2011-12-31    5    False     6  \n",
      "551879 2011-12-31    5    False     6  \n",
      "\n",
      "[551880 rows x 12 columns]\n"
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
     ]
    }
   ],
   "source": [
    "#read in 4 hour interval counter\n",
<<<<<<< HEAD
    "interval_4hr = pd.read_csv('inputs/interval_4hr.csv')\n",
    "#print(interval_4hr)\n",
||||||| merged common ancestors
    "interval_4hr = pd.read_csv('inputs/sequential_hours.csv')\n",
    "interval_4hr = interval_4hr.drop(columns=['2-hr','8-hr','12-hr','24-hr','48-hr','120-hr'])\n",
    "print(interval_4hr)\n",
    "\n",
    "#add an hour counter\n",
    "x2['Hour_Counter'] = (x2['Hour']) + (x2['Day'] - 1) * 24\n",
    "x2 = x2.sort_values(by=['Region','Hour_Counter'])\n",
    "unique_hc = pd.Series(x2['Hour_Counter'].unique()).dropna()\n",
    "#print(unique_hc.tail(2))\n",
=======
    "interval_4hr = pd.read_csv('inputs/daytype_4hr.csv')\n",
    "#print(interval_4hr)\n",
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
    "\n",
<<<<<<< HEAD
    "x3 = pd.merge(x2,interval_4hr,on='Hour',how='left')\n",
    "print(x3.tail())"
||||||| merged common ancestors
    "x3 = pd.merge(x2,interval_4hr,on='Hour_Counter',how='left')\n",
    "print(x3)"
=======
    "x3 = pd.merge(x2,interval_4hr,on='Hour',how='left').rename(columns={'4-hr_Day':'4-hr'})\n",
    "print(x3)"
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 5: Monthly, single day type, 4 hour intervals (72 segments)\n",
    "#### Methodology: use groupby by month, 4 hour intervals"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
||||||| merged common ancestors
   "execution_count": 8,
=======
   "execution_count": 5,
>>>>>>> master
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "     Region  Month  4-hr  Hour_Tot  Load_Tot      Load_Avg\n",
      "0  ERC_REST      1     1       124   4318338  34825.306452\n",
      "1  ERC_REST      1     2       124   5025668  40529.580645\n",
      "2  ERC_REST      1     3       124   4711362  37994.854839\n",
      "3  ERC_REST      1     4       124   4504207  36324.250000\n",
      "4  ERC_REST      1     5       124   4847894  39095.919355\n",
      "number of segments in dataset = 72.0\n",
      "\n",
      "     R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "1545     ERC       REST  shoulder      3   86     3  26989   27  2016   \n",
      "823      ERC       REST  shoulder      4   94     2  27006    4  2016   \n",
      "1531     ERC       REST  shoulder      3   72     3  27025   13  2016   \n",
      "1524     ERC       REST  shoulder      3   65     3  27059    6  2016   \n",
      "794      ERC       REST  shoulder      3   65     2  27069    6  2016   \n",
      "\n",
      "           Date  DOW  Weekday  4-hr  Hour_Tot  Load_Tot      Load_Avg  \n",
      "1545 2016-03-27    6    False     1       124   3572927  28813.927419  \n",
      "823  2016-04-04    0     True     1       120   3539489  29495.741667  \n",
      "1531 2016-03-13    6    False     1       124   3572927  28813.927419  \n",
      "1524 2016-03-06    6    False     1       124   3572927  28813.927419  \n",
      "794  2016-03-06    6    False     1       124   3572927  28813.927419  \n",
      "number of rows in dataset = 17520\n"
||||||| merged common ancestors
      "     Region  Month  4-hr  Hour_Tot  Wind_Gen_Tot  Wind_Gen_Avg\n",
      "0  ERC_PHDL      1     1         4        2524.0        631.00\n",
      "1  ERC_PHDL      1     2         4        1702.0        425.50\n",
      "2  ERC_PHDL      1     3         4        1133.0        283.25\n",
      "3  ERC_PHDL      1     4         4         936.0        234.00\n",
      "4  ERC_PHDL      1     5         4        1575.0        393.75\n",
      "number of rows in dataset = 99658\n",
      "\n",
      "     R_Group R_Subgroup  Season  Month  Day  Hour  TRG4       Date  DOW  \\\n",
      "5913     ERC       PHDL  summer      9  247    10  33.0 2016-09-03    5   \n",
      "5914     ERC       PHDL  summer      9  247    11  49.0 2016-09-03    5   \n",
      "4883     ERC       PHDL  summer      7  204    12  51.0 2016-07-22    4   \n",
      "4880     ERC       PHDL  summer      7  204     9  53.0 2016-07-22    4   \n",
      "5028     ERC       PHDL  summer      7  210    13  55.0 2016-07-28    3   \n",
      "\n",
      "      Weekday  Hour_Counter  4-hr  Hour_Tot  Wind_Gen_Tot  Wind_Gen_Avg  \n",
      "5913    False          5914  1479         4         228.0         57.00  \n",
      "5914    False          5915  1479         4         228.0         57.00  \n",
      "4883     True          4884  1221         4         219.0         54.75  \n",
      "4880     True          4881  1221         4         219.0         54.75  \n",
      "5028     True          5029  1258         4         446.0        111.50  \n",
      "number of rows in dataset = 867240\n"
=======
      "     Region  Month  Day  4-hr  Hour_Tot     Tot       Avg\n",
      "0  ERC_REST      1    1     1         4  139677  34919.25\n",
      "1  ERC_REST      1    1     2         4  153285  38321.25\n",
      "2  ERC_REST      1    1     3         4  160446  40111.50\n",
      "3  ERC_REST      1    1     4         4  155314  38828.50\n",
      "4  ERC_REST      1    1     5         4  162472  40618.00\n",
      "number of rows in dataset = 137970\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1524  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "794   ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW  Weekday  4-hr  Hour_Tot     Tot       Avg  \n",
      "1545 2016-03-05    5    False     1         4  108862  27215.50  \n",
      "823  2016-02-04    3     True     1         4  111068  27767.00  \n",
      "1531 2016-03-04    4     True     1         4  109231  27307.75  \n",
      "1524 2016-03-04    4     True     1         4  108948  27237.00  \n",
      "794  2016-02-03    2     True     1         4  108948  27237.00  \n",
      "number of rows in dataset = 551880\n"

>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
     ]
    }
   ],
   "source": [
    "case5_x = x3.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case5 = case5_x.groupby(['Region','Month','Day','4-hr'],as_index=False).agg(aggregations)\n",
    "case5.columns = case5.columns.droplevel(0)\n",
    "case5.columns = ['Region','Month','Day','4-hr','Hour_Tot','Tot','Avg']\n",
    "print(case5.head())\n",
    "print('number of segments in dataset =',case5.shape[0]/reg_count)\n",
    "case5.to_csv('../outputs/'+x_name+'_segments_2daytype_month_4hr.csv')\n",
    "print()\n",
    "\n",
    "case5_x2 = pd.merge(case5_x,case5,on=['Region','Month','Day','4-hr'],how='left')\n",
    "case5_x2 = case5_x2.sort_values(['Region',x_column])\n",
    "print(case5_x2.head())\n",
    "print('number of rows in dataset =',case5_x2.shape[0])\n",
    "case5_x2.to_csv('../outputs/'+x_name+'_8760_2daytype_month_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 6: Bi-monthly weekend/weekday day-types, 4-hour intervals\n",
    "#### Methodology: use groupby function and bimonthly groups"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
||||||| merged common ancestors
   "execution_count": 9,
=======
   "execution_count": 6,
>>>>>>> master
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "     Region  Bimonth  Weekday  4-hr  Hour_Tot  Load_Tot      Load_Avg\n",
      "0  ERC_REST        1    False     1        72   2362801  32816.680556\n",
      "1  ERC_REST        1    False     2        72   2705068  37570.388889\n",
      "2  ERC_REST        1    False     3        72   2622321  36421.125000\n",
      "3  ERC_REST        1    False     4        72   2539521  35271.125000\n",
      "4  ERC_REST        1    False     5        72   2729661  37911.958333\n",
      "number of segments in dataset = 72.0\n",
      "\n",
      "     R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "1545     ERC       REST  shoulder      3   86     3  26989   27  2016   \n",
      "823      ERC       REST  shoulder      4   94     2  27006    4  2016   \n",
      "1531     ERC       REST  shoulder      3   72     3  27025   13  2016   \n",
      "1524     ERC       REST  shoulder      3   65     3  27059    6  2016   \n",
      "794      ERC       REST  shoulder      3   65     2  27069    6  2016   \n",
      "\n",
      "           Date  DOW  Weekday  4-hr  Bimonth  Hour_Tot  Load_Tot      Load_Avg  \n",
      "1545 2016-03-27    6    False     1        2        68   1932774  28423.147059  \n",
      "823  2016-04-04    0     True     1        2       176   5179642  29429.784091  \n",
      "1531 2016-03-13    6    False     1        2        68   1932774  28423.147059  \n",
      "1524 2016-03-06    6    False     1        2        68   1932774  28423.147059  \n",
      "794  2016-03-06    6    False     1        2        68   1932774  28423.147059  \n",
      "number of rows in dataset = 17520\n"
||||||| merged common ancestors
      "     Region  Bimonth  4-hr  Hour_Tot  Wind_Gen_Tot  Wind_Gen_Avg\n",
      "0  ERC_PHDL        1     1         4        2524.0        631.00\n",
      "1  ERC_PHDL        1     2         4        1702.0        425.50\n",
      "2  ERC_PHDL        1     3         4        1133.0        283.25\n",
      "3  ERC_PHDL        1     4         4         936.0        234.00\n",
      "4  ERC_PHDL        1     5         4        1575.0        393.75\n",
      "number of rows in dataset = 94143\n",
      "\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 52.9 MiB for an array with shape (8, 867240) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-6d3717775c37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mcase6_x2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcase6_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcase6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Region'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Bimonth'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'4-hr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mcase6_x2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcase6_x2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Region'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_column\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcase6_x2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcase6_x2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'number of rows in dataset ='\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcase6_x2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36msort_values\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index)\u001b[0m\n\u001b[0;32m   4935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4936\u001b[0m         new_data = self._data.take(\n\u001b[1;32m-> 4937\u001b[1;33m             \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4938\u001b[0m         )\n\u001b[0;32m   4939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[0;32m   1375\u001b[0m         \u001b[0mTake\u001b[0m \u001b[0mitems\u001b[0m \u001b[0malong\u001b[0m \u001b[0many\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1376\u001b[0m         \"\"\"\n\u001b[1;32m-> 1377\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1378\u001b[0m         indexer = (\n\u001b[0;32m   1379\u001b[0m             \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"int64\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    943\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 945\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_known_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   1885\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_blocks\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1886\u001b[0m         merged_blocks = _merge_blocks(\n\u001b[1;32m-> 1887\u001b[1;33m             \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_can_consolidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1888\u001b[0m         )\n\u001b[0;32m   1889\u001b[0m         \u001b[0mnew_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_blocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, _can_consolidate)\u001b[0m\n\u001b[0;32m   3094\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3095\u001b[0m         \u001b[0margsort\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3096\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3097\u001b[0m         \u001b[0mnew_mgr_locs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 52.9 MiB for an array with shape (8, 867240) and data type int64"
=======
      "     Region  Bimonth  Day  4-hr  Hour_Tot     Tot       Avg\n",
      "0  ERC_REST        1    1     1         4  139677  34919.25\n",
      "1  ERC_REST        1    1     2         4  153285  38321.25\n",
      "2  ERC_REST        1    1     3         4  160446  40111.50\n",
      "3  ERC_REST        1    1     4         4  155314  38828.50\n",
      "4  ERC_REST        1    1     5         4  162472  40618.00\n",
      "number of rows in dataset = 137970\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1524  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "794   ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW  Weekday  4-hr  Bimonth  Hour_Tot     Tot       Avg  \n",
      "1545 2016-03-05    5    False     1        2         4  108862  27215.50  \n",
      "823  2016-02-04    3     True     1        2         4  111068  27767.00  \n",
      "1531 2016-03-04    4     True     1        2         4  109231  27307.75  \n",
      "1524 2016-03-04    4     True     1        2         4  108948  27237.00  \n",
      "794  2016-02-03    2     True     1        2         4  108948  27237.00  \n",
      "number of rows in dataset = 551880\n"
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
     ]
    }
   ],
   "source": [
    "case6_x = x3.copy()\n",
    "case6_bimonth = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case6_bimonth = case6_bimonth.drop(['seasonal'], axis=1)\n",
    "case6_bimonth = case6_bimonth.rename(columns={'bimonthly':'Bimonth','month':'Month'})\n",
    "\n",
    "case6_x = pd.merge(case6_x, case6_bimonth, on='Month', how='left')\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
<<<<<<< HEAD
    "case6 = case6_x.groupby(['Region','Bimonth','Weekday','4-hr'],as_index=False).agg(aggregations)\n",
||||||| merged common ancestors
    "case6 = case6_x.groupby(['Region','Bimonth','4-hr'],as_index=False).agg(aggregations)\n",
=======
    "case6 = case6_x.groupby(['Region','Bimonth','Day','4-hr'],as_index=False).agg(aggregations)\n",
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
    "case6.columns = case6.columns.droplevel(0)\n",
<<<<<<< HEAD
    "case6.columns = ['Region','Bimonth','Weekday','4-hr','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
||||||| merged common ancestors
    "case6.columns = ['Region','Bimonth','4-hr','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
=======
    "case6.columns = ['Region','Bimonth','Day','4-hr','Hour_Tot','Tot','Avg']\n",
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
    "print(case6.head())\n",
    "print('number of segments in dataset =',case6.shape[0]/reg_count)\n",
    "case6.to_csv('../outputs/'+x_name+'_segments_2daytype_bimonth_4hr.csv')\n",
    "print()\n",
    "\n",
<<<<<<< HEAD
    "case6_x2 = pd.merge(case6_x,case6,on=['Region','Bimonth','Weekday','4-hr'],how='left')\n",
    "case6_x2 = case6_x2.sort_values(['Region',x_column]).drop(case6_x2.columns[0],axis=1)\n",
||||||| merged common ancestors
    "case6_x2 = pd.merge(case6_x,case6,on=['Region','Bimonth','4-hr'],how='left')\n",
    "case6_x2 = case6_x2.sort_values(['Region',x_column]).drop(case6_x2.columns[0],axis=1)\n",
=======
    "case6_x2 = pd.merge(case6_x,case6,on=['Region','Bimonth','Day','4-hr'],how='left')\n",
    "case6_x2 = case6_x2.sort_values(['Region',x_column])\n",
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
    "print(case6_x2.head())\n",
    "print('number of rows in dataset =',case6_x2.shape[0])\n",
    "case6_x2.to_csv('../outputs/'+x_name+'_8760_2daytype_bimonth_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 7: Season-based months, weekend/weekday day-types, 4-hour intervals\n",
    "#### Methodology: groupby function and applied season groups"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
||||||| merged common ancestors
   "execution_count": 10,
=======
<<<<<<< HEAD
   "execution_count": 10,
||||||| merged common ancestors
   "execution_count": null,
=======
   "execution_count": 7,
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
>>>>>>> master
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Season_Group  Weekday  4-hr  Hour_Tot  Load_Tot      Load_Avg\n",
      "0  ERC_REST             1    False     1       108   3533655  32719.027778\n",
      "1  ERC_REST             1    False     2       108   3948984  36564.666667\n",
      "2  ERC_REST             1    False     3       108   3994962  36990.388889\n",
      "3  ERC_REST             1    False     4       108   3897223  36085.398148\n",
      "4  ERC_REST             1    False     5       108   4166114  38575.129630\n",
      "number of segments in dataset = 60.0\n",
      "\n",
      "     R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "1545     ERC       REST  shoulder      3   86     3  26989   27  2016   \n",
      "823      ERC       REST  shoulder      4   94     2  27006    4  2016   \n",
      "1531     ERC       REST  shoulder      3   72     3  27025   13  2016   \n",
      "1524     ERC       REST  shoulder      3   65     3  27059    6  2016   \n",
      "794      ERC       REST  shoulder      3   65     2  27069    6  2016   \n",
      "\n",
      "           Date  DOW  Weekday  4-hr  Season_Group  Hour_Tot  Load_Tot  \\\n",
      "1545 2016-03-27    6    False     1             2       140   4051780   \n",
      "823  2016-04-04    0     True     1             2       348  10506196   \n",
      "1531 2016-03-13    6    False     1             2       140   4051780   \n",
      "1524 2016-03-06    6    False     1             2       140   4051780   \n",
      "794  2016-03-06    6    False     1             2       140   4051780   \n",
      "\n",
      "          Load_Avg  \n",
      "1545  28941.285714  \n",
      "823   30190.218391  \n",
      "1531  28941.285714  \n",
      "1524  28941.285714  \n",
      "794   28941.285714  \n",
      "number of rows in dataset = 17520\n"
     ]
    }
   ],
||||||| merged common ancestors
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region    Season  Weekday  4-hr  Hour_Tot      Tot           Avg\n",
      "0  ERC_REST  shoulder    False     1       115  3399710  29562.695652\n",
      "1  ERC_REST  shoulder    False     2       144  4872764  33838.638889\n",
      "2  ERC_REST  shoulder    False     3       155  5809365  37479.774194\n",
      "3  ERC_REST  shoulder    False     4       115  4632532  40282.886957\n",
      "4  ERC_REST  shoulder    False     5       144  5663089  39327.006944\n",
      "number of rows in dataset = 2268\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1524  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "794   ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW  Weekday  4-hr  Season_Group  Hour_Tot       Tot  \\\n",
      "1545 2016-03-05    5    False     1             2       115   3399710   \n",
      "823  2016-02-04    3     True     1             2       373  11158266   \n",
      "1531 2016-03-04    4     True     1             2       373  11158266   \n",
      "1524 2016-03-04    4     True     1             2       373  11158266   \n",
      "794  2016-02-03    2     True     1             2       373  11158266   \n",
      "\n",
      "               Avg  \n",
      "1545  29562.695652  \n",
      "823   29914.922252  \n",
      "1531  29914.922252  \n",
      "1524  29914.922252  \n",
      "794   29914.922252  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
   "source": [
    "case7_x = x3.copy()\n",
    "case7_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case7_seasons = case7_seasons.drop(['bimonthly'], axis=1)\n",
    "case7_seasons = case7_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "\n",
    "case7_x = pd.merge(case7_x, case7_seasons, on='Month', how='left')\n",
    "#print(case4_load)\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case7 = case7_x.groupby(['Region','Season_Group','Weekday','4-hr'],as_index=False).agg(aggregations)\n",
    "case7.columns = case7.columns.droplevel(0)\n",
    "case7.columns = ['Region','Season_Group','Weekday','4-hr','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case7.head())\n",
    "print('number of segments in dataset =',case7.shape[0]/reg_count)\n",
    "case7.to_csv('../outputs/'+x_name+'_segments_2daytype_seasonbased_4hr.csv')\n",
    "print()\n",
    "\n",
    "case7_x2 = pd.merge(case7_x,case7,on=['Region','Season_Group','Weekday','4-hr'],how='left')\n",
    "case7_x2 = case7_x2.sort_values(['Region',x_column]).drop(case7_x2.columns[0],axis=1)\n",
    "print(case7_x2.head())\n",
    "print('number of rows in dataset =',case7_x2.shape[0])\n",
    "case7_x2.to_csv('../outputs/'+x_name+'_8760_2daytype_seasonbased_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 8: Seasons, weekend/weekday day-types, 4-hour intervals\n",
    "#### Methodology: groupby function and applied season groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region    Season  Weekday  4-hr  Hour_Tot  Load_Tot      Load_Avg\n",
      "0  ERC_REST  shoulder    False     1       140   4051780  28941.285714\n",
      "1  ERC_REST  shoulder    False     2       140   4378835  31277.392857\n",
      "2  ERC_REST  shoulder    False     3       140   4993309  35666.492857\n",
      "3  ERC_REST  shoulder    False     4       140   5296142  37829.585714\n",
      "4  ERC_REST  shoulder    False     5       140   5276131  37686.650000\n",
      "number of segments in dataset = 36.0\n",
      "\n",
      "     R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "1545     ERC       REST  shoulder      3   86     3  26989   27  2016   \n",
      "823      ERC       REST  shoulder      4   94     2  27006    4  2016   \n",
      "1531     ERC       REST  shoulder      3   72     3  27025   13  2016   \n",
      "1524     ERC       REST  shoulder      3   65     3  27059    6  2016   \n",
      "794      ERC       REST  shoulder      3   65     2  27069    6  2016   \n",
      "\n",
      "           Date  DOW  Weekday  4-hr  Hour_Tot  Load_Tot      Load_Avg  \n",
      "1545 2016-03-27    6    False     1       140   4051780  28941.285714  \n",
      "823  2016-04-04    0     True     1       348  10506196  30190.218391  \n",
      "1531 2016-03-13    6    False     1       140   4051780  28941.285714  \n",
      "1524 2016-03-06    6    False     1       140   4051780  28941.285714  \n",
      "794  2016-03-06    6    False     1       140   4051780  28941.285714  \n",
      "number of rows in dataset = 17520\n"
     ]
    }
   ],
   "source": [
    "case8_x = x3.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case8 = case8_x.groupby(['Region','Season','Weekday','4-hr'],as_index=False).agg(aggregations)\n",
    "case8.columns = case8.columns.droplevel(0)\n",
    "case8.columns = ['Region','Season','Weekday','4-hr','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case8.head())\n",
    "print('number of segments in dataset =',case8.shape[0]/reg_count)\n",
    "case8.to_csv('../outputs/'+x_name+'_segments_2daytype_seasonbased_4hr.csv')\n",
    "print()\n",
    "\n",
    "case8_x2 = pd.merge(case8_x,case8,on=['Region','Season','Weekday','4-hr'],how='left')\n",
    "case8_x2 = case8_x2.sort_values(['Region',x_column]).drop(case8_x2.columns[0],axis=1)\n",
    "print(case8_x2.head())\n",
    "print('number of rows in dataset =',case8_x2.shape[0])\n",
    "case8_x2.to_csv('../outputs/'+x_name+'_8760_2daytype_seasonbased_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day type: Three (Weekday, Weekend, Peak Load)\n",
    "## Find Peak Load Days in Each Month"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
||||||| merged common ancestors
   "execution_count": 12,
=======
<<<<<<< HEAD
   "execution_count": 12,
||||||| merged common ancestors
   "execution_count": null,
=======
   "execution_count": 209,
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
>>>>>>> master
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows in regional data = 365.0\n",
      "      Region  Month  Load_MD_Max\n",
      "0   ERC_REST      1      1000605\n",
      "1   ERC_REST      2       932895\n",
      "2   ERC_REST      3       886217\n",
      "3   ERC_REST      4       983152\n",
      "4   ERC_REST      5      1083459\n",
      "5   ERC_REST      6      1211630\n",
      "6   ERC_REST      7      1267082\n",
      "7   ERC_REST      8      1332321\n",
      "8   ERC_REST      9      1231350\n",
      "9   ERC_REST     10      1109240\n",
      "10  ERC_REST     11      1004548\n",
      "11  ERC_REST     12      1176963\n",
      "\n",
      "     Region R_Group R_Subgroup  Season  Month  DOY  Hour  Load_MD_Tot  \\\n",
      "0  ERC_REST     ERC       REST  winter      1    1     1       917588   \n",
      "1  ERC_REST     ERC       REST  winter      1    2     1       863132   \n",
      "2  ERC_REST     ERC       REST  winter      1    3     1       934513   \n",
      "3  ERC_REST     ERC       REST  winter      1    4     1       960576   \n",
      "4  ERC_REST     ERC       REST  winter      1    5     1       916216   \n",
      "\n",
      "   Load_MD_Max  \n",
      "0      1000605  \n",
      "1      1000605  \n",
      "2      1000605  \n",
      "3      1000605  \n",
      "4      1000605  \n"
     ]
    }
   ],
||||||| merged common ancestors
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Region  Month  Load_Max\n",
      "0    FRCC      1    839413\n",
      "1    FRCC      2    632009\n",
      "2    FRCC      3    629063\n",
      "3    FRCC      4    774706\n",
      "4    FRCC      5    761238\n",
      "5    FRCC      6    855573\n",
      "6    FRCC      7    848154\n",
      "7    FRCC      8    864191\n",
      "8    FRCC      9    798533\n",
      "9    FRCC     10    707155\n",
      "10   FRCC     11    657292\n",
      "11   FRCC     12    584382\n",
      "\n",
      "     Region R_Group R_Subgroup  Season  Month  Day  Hour   Load  Load_Tot  \\\n",
      "8755   FRCC    FRCC       FRCC  winter     12  361    24  17370    556346   \n",
      "8756   FRCC    FRCC       FRCC  winter     12  362    24  19366    546778   \n",
      "8757   FRCC    FRCC       FRCC  winter     12  363    24  19024    564643   \n",
      "8758   FRCC    FRCC       FRCC  winter     12  364    24  16730    542878   \n",
      "8759   FRCC    FRCC       FRCC  winter     12  365    24  17794    528679   \n",
      "\n",
      "      Load_Max  \n",
      "8755    584382  \n",
      "8756    584382  \n",
      "8757    584382  \n",
      "8758    584382  \n",
      "8759    584382  \n"
     ]
    }
   ],
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
   "source": [
    "#create temporary DF to find peak\n",
<<<<<<< HEAD
    "load = load_dur.copy()\n",
||||||| merged common ancestors
    "test = x.copy()\n",
=======
    "test = load_dur.copy()\n",
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
    "\n",
    "#groupby region, month, and day to sum the total day\n",
<<<<<<< HEAD
    "aggregations1 = {'Load':sum}\n",
    "md_sum = load.groupby(['Region','Month','Day'],as_index=False).agg(aggregations1)\n",
    "md_sum.columns = ['Region','Month','Day','Load_MD_Tot']\n",
    "#print(md_sum.tail())\n",
    "print('number of rows in regional data =',md_sum.shape[0]/reg_count)\n",
||||||| merged common ancestors
    "aggregations1 = {x_column:sum}\n",
    "test_sum = test.groupby(['Region','Month','Day'],as_index=False).agg(aggregations1)\n",
    "#test1.columns = test1.columns.droplevel(0)\n",
    "test_sum.columns = ['Region','Month','Day',x_name2+'_Tot']\n",
    "#print(test_sum.head())\n",
    "#print('number of rows in dataset =',test_sum.shape[0])\n",
=======
    "aggregations1 = {x_column:sum}\n",
    "md_sum = test.groupby(['Region','Month','Day'],as_index=False).agg(aggregations1)\n",
    "#test1.columns = test1.columns.droplevel(0)\n",
    "md_sum.columns = ['Region','Month','Day',x_name2+'_Tot']\n",
    "#print(test_sum.head())\n",
    "#print('number of rows in dataset =',test_sum.shape[0])\n",
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
    "\n",
<<<<<<< HEAD
    "md_sum2 = pd.merge(load,md_sum,on=['Region','Month','Day'],how='left')\n",
    "#print(md_sum2.tail())\n",
||||||| merged common ancestors
    "test3 = pd.merge(test,test_sum,on=['Region','Month','Day'],how='left')\n",
    "#print(test3)\n",
=======
    "md_sum2 = pd.merge(test,md_sum,on=['Region','Month','Day'],how='left')\n",
    "#print(test3)\n",
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
    "\n",
    "#groupby region and month to find maximum \n",
<<<<<<< HEAD
    "aggregations2 = {'Load_MD_Tot':max}\n",
    "md_max = md_sum2.groupby(['Region','Month'],as_index=False).agg(aggregations2)\n",
    "md_max.columns = ['Region','Month','Load_MD_Max']\n",
    "print(md_max[0:12])\n",
    "print()\n",
||||||| merged common ancestors
    "aggregations2 = {x_name2+'_Tot':max}\n",
    "test_max = test_sum.groupby(['Region','Month'],as_index=False).agg(aggregations2)\n",
    "test_max.columns = ['Region','Month',x_name2+'_Max']\n",
    "#print(test_max.head())\n",
=======
    "aggregations2 = {x_name2+'_Tot':max}\n",
    "md_max = md_sum2.groupby(['Region','Month'],as_index=False).agg(aggregations2)\n",
    "md_max.columns = ['Region','Month',x_name2+'_Max']\n",
    "print(md_max)\n",
    "print()\n",
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
    "\n",
<<<<<<< HEAD
    "peakd = pd.merge(md_sum2,md_max,on=['Region','Month'],how='left')\n",
    "peakd = peakd.drop(columns=['Unnamed: 0','Load'])\n",
    "peakd = peakd.rename(columns={'Day':'DOY'})\n",
    "print(peakd.head())"
||||||| merged common ancestors
    "test4 = pd.merge(test3,test_max,on=['Region','Month'],how='left')\n",
    "print(test4)"
=======
    "peakd = pd.merge(test3,md_max,on=['Region','Month'],how='left')\n",
    "print(peakd.tail())"
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 21,
||||||| merged common ancestors
   "execution_count": 13,
=======
<<<<<<< HEAD
   "execution_count": 13,
||||||| merged common ancestors
   "execution_count": null,
=======
   "execution_count": 210,
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
>>>>>>> master
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Region R_Group R_Subgroup  Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "0   ERC_REST     ERC       REST  winter      1    1     1  34807    1  2016   \n",
      "1   ERC_REST     ERC       REST  winter      1    2     1  34716    2  2016   \n",
      "2   ERC_REST     ERC       REST  winter      1    3     1  34736    3  2016   \n",
      "3   ERC_REST     ERC       REST  winter      1    4     1  35914    4  2016   \n",
      "4   ERC_REST     ERC       REST  winter      1    5     1  33845    5  2016   \n",
      "5   ERC_REST     ERC       REST  winter      1    6     1  30995    6  2016   \n",
      "6   ERC_REST     ERC       REST  winter      1    7     1  30186    7  2016   \n",
      "7   ERC_REST     ERC       REST  winter      1    8     1  30745    8  2016   \n",
      "8   ERC_REST     ERC       REST  winter      1    9     1  38702    9  2016   \n",
      "9   ERC_REST     ERC       REST  winter      1   10     1  38567   10  2016   \n",
      "10  ERC_REST     ERC       REST  winter      1   11     1  37495   11  2016   \n",
      "11  ERC_REST     ERC       REST  winter      1   12     1  34274   12  2016   \n",
      "12  ERC_REST     ERC       REST  winter      1   13     1  31815   13  2016   \n",
      "13  ERC_REST     ERC       REST  winter      1   14     1  29614   14  2016   \n",
      "14  ERC_REST     ERC       REST  winter      1   15     1  31453   15  2016   \n",
      "\n",
      "         Date  DOW Day_Type  \n",
      "0  2016-01-01    4  Weekday  \n",
      "1  2016-01-02    5  Weekend  \n",
      "2  2016-01-03    6  Weekend  \n",
      "3  2016-01-04    0  Weekday  \n",
      "4  2016-01-05    1  Weekday  \n",
      "5  2016-01-06    2  Weekday  \n",
      "6  2016-01-07    3  Weekday  \n",
      "7  2016-01-08    4  Weekday  \n",
      "8  2016-01-09    5  Weekend  \n",
      "9  2016-01-10    6     Peak  \n",
      "10 2016-01-11    0  Weekday  \n",
      "11 2016-01-12    1  Weekday  \n",
      "12 2016-01-13    2  Weekday  \n",
      "13 2016-01-14    3  Weekday  \n",
      "14 2016-01-15    4  Weekday  \n"
     ]
    }
   ],
||||||| merged common ancestors
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region R_Group R_Subgroup  Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "0   FRCC    FRCC       FRCC  winter      1    1     1  17005    1  2011   \n",
      "1   FRCC    FRCC       FRCC  winter      1    2     1  15750    2  2011   \n",
      "2   FRCC    FRCC       FRCC  winter      1    3     1  16123    3  2011   \n",
      "3   FRCC    FRCC       FRCC  winter      1    4     1  18300    4  2011   \n",
      "4   FRCC    FRCC       FRCC  winter      1    5     1  17370    5  2011   \n",
      "\n",
      "        Date  DOW Day_Type  \n",
      "0 2011-01-01    5  Weekend  \n",
      "1 2011-01-02    6  Weekend  \n",
      "2 2011-01-03    0  Weekday  \n",
      "3 2011-01-04    1  Weekday  \n",
      "4 2011-01-05    2  Weekday  \n"
     ]
    }
   ],
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
   "source": [
<<<<<<< HEAD
    "x_peak = pd.merge(x2,peakd,on=['Region','R_Group','R_Subgroup','Season','Month','DOY','Hour'],how='left')\n",
||||||| merged common ancestors
    "x_peak = x2.copy()\n",
    "\n",
    "x_peak = pd.merge(x_peak,test4,on=['Region','R_Group','R_Subgroup','Season','Month','Day','Hour',x_column],how='left')\n",
=======
    "x_peak = x2.copy()\n",
    "\n",
    "x_peak = pd.merge(x_peak,peakd,on=['Region','R_Group','R_Subgroup','Season','Month','Day','Hour',x_column],how='left')\n",
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
    "x_peak = x_peak.rename(columns={'Weekday':'Day_Type'})\n",
    "#print(x_peak.tail())\n",
    "\n",
    "#Return True if the load total equals the day identified as the max\n",
    "x_peak.loc[x_peak['Day_Type'] == True, 'Day_Type'] = 'Weekday'\n",
    "x_peak.loc[x_peak['Day_Type'] == False, 'Day_Type'] = 'Weekend'\n",
<<<<<<< HEAD
    "x_peak.loc[x_peak['Load_MD_Tot'] == x_peak['Load_MD_Max'], 'Day_Type'] = 'Peak'\n",
    "x_peak = x_peak.drop(['Load_MD_Tot','Load_MD_Max'], axis=1)\n",
    "print(x_peak[0:15])"
||||||| merged common ancestors
    "x_peak.loc[x_peak[x_name2+'_Tot'] == x_peak[x_name2+'_Max'], 'Day_Type'] = 'Peak'\n",
    "x_peak = x_peak.drop([x_name2+'_Tot',x_name2+'_Max'], axis=1)\n",
    "print(x_peak)"
=======
    "x_peak.loc[x_peak[x_name2+'_Tot'] == x_peak[x_name2+'_Max'], 'Day_Type'] = 'Peak'\n",
    "x_peak = x_peak.drop([x_name2+'_Tot',x_name2+'_Max'], axis=1)\n",
    "print(x_peak.head())"
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Monthly, Weekend/weekday/peak day-types, 24 hours (864 segments)\n",
    "#### Methodology: similar to two day type, just adding in peak day types to sort by"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
||||||| merged common ancestors
   "execution_count": 14,
=======
<<<<<<< HEAD
   "execution_count": 14,
||||||| merged common ancestors
   "execution_count": null,
=======
   "execution_count": 150,
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
>>>>>>> master
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Month Day_Type  Hour  Hour_Tot  Load_Tot  Load_Avg\n",
      "0  ERC_REST      1     Peak     1         1     38567   38567.0\n",
      "1  ERC_REST      1     Peak     2         1     39107   39107.0\n",
      "2  ERC_REST      1     Peak     3         1     40311   40311.0\n",
      "3  ERC_REST      1     Peak     4         1     43115   43115.0\n",
      "4  ERC_REST      1     Peak     5         1     47186   47186.0\n",
      "number of segments in dataset = 864.0\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   27   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006    4   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   13   \n",
      "\n",
      "      Year       Date  DOW Day_Type  Hour_Tot  Load_Tot   Load_Avg  \n",
      "1545  2016 2016-03-27    6  Weekend         8    222011  27751.375  \n",
      "823   2016 2016-04-04    0  Weekday        20    579213  28960.650  \n",
      "1531  2016 2016-03-13    6  Weekend         8    222011  27751.375  \n",
      "number of rows in dataset = 17520\n"
     ]
    }
   ],
||||||| merged common ancestors
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region  Month Day_Type  Hour  Hour_Tot  Load_Tot  Load_Avg\n",
      "0   FRCC      1     Peak     1         1     32350   32350.0\n",
      "1   FRCC      1     Peak     2         1     33972   33972.0\n",
      "2   FRCC      1     Peak     3         1     37185   37185.0\n",
      "3   FRCC      1     Peak     4         1     41916   41916.0\n",
      "4   FRCC      1     Peak     5         1     44276   44276.0\n",
      "number of rows in dataset = 600\n",
      "\n",
      "     Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "300    FRCC    FRCC       FRCC    winter      1    2     2  15694    2  2011   \n",
      "4034   FRCC    FRCC       FRCC  shoulder      3   74     1  15738   15  2011   \n",
      "288    FRCC    FRCC       FRCC    winter      1    2     1  15750    2  2011   \n",
      "\n",
      "           Date  DOW Day_Type  Hour_Counter  Hour_Tot  Load_Tot      Load_Avg  \n",
      "300  2011-01-02    6  Weekend            26        10    212142  21214.200000  \n",
      "4034 2011-03-15    1  Weekday           337        23    382919  16648.652174  \n",
      "288  2011-01-02    6  Weekend            25        10    208342  20834.200000  \n",
      "number of rows in dataset = 8760\n"
     ]
    }
   ],
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
   "source": [
    "case1_x = x_peak.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case1 = case1_x.groupby(['Region','Month','Day_Type','Hour'],as_index=False).agg(aggregations)\n",
    "case1.columns = case1.columns.droplevel(0)\n",
    "case1.columns = ['Region','Month','Day_Type','Hour','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case1.head())\n",
    "print('number of segments in dataset =',case1.shape[0]/reg_count)\n",
    "case1.to_csv('../outputs/'+x_name+'_segments_3daytype_monthly_24hr.csv')\n",
    "print()\n",
    "\n",
    "case1_x2 = pd.merge(case1_x,case1,on=['Region','Month','Day_Type','Hour'],how='left')\n",
    "case1_x2 = case1_x2.sort_values(['Region',x_column])\n",
    "print(case1_x2.head(3))\n",
    "print('number of rows in dataset =',case1_x2.shape[0])\n",
    "case1_x2.to_csv('../outputs/'+x_name+'_8760_3daytype_monthly_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: Season, weekend/weekday/peak day-types, 24-hours (216 segments)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
||||||| merged common ancestors
   "execution_count": 15,
=======
<<<<<<< HEAD
   "execution_count": 15,
||||||| merged common ancestors
   "execution_count": null,
=======
   "execution_count": 151,
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
>>>>>>> master
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region    Season Day_Type  Hour  Hour_Tot  Load_Tot  Load_Avg\n",
      "0  ERC_REST  shoulder     Peak     1         4    132690  33172.50\n",
      "1  ERC_REST  shoulder     Peak     2         4    130802  32700.50\n",
      "2  ERC_REST  shoulder     Peak     3         4    131751  32937.75\n",
      "3  ERC_REST  shoulder     Peak     4         4    137910  34477.50\n",
      "4  ERC_REST  shoulder     Peak     5         4    150015  37503.75\n",
      "number of segments in dataset = 216.0\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   27   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006    4   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   13   \n",
      "\n",
      "      Year       Date  DOW Day_Type  Hour_Tot  Load_Tot   Load_Avg  \n",
      "1545  2016 2016-03-27    6  Weekend         8    222011  27751.375  \n",
      "823   2016 2016-04-04    0  Weekday        20    579213  28960.650  \n",
      "1531  2016 2016-03-13    6  Weekend         8    222011  27751.375  \n",
      "number of rows in dataset = 17520\n"
     ]
    }
   ],
||||||| merged common ancestors
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region    Season Day_Type  Hour  Hour_Tot  Load_Tot      Load_Avg\n",
      "0   FRCC  shoulder  Weekday     1        87   1541095  17713.735632\n",
      "1   FRCC  shoulder  Weekday     2        87   1553709  17858.724138\n",
      "2   FRCC  shoulder  Weekday     3        87   1661960  19102.988506\n",
      "3   FRCC  shoulder  Weekday     4        87   1886198  21680.436782\n",
      "4   FRCC  shoulder  Weekday     5        87   2013602  23144.850575\n",
      "number of rows in dataset = 168\n",
      "\n",
      "     Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "300    FRCC    FRCC       FRCC    winter      1    2     2  15694    2  2011   \n",
      "4034   FRCC    FRCC       FRCC  shoulder      3   74     1  15738   15  2011   \n",
      "288    FRCC    FRCC       FRCC    winter      1    2     1  15750    2  2011   \n",
      "\n",
      "           Date  DOW Day_Type  Hour_Counter  Hour_Tot  Load_Tot      Load_Avg  \n",
      "300  2011-01-02    6  Weekend            26        10    212142  21214.200000  \n",
      "4034 2011-03-15    1  Weekday           337        23    382919  16648.652174  \n",
      "288  2011-01-02    6  Weekend            25        10    208342  20834.200000  \n",
      "number of rows in dataset = 8760\n"
     ]
    }
   ],
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
   "source": [
    "case2_x = x_peak.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case2 = case2_x.groupby(['Region','Season','Day_Type','Hour'],as_index=False).agg(aggregations)\n",
    "case2.columns = case2.columns.droplevel(0)\n",
    "case2.columns = ['Region','Season','Day_Type','Hour','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case2.head())\n",
    "print('number of segments in dataset =',case2.shape[0]/reg_count)\n",
    "case2.to_csv('../outputs/'+x_name+'_segments_3daytype_season_24hr.csv')\n",
    "print()\n",
    "\n",
    "case2_x2 = pd.merge(case2_x,case2,on=['Region','Season','Day_Type','Hour'],how='left')\n",
    "case2_x2 = case2_x2.sort_values(['Region',x_column])\n",
    "print(case1_x2.head(3))\n",
    "print('number of rows in dataset =',case1_x2.shape[0])\n",
    "case2_x2.to_csv('../outputs/'+x_name+'_8760_3daytype_season_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3: Annual, weekend/weekday/peak day-types, 24-hours (72 segments)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 24,
||||||| merged common ancestors
   "execution_count": 16,
=======
<<<<<<< HEAD
   "execution_count": 16,
||||||| merged common ancestors
   "execution_count": null,
=======
   "execution_count": 152,
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
>>>>>>> master
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region Day_Type  Hour  Hour_Tot  Load_Tot      Load_Avg\n",
      "0  ERC_REST     Peak     1        12    455893  37991.083333\n",
      "1  ERC_REST     Peak     2        12    450804  37567.000000\n",
      "2  ERC_REST     Peak     3        12    454384  37865.333333\n",
      "3  ERC_REST     Peak     4        12    473371  39447.583333\n",
      "4  ERC_REST     Peak     5        12    505999  42166.583333\n",
      "number of segments in dataset = 72.0\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   27   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006    4   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   13   \n",
      "\n",
      "      Year       Date  DOW Day_Type  Hour_Tot  Load_Tot      Load_Avg  \n",
      "1545  2016 2016-03-27    6  Weekend       104   3359100  32299.038462  \n",
      "823   2016 2016-04-04    0  Weekday       249   8176806  32838.578313  \n",
      "1531  2016 2016-03-13    6  Weekend       104   3359100  32299.038462  \n",
      "number of rows in dataset = 17520\n"
     ]
    }
   ],
||||||| merged common ancestors
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region Day_Type  Hour  Hour_Tot  Load_Tot  Load_Avg\n",
      "0   FRCC     Peak     1         1     32350   32350.0\n",
      "1   FRCC     Peak     2         1     33972   33972.0\n",
      "2   FRCC     Peak     3         1     37185   37185.0\n",
      "3   FRCC     Peak     4         1     41916   41916.0\n",
      "4   FRCC     Peak     5         1     44276   44276.0\n",
      "number of rows in dataset = 72\n",
      "\n",
      "     Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "300    FRCC    FRCC       FRCC    winter      1    2     2  15694    2  2011   \n",
      "4034   FRCC    FRCC       FRCC  shoulder      3   74     1  15738   15  2011   \n",
      "288    FRCC    FRCC       FRCC    winter      1    2     1  15750    2  2011   \n",
      "\n",
      "           Date  DOW Day_Type  Hour_Counter  Hour_Tot  Load_Tot      Load_Avg  \n",
      "300  2011-01-02    6  Weekend            26       105   2051738  19540.361905  \n",
      "4034 2011-03-15    1  Weekday           337       259   5088736  19647.629344  \n",
      "288  2011-01-02    6  Weekend            25       105   2072468  19737.790476  \n",
      "number of rows in dataset = 8760\n"
     ]
    }
   ],
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
   "source": [
    "case3_x = x_peak.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case3 = case3_x.groupby(['Region','Day_Type','Hour'],as_index=False).agg(aggregations)\n",
    "case3.columns = case3.columns.droplevel(0)\n",
    "case3.columns = ['Region','Day_Type','Hour','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case3.head())\n",
    "print('number of segments in dataset =',case3.shape[0]/reg_count)\n",
    "case3.to_csv('../outputs/'+x_name+'_segments_3daytype_annual_24hr.csv')\n",
    "print()\n",
    "\n",
    "case3_x2 = pd.merge(case3_x,case3,on=['Region','Day_Type','Hour'],how='left')\n",
    "case3_x2 = case3_x2.sort_values(['Region',x_column])\n",
    "print(case3_x2.head(3))\n",
    "print('number of rows in dataset =',case3_x2.shape[0])\n",
    "case3_x2.to_csv('../outputs/'+x_name+'_8760_3daytype_annual_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 hour interval"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 25,
||||||| merged common ancestors
   "execution_count": 17,
=======
<<<<<<< HEAD
   "execution_count": 17,
||||||| merged common ancestors
   "execution_count": null,
=======
   "execution_count": 5,
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
>>>>>>> master
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Region R_Group R_Subgroup  Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "0   ERC_REST     ERC       REST  winter      1    1     1  34807    1  2016   \n",
      "1   ERC_REST     ERC       REST  winter      1    2     1  34716    2  2016   \n",
      "2   ERC_REST     ERC       REST  winter      1    3     1  34736    3  2016   \n",
      "3   ERC_REST     ERC       REST  winter      1    4     1  35914    4  2016   \n",
      "4   ERC_REST     ERC       REST  winter      1    5     1  33845    5  2016   \n",
      "5   ERC_REST     ERC       REST  winter      1    6     1  30995    6  2016   \n",
      "6   ERC_REST     ERC       REST  winter      1    7     1  30186    7  2016   \n",
      "7   ERC_REST     ERC       REST  winter      1    8     1  30745    8  2016   \n",
      "8   ERC_REST     ERC       REST  winter      1    9     1  38702    9  2016   \n",
      "9   ERC_REST     ERC       REST  winter      1   10     1  38567   10  2016   \n",
      "10  ERC_REST     ERC       REST  winter      1   11     1  37495   11  2016   \n",
      "11  ERC_REST     ERC       REST  winter      1   12     1  34274   12  2016   \n",
      "12  ERC_REST     ERC       REST  winter      1   13     1  31815   13  2016   \n",
      "13  ERC_REST     ERC       REST  winter      1   14     1  29614   14  2016   \n",
      "14  ERC_REST     ERC       REST  winter      1   15     1  31453   15  2016   \n",
      "\n",
      "         Date  DOW Day_Type  4-hr  \n",
      "0  2016-01-01    4  Weekday     1  \n",
      "1  2016-01-02    5  Weekend     1  \n",
      "2  2016-01-03    6  Weekend     1  \n",
      "3  2016-01-04    0  Weekday     1  \n",
      "4  2016-01-05    1  Weekday     1  \n",
      "5  2016-01-06    2  Weekday     1  \n",
      "6  2016-01-07    3  Weekday     1  \n",
      "7  2016-01-08    4  Weekday     1  \n",
      "8  2016-01-09    5  Weekend     1  \n",
      "9  2016-01-10    6     Peak     1  \n",
      "10 2016-01-11    0  Weekday     1  \n",
      "11 2016-01-12    1  Weekday     1  \n",
      "12 2016-01-13    2  Weekday     1  \n",
      "13 2016-01-14    3  Weekday     1  \n",
      "14 2016-01-15    4  Weekday     1  \n"
     ]
    }
   ],
||||||| merged common ancestors
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Region R_Group R_Subgroup  Season  Month  Day  Hour   Load  \\\n",
      "0       ERC_REST     ERC       REST  winter      1    1     1  34807   \n",
      "1       ERC_REST     ERC       REST  winter      1    2     1  34716   \n",
      "2       ERC_REST     ERC       REST  winter      1    3     1  34736   \n",
      "3       ERC_REST     ERC       REST  winter      1    4     1  35914   \n",
      "4       ERC_REST     ERC       REST  winter      1    5     1  33845   \n",
      "...          ...     ...        ...     ...    ...  ...   ...    ...   \n",
      "551875   WECC_WY     WEC         WY  winter     12  361    24   1767   \n",
      "551876   WECC_WY     WEC         WY  winter     12  362    24   1744   \n",
      "551877   WECC_WY     WEC         WY  winter     12  363    24   1805   \n",
      "551878   WECC_WY     WEC         WY  winter     12  364    24   1791   \n",
      "551879   WECC_WY     WEC         WY  winter     12  365    24   1834   \n",
      "\n",
      "             Date  DOW Day_Type  4-hr  \n",
      "0      2016-01-01    4  Weekday     1  \n",
      "1      2016-01-01    4  Weekday     1  \n",
      "2      2016-01-01    4  Weekday     1  \n",
      "3      2016-01-01    4  Weekday     1  \n",
      "4      2016-01-01    4  Weekday     1  \n",
      "...           ...  ...      ...   ...  \n",
      "551875 2011-12-31    5  Weekend     6  \n",
      "551876 2011-12-31    5  Weekend     6  \n",
      "551877 2011-12-31    5  Weekend     6  \n",
      "551878 2011-12-31    5  Weekend     6  \n",
      "551879 2011-12-31    5  Weekend     6  \n",
      "\n",
      "[551880 rows x 12 columns]\n"
     ]
    }
   ],
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
   "source": [
    "#read in 4 hour interval counter\n",
<<<<<<< HEAD
    "#interval_4hr = pd.read_csv('inputs/interval_4hr.csv')\n",
    "#print(interval_4hr.head())\n",
    "#print(x_peak.head())\n",
    "\n",
    "x_peak2 = pd.merge(x_peak,interval_4hr,on='Hour',how='left')\n",
    "print(x_peak2.head(15))"
||||||| merged common ancestors
    "interval_4hr = pd.read_csv('inputs/sequential_hours.csv')\n",
    "interval_4hr = interval_4hr.drop(columns=['2-hr','8-hr','12-hr','24-hr','48-hr','120-hr'])\n",
    "#print(interval_4hr)\n",
    "\n",
    "#add an hour counter\n",
    "x_peak['Hour_Counter'] = (x_peak['Hour']) + (x_peak['Day'] - 1) * 24\n",
    "x_peak = x_peak.sort_values(by=['Region','Hour_Counter'])\n",
    "unique_hc = pd.Series(x_peak['Hour_Counter'].unique()).dropna()\n",
    "#print(unique_hc.tail(2))\n",
    "\n",
    "x_peak2 = pd.merge(x_peak,interval_4hr,on='Hour_Counter',how='left')"
=======
    "interval_4hr = pd.read_csv('inputs/daytype_4hr.csv')\n",
    "\n",
    "x_peak2 = pd.merge(x_peak,interval_4hr,on='Hour',how='left').rename(columns={'4-hr_Day':'4-hr'})\n",
    "print(x_peak2)"
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 4: Bi-monthly, weekend/weekday/peak day-types, 4-hour intervals"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 26,
||||||| merged common ancestors
   "execution_count": 18,
=======
<<<<<<< HEAD
   "execution_count": 18,
||||||| merged common ancestors
   "execution_count": null,
=======
   "execution_count": 6,
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
>>>>>>> master
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Bimonth Day_Type  4-hr  Hour_Tot  Load_Tot   Load_Avg\n",
      "0  ERC_REST        1     Peak     1         8    312443  39055.375\n",
      "1  ERC_REST        1     Peak     2         8    366535  45816.875\n",
      "2  ERC_REST        1     Peak     3         8    322104  40263.000\n",
      "3  ERC_REST        1     Peak     4         8    297348  37168.500\n",
      "4  ERC_REST        1     Peak     5         8    332533  41566.625\n",
      "number of segments in dataset = 108.0\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   27   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006    4   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   13   \n",
      "1524  ERC_REST     ERC       REST  shoulder      3   65     3  27059    6   \n",
      "794   ERC_REST     ERC       REST  shoulder      3   65     2  27069    6   \n",
      "\n",
      "      Year       Date  DOW Day_Type  4-hr  Bimonth  Hour_Tot  Load_Tot  \\\n",
      "1545  2016 2016-03-27    6  Weekend     1        2        68   1932774   \n",
      "823   2016 2016-04-04    0  Weekday     1        2       168   4924979   \n",
      "1531  2016 2016-03-13    6  Weekend     1        2        68   1932774   \n",
      "1524  2016 2016-03-06    6  Weekend     1        2        68   1932774   \n",
      "794   2016 2016-03-06    6  Weekend     1        2        68   1932774   \n",
      "\n",
      "          Load_Avg  \n",
      "1545  28423.147059  \n",
      "823   29315.351190  \n",
      "1531  28423.147059  \n",
      "1524  28423.147059  \n",
      "794   28423.147059  \n",
      "number of rows in dataset = 17520\n"
     ]
    }
   ],
||||||| merged common ancestors
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Bimonth Day_Type  4-hr  Hour_Tot     Tot        Avg\n",
      "0  ERC_REST        1     Peak     1         8  312443  39055.375\n",
      "1  ERC_REST        1     Peak     2         8  366535  45816.875\n",
      "2  ERC_REST        1     Peak     3         8  322104  40263.000\n",
      "3  ERC_REST        1     Peak     4         8  297348  37168.500\n",
      "4  ERC_REST        1     Peak     5         8  332533  41566.625\n",
      "number of rows in dataset = 6804\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1524  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "794   ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW Day_Type  4-hr  Bimonth  Hour_Tot      Tot           Avg  \n",
      "1545 2016-03-05    5  Weekend     1        2        62  1790938  28886.096774  \n",
      "823  2016-02-04    3  Weekday     1        2       174  5066815  29119.626437  \n",
      "1531 2016-03-04    4  Weekday     1        2       174  5066815  29119.626437  \n",
      "1524 2016-03-04    4  Weekday     1        2       174  5066815  29119.626437  \n",
      "794  2016-02-03    2  Weekday     1        2       174  5066815  29119.626437  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
   "source": [
    "case4_x = x_peak2.copy()\n",
    "case4_bimonth = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case4_bimonth = case4_bimonth.drop(['seasonal'], axis=1)\n",
    "case4_bimonth = case4_bimonth.rename(columns={'bimonthly':'Bimonth','month':'Month'})\n",
    "\n",
    "case4_x = pd.merge(case4_x, case4_bimonth, on='Month', how='left')\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case4 = case4_x.groupby(['Region','Bimonth','Day_Type','4-hr'],as_index=False).agg(aggregations)\n",
    "case4.columns = case4.columns.droplevel(0)\n",
    "case4.columns = ['Region','Bimonth','Day_Type','4-hr','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case4.head())\n",
    "print('number of segments in dataset =',case4.shape[0]/reg_count)\n",
    "case4.to_csv('../outputs/'+x_name+'_segments_3daytype_bimonth_4hr.csv')\n",
    "print()\n",
    "\n",
    "case4_x2 = pd.merge(case4_x,case4,on=['Region','Bimonth','Day_Type','4-hr'],how='left')\n",
    "case4_x2 = case4_x2.sort_values(['Region',x_column])\n",
    "print(case4_x2.head())\n",
    "print('number of rows in dataset =',case4_x2.shape[0])\n",
    "case4_x2.to_csv('../outputs/'+x_name+'_8760_3daytype_bimonth_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 5: Season-based months, weekend/weekday/peak day-types, 4-hour intervals"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 27,
||||||| merged common ancestors
   "execution_count": 19,
=======
<<<<<<< HEAD
   "execution_count": 19,
||||||| merged common ancestors
   "execution_count": null,
=======
   "execution_count": 7,
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
>>>>>>> master
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Season_Group Day_Type  4-hr  Hour_Tot  Load_Tot      Load_Avg\n",
      "0  ERC_REST             1     Peak     1        12    504276  42023.000000\n",
      "1  ERC_REST             1     Peak     2        12    583360  48613.333333\n",
      "2  ERC_REST             1     Peak     3        12    527061  43921.750000\n",
      "3  ERC_REST             1     Peak     4        12    478413  39867.750000\n",
      "4  ERC_REST             1     Peak     5        12    530620  44218.333333\n",
      "number of segments in dataset = 90.0\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   27   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006    4   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   13   \n",
      "1524  ERC_REST     ERC       REST  shoulder      3   65     3  27059    6   \n",
      "794   ERC_REST     ERC       REST  shoulder      3   65     2  27069    6   \n",
      "\n",
      "      Year       Date  DOW Day_Type  4-hr  Season_Group  Hour_Tot  Load_Tot  \\\n",
      "1545  2016 2016-03-27    6  Weekend     1             2       140   4051780   \n",
      "823   2016 2016-04-04    0  Weekday     1             2       332   9973043   \n",
      "1531  2016 2016-03-13    6  Weekend     1             2       140   4051780   \n",
      "1524  2016 2016-03-06    6  Weekend     1             2       140   4051780   \n",
      "794   2016 2016-03-06    6  Weekend     1             2       140   4051780   \n",
      "\n",
      "          Load_Avg  \n",
      "1545  28941.285714  \n",
      "823   30039.286145  \n",
      "1531  28941.285714  \n",
      "1524  28941.285714  \n",
      "794   28941.285714  \n",
      "number of rows in dataset = 17520\n"
     ]
    }
   ],
||||||| merged common ancestors
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Season_Group Day_Type  4-hr  Hour_Tot     Tot           Avg\n",
      "0  ERC_REST             1     Peak     1        12  504276  42023.000000\n",
      "1  ERC_REST             1     Peak     2        12  583360  48613.333333\n",
      "2  ERC_REST             1     Peak     3        12  527061  43921.750000\n",
      "3  ERC_REST             1     Peak     4        12  478413  39867.750000\n",
      "4  ERC_REST             1     Peak     5        12  530620  44218.333333\n",
      "number of rows in dataset = 5639\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1524  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "794   ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW Day_Type  4-hr  Season_Group  Hour_Tot       Tot  \\\n",
      "1545 2016-03-05    5  Weekend     1             2       111   3268235   \n",
      "823  2016-02-04    3  Weekday     1             2       361  10756588   \n",
      "1531 2016-03-04    4  Weekday     1             2       361  10756588   \n",
      "1524 2016-03-04    4  Weekday     1             2       361  10756588   \n",
      "794  2016-02-03    2  Weekday     1             2       361  10756588   \n",
      "\n",
      "               Avg  \n",
      "1545  29443.558559  \n",
      "823   29796.642659  \n",
      "1531  29796.642659  \n",
      "1524  29796.642659  \n",
      "794   29796.642659  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
   "source": [
    "case5_x = x_peak2.copy()\n",
    "case5_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case5_seasons = case5_seasons.drop(['bimonthly'], axis=1)\n",
    "case5_seasons = case5_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "\n",
    "case5_x = pd.merge(case5_x, case5_seasons, on='Month', how='left')\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case5 = case5_x.groupby(['Region','Season_Group','Day_Type','4-hr'],as_index=False).agg(aggregations)\n",
    "case5.columns = case5.columns.droplevel(0)\n",
    "case5.columns = ['Region','Season_Group','Day_Type','4-hr','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case5.head())\n",
    "print('number of segments in dataset =',case5.shape[0]/reg_count)\n",
    "case5.to_csv('../outputs/'+x_name+'_segments_3daytype_seasonbased_4hr.csv')\n",
    "print()\n",
    "\n",
    "case5_x2 = pd.merge(case5_x,case5,on=['Region','Season_Group','Day_Type','4-hr'],how='left')\n",
    "case5_x2 = case5_x2.sort_values(['Region',x_column])\n",
    "print(case5_x2.head())\n",
    "print('number of rows in dataset =',case5_x2.shape[0])\n",
    "case5_x2.to_csv('../outputs/'+x_name+'_8760_3daytype_seasonbased_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 6: Season, weekend/weekday/peak day-types, 4-hour intervals"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 28,
||||||| merged common ancestors
   "execution_count": 20,
=======
<<<<<<< HEAD
   "execution_count": 20,
||||||| merged common ancestors
   "execution_count": null,
=======
   "execution_count": 156,
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
>>>>>>> master
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region    Season Day_Type  4-hr  Hour_Tot  Load_Tot    Load_Avg\n",
      "0  ERC_REST  shoulder     Peak     1        16    533153  33322.0625\n",
      "1  ERC_REST  shoulder     Peak     2        16    619476  38717.2500\n",
      "2  ERC_REST  shoulder     Peak     3        16    707544  44221.5000\n",
      "3  ERC_REST  shoulder     Peak     4        16    785332  49083.2500\n",
      "4  ERC_REST  shoulder     Peak     5        16    743101  46443.8125\n",
      "number of segments in dataset = 54.0\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   27   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006    4   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   13   \n",
      "1524  ERC_REST     ERC       REST  shoulder      3   65     3  27059    6   \n",
      "794   ERC_REST     ERC       REST  shoulder      3   65     2  27069    6   \n",
      "\n",
      "      Year       Date  DOW Day_Type  4-hr  Hour_Tot  Load_Tot      Load_Avg  \n",
      "1545  2016 2016-03-27    6  Weekend     1       140   4051780  28941.285714  \n",
      "823   2016 2016-04-04    0  Weekday     1       332   9973043  30039.286145  \n",
      "1531  2016 2016-03-13    6  Weekend     1       140   4051780  28941.285714  \n",
      "1524  2016 2016-03-06    6  Weekend     1       140   4051780  28941.285714  \n",
      "794   2016 2016-03-06    6  Weekend     1       140   4051780  28941.285714  \n",
      "number of rows in dataset = 17520\n"
     ]
    }
   ],
||||||| merged common ancestors
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region    Season Day_Type  4-hr  Hour_Tot     Tot         Avg\n",
      "0  ERC_REST  shoulder     Peak     1        16  533153  33322.0625\n",
      "1  ERC_REST  shoulder     Peak     2        16  619476  38717.2500\n",
      "2  ERC_REST  shoulder     Peak     3        16  707544  44221.5000\n",
      "3  ERC_REST  shoulder     Peak     4        16  785332  49083.2500\n",
      "4  ERC_REST  shoulder     Peak     5        16  743101  46443.8125\n",
      "number of rows in dataset = 3402\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1524  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "794   ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW Day_Type  4-hr  Hour_Tot       Tot           Avg  \n",
      "1545 2016-03-05    5  Weekend     1       111   3268235  29443.558559  \n",
      "823  2016-02-04    3  Weekday     1       361  10756588  29796.642659  \n",
      "1531 2016-03-04    4  Weekday     1       361  10756588  29796.642659  \n",
      "1524 2016-03-04    4  Weekday     1       361  10756588  29796.642659  \n",
      "794  2016-02-03    2  Weekday     1       361  10756588  29796.642659  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
>>>>>>> b533966df04c0d9b9ae7276aee57a4b4aa686161
   "source": [
    "case6_x = x_peak2.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case6 = case6_x.groupby(['Region','Season','Day_Type','4-hr'],as_index=False).agg(aggregations)\n",
    "case6.columns = case6.columns.droplevel(0)\n",
    "case6.columns = ['Region','Season','Day_Type','4-hr','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case6.head())\n",
    "print('number of segments in dataset =',case6.shape[0]/reg_count)\n",
    "case6.to_csv('../outputs/'+x_name+'_segments_3daytype_season_4hr.csv')\n",
    "print()\n",
    "\n",
    "case6_x2 = pd.merge(case6_x,case6,on=['Region','Season','Day_Type','4-hr'],how='left')\n",
    "case6_x2 = case6_x2.sort_values(['Region',x_column])\n",
    "print(case6_x2.head())\n",
    "print('number of rows in dataset =',case6_x2.shape[0])\n",
    "case6_x2.to_csv('../outputs/'+x_name+'_8760_3daytype_season_4hr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
