{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data here: \n",
    "## One and two day type \n",
    "## Three day type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day Type: One/two\n",
    "## Create Date and Day of Week columns in the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output files are written out in parent directory: C:\\Users\\tgoforth\\Documents\\IPM temporal resolution project\\outputs\n",
      "output files are written out in parent directory: C:\\Users\\tgoforth\\Documents\\IPM temporal resolution project\\outputs/load\n"
     ]
    }
   ],
   "source": [
    "#importing packages needed for analysis\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from pandas import DataFrame\n",
    "\n",
    "path = os.getcwd()\n",
    "#print(path)\n",
    "\n",
    "load_dur = pd.read_csv('../outputs/load_long_format.csv')\n",
    "solar_dur = pd.read_csv('../outputs/solar_long_format.csv')\n",
    "wind_dur = pd.read_csv('../outputs/wind_long_format.csv')\n",
    "\n",
    "## UNCOMMENT WHICH PROFILE TO BE USED\n",
    "x = load_dur\n",
    "x_name = 'load'\n",
    "x_name2 = 'Load'\n",
    "x_column = 'Load'\n",
    "\n",
    "#x = solar_dur\n",
    "#x_name = 'solar'\n",
    "#x_name2 = 'Solar_Gen'\n",
    "#choose TRG \n",
    "#x_column = 'TRG6'\n",
    "\n",
    "#x = wind_dur\n",
    "#x_name = 'wind'\n",
    "#x_name2 = 'Wind_Gen'\n",
    "#x_column = 'TRG4'\n",
    "\n",
    "#this code creates an output directory in the parent director, if one does not exist yet\n",
    "#Note: this is where all of the output files will be written, since outputs are large this saves space in git\n",
    "path = os.getcwd()\n",
    "parent = os.path.dirname(path)\n",
    "outputs_dir = parent+'\\outputs'\n",
    "if not os.path.exists(outputs_dir):\n",
    "    os.makedirs(outputs_dir)\n",
    "print('output files are written out in parent directory: '+outputs_dir)\n",
    "\n",
    "outputs_x = outputs_dir+'/'+x_name\n",
    "if not os.path.exists(outputs_x):\n",
    "    os.makedirs(outputs_x)\n",
    "print('output files are written out in parent directory: '+outputs_x)\n",
    "\n",
    "x = x[['Region','R_Group','R_Subgroup','Season','Month','Day','Hour',x_column]]\n",
    "years = pd.read_csv('inputs/years.csv').dropna()\n",
    "\n",
    "#print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "#repeat the date for 24 times for each hour in the day\n",
    "year_2016 = years['2016'].repeat(24).reset_index(drop=True)\n",
    "year_2011 = years['2011'].repeat(24).reset_index(drop=True)\n",
    "#print(year_2016)\n",
    "\n",
    "#use itertools.cycle to make the date repeat until it reaches the end of the dataframe\n",
    "year_2016 = cycle(year_2016)\n",
    "year_2011 = cycle(year_2011)\n",
    "\n",
    "#create temporary data frames where they will iterate through. Needed to achieve the correct length of the DF\n",
    "temp2016 = x.loc[x['R_Group'] == 'ERC'].copy()\n",
    "temp2011 = x.loc[x['R_Group'] != 'ERC'].copy()\n",
    "\n",
    "#iterate through the date until the end of DF \n",
    "temp2016['Date'] = [next(year_2016) for year in range(len(temp2016))]\n",
    "temp2011['Date'] = [next(year_2011) for year in range(len(temp2011))]\n",
    "#print(temp2016)\n",
    "#print(temp2011)\n",
    "\n",
    "x2 = pd.concat([temp2016, temp2011], ignore_index=True)\n",
    "#print(x2)\n",
    "\n",
    "#convert date to a datetime type \n",
    "x2['Date'] = pd.to_datetime(x2['Date'])\n",
    "x2['DOW'] = x2['Date'].dt.weekday\n",
    "\n",
    "#check if it is a weekday or not \n",
    "weekday = pd.read_csv('inputs/weekday.csv')\n",
    "x2 = pd.merge(x2,weekday,on='DOW',how='left')\n",
    "#print(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Monthly, single day type, 24 hours (288 segments)\n",
    "#### Methodology: Using groupby function to group first by month, then by 24 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Month  Hour  Hour_Tot      Tot           Avg\n",
      "0  ERC_REST      1     1        31  1042446  33627.290323\n",
      "1  ERC_REST      1     2        31  1051661  33924.548387\n",
      "2  ERC_REST      1     3        31  1079739  34830.290323\n",
      "3  ERC_REST      1     4        31  1144492  36919.096774\n",
      "4  ERC_REST      1     5        31  1239385  39980.161290\n",
      "number of rows in dataset = 18144\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "\n",
      "           Date  DOW  Weekday  Hour_Tot     Tot           Avg  \n",
      "1545 2016-03-05    5    False        31  886708  28603.483871  \n",
      "823  2016-02-04    3     True        30  870591  29019.700000  \n",
      "1531 2016-03-04    4     True        31  886708  28603.483871  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case1_x = x2.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case1 = case1_x.groupby(['Region','Month','Hour'],as_index=False).agg(aggregations)\n",
    "case1.columns = case1.columns.droplevel(0)\n",
    "case1.columns = ['Region','Month','Hour','Hour_Tot','Tot','Avg']\n",
    "print(case1.head())\n",
    "print('number of rows in dataset =',case1.shape[0])\n",
    "case1.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_1daytype_monthly_24hr.csv')\n",
    "print()\n",
    "\n",
    "case1_x2 = pd.merge(case1_x,case1,on=['Region','Month','Hour'],how='left')\n",
    "case1_x2 = case1_x2.sort_values(['Region',x_column])\n",
    "print(case1_x2.head(3))\n",
    "print('number of rows in dataset =',case1_x2.shape[0])\n",
    "case1_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_1daytype_monthly_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: Season, single day-type, 24 hours (72 segments)\n",
    "#### Methodology: Use groupby function to group by season and hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Season_Group  Hour  Hour_Tot      Tot           Avg\n",
      "0  ERC_REST             1     1        90  2928629  32540.322222\n",
      "1  ERC_REST             1     2        90  2929331  32548.122222\n",
      "2  ERC_REST             1     3        90  2978696  33096.622222\n",
      "3  ERC_REST             1     4        90  3116896  34632.177778\n",
      "4  ERC_REST             1     5        90  3346197  37179.966667\n",
      "number of rows in dataset = 7560\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "\n",
      "           Date  DOW  Weekday  Season_Group  Hour_Tot      Tot           Avg  \n",
      "1545 2016-03-05    5    False             2       122  3603145  29533.975410  \n",
      "823  2016-02-04    3     True             2       122  3587428  29405.147541  \n",
      "1531 2016-03-04    4     True             2       122  3603145  29533.975410  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case2_x = x2.copy()\n",
    "case2_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case2_seasons = case2_seasons.drop(['bimonthly'], axis=1)\n",
    "case2_seasons = case2_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "\n",
    "case2_x = pd.merge(case2_x, case2_seasons, on='Month', how='left')\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case2 = case2_x.groupby(['Region','Season_Group','Hour'],as_index=False).agg(aggregations)\n",
    "case2.columns = case2.columns.droplevel(0)\n",
    "case2.columns = ['Region','Season_Group','Hour','Hour_Tot','Tot','Avg']\n",
    "print(case2.head())\n",
    "print('number of rows in dataset =',case2.shape[0])\n",
    "case2.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_1daytype_season_24hr.csv')\n",
    "print()\n",
    "\n",
    "case2_x2 = pd.merge(case2_x,case2,on=['Region','Season_Group','Hour'],how='left')\n",
    "case2_x2 = case2_x2.sort_values(['Region',x_column])\n",
    "print(case2_x2.head(3))\n",
    "print('number of rows in dataset =',case2_x2.shape[0])\n",
    "case2_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_1daytype_season_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3: Monthly, weekend/weekday, 24 hours (576 segments)\n",
    "#### Metholodogy: Use groupby function to group by month, then weekend/weekday, then by 24 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Month  Hour  Hour_Tot      Tot           Avg\n",
      "0  ERC_REST      1     1        31  1042446  33627.290323\n",
      "1  ERC_REST      1     2        31  1051661  33924.548387\n",
      "2  ERC_REST      1     3        31  1079739  34830.290323\n",
      "3  ERC_REST      1     4        31  1144492  36919.096774\n",
      "4  ERC_REST      1     5        31  1239385  39980.161290\n",
      "number of rows in dataset = 24507\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1524  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "794   ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW  Weekday  Hour_Tot     Tot           Avg  \n",
      "1545 2016-03-05    5    False        14  406333  29023.785714  \n",
      "823  2016-02-04    3     True        30  870591  29019.700000  \n",
      "1531 2016-03-04    4     True        17  480375  28257.352941  \n",
      "1524 2016-03-04    4     True        17  480375  28257.352941  \n",
      "794  2016-02-03    2     True        31  875126  28229.870968  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case3_x = x2.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case3 = case3_x.groupby(['Region','Month','Weekday','Hour'],as_index=False).agg(aggregations)\n",
    "case3.columns = case3.columns.droplevel(0)\n",
    "case3.columns = ['Region','Month','Weekday','Hour','Hour_Tot','Tot','Avg']\n",
    "print(case1.head())\n",
    "print('number of rows in dataset =',case3.shape[0])\n",
    "case3.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_2daytype_monthly_24hr.csv')\n",
    "print()\n",
    "\n",
    "case3_x2 = pd.merge(case3_x,case3,on=['Region','Month','Weekday','Hour'],how='left')\n",
    "case3_x2 = case3_x2.sort_values(['Region',x_column])\n",
    "print(case3_x2.head())\n",
    "print('number of rows in dataset =',case3_x2.shape[0])\n",
    "case3_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_2daytype_monthly_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 4: Season, weekend/weekday, 24 hours (144 segments)\n",
    "#### Methodology: groupby season, weekday, hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Season_Group  Weekday  Hour  Hour_Tot      Tot           Avg\n",
      "0  ERC_REST             1    False     1        40  1226650  30666.250000\n",
      "1  ERC_REST             1    False     2        30  1003087  33436.233333\n",
      "2  ERC_REST             1    False     4        52  1741883  33497.750000\n",
      "3  ERC_REST             1    False     5        18   669258  37181.000000\n",
      "4  ERC_REST             1    False     7        64  2446079  38219.984375\n",
      "number of rows in dataset = 12478\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "1545  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "823   ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1531  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1524  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "794   ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW  Weekday  Season_Group  Hour_Tot      Tot           Avg  \n",
      "1545 2016-03-05    5    False             2        63  1894236  30067.238095  \n",
      "823  2016-02-04    3     True             2        90  2653648  29484.977778  \n",
      "1531 2016-03-04    4     True             2        59  1708909  28964.559322  \n",
      "1524 2016-03-04    4     True             2        59  1708909  28964.559322  \n",
      "794  2016-02-03    2     True             2        90  2653648  29484.977778  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case4_x = x2.copy()\n",
    "case4_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case4_seasons = case4_seasons.drop(['bimonthly'], axis=1)\n",
    "case4_seasons = case4_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "\n",
    "case4_x = pd.merge(case4_x, case4_seasons, on='Month', how='left')\n",
    "#print(case4_x)\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case4 = case4_x.groupby(['Region','Season_Group','Weekday','Hour'],as_index=False).agg(aggregations)\n",
    "case4.columns = case4.columns.droplevel(0)\n",
    "case4.columns = ['Region','Season_Group','Weekday','Hour','Hour_Tot','Tot','Avg']\n",
    "print(case4.head())\n",
    "print('number of rows in dataset =',case4.shape[0])\n",
    "case4.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_2daytype_season_24hr.csv')\n",
    "print()\n",
    "\n",
    "case4_x2 = pd.merge(case4_x,case4,on=['Region','Season_Group','Weekday','Hour'],how='left')\n",
    "case4_x2 = case4_x2.sort_values(['Region',x_column])\n",
    "print(case4_x2.head())\n",
    "print('number of rows in dataset =',case4_x2.shape[0])\n",
    "case4_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_2daytype_season_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 hour interval day types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Hour_Counter  4-hr\n",
      "0                1     1\n",
      "1                2     1\n",
      "2                3     1\n",
      "3                4     1\n",
      "4                5     2\n",
      "...            ...   ...\n",
      "8755          8756  2189\n",
      "8756          8757  2190\n",
      "8757          8758  2190\n",
      "8758          8759  2190\n",
      "8759          8760  2190\n",
      "\n",
      "[8760 rows x 2 columns]\n",
      "          Region R_Group R_Subgroup  Season  Month  Day  Hour   Load  \\\n",
      "0       ERC_REST     ERC       REST  winter      1    1     1  34807   \n",
      "1       ERC_REST     ERC       REST  winter      1    1     2  34551   \n",
      "2       ERC_REST     ERC       REST  winter      1    1     3  34788   \n",
      "3       ERC_REST     ERC       REST  winter      1    1     4  35531   \n",
      "4       ERC_REST     ERC       REST  winter      1    1     5  36633   \n",
      "...          ...     ...        ...     ...    ...  ...   ...    ...   \n",
      "551875  WEC_SDGE     WEC       SDGE  winter     12  365    20   2809   \n",
      "551876  WEC_SDGE     WEC       SDGE  winter     12  365    21   2675   \n",
      "551877  WEC_SDGE     WEC       SDGE  winter     12  365    22   2524   \n",
      "551878  WEC_SDGE     WEC       SDGE  winter     12  365    23   2362   \n",
      "551879  WEC_SDGE     WEC       SDGE  winter     12  365    24   2206   \n",
      "\n",
      "             Date  DOW  Weekday  Hour_Counter  4-hr  \n",
      "0      2016-01-01    4     True             1     1  \n",
      "1      2016-01-31    6    False             2     1  \n",
      "2      2016-03-01    1     True             3     1  \n",
      "3      2016-04-01    4     True             4     1  \n",
      "4      2016-05-01    6    False             5     2  \n",
      "...           ...  ...      ...           ...   ...  \n",
      "551875 2011-05-02    0     True          8756  2189  \n",
      "551876 2011-11-16    2     True          8757  2190  \n",
      "551877 2011-06-02    3     True          8758  2190  \n",
      "551878 2011-12-16    4     True          8759  2190  \n",
      "551879 2011-07-02    5    False          8760  2190  \n",
      "\n",
      "[551880 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "#read in 4 hour interval counter\n",
    "interval_4hr = pd.read_csv('inputs/sequential_hours.csv')\n",
    "interval_4hr = interval_4hr.drop(columns=['2-hr','8-hr','12-hr','24-hr','48-hr','120-hr'])\n",
    "print(interval_4hr)\n",
    "\n",
    "#add an hour counter\n",
    "x2['Hour_Counter'] = (x2['Hour']) + (x2['Day'] - 1) * 24\n",
    "x2 = x2.sort_values(by=['Region','Hour_Counter'])\n",
    "unique_hc = pd.Series(x2['Hour_Counter'].unique()).dropna()\n",
    "#print(unique_hc.tail(2))\n",
    "\n",
    "x3 = pd.merge(x2,interval_4hr,on='Hour_Counter',how='left')\n",
    "print(x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 5: Monthly, single day type, 4 hour intervals (72 segments)\n",
    "#### Methodology: use groupby by month, 4 hour intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Month  4-hr  Hour_Tot     Tot       Avg\n",
      "0  ERC_REST      1     1         4  139677  34919.25\n",
      "1  ERC_REST      1     2         4  153285  38321.25\n",
      "2  ERC_REST      1     3         4  160446  40111.50\n",
      "3  ERC_REST      1     4         4  155314  38828.50\n",
      "4  ERC_REST      1     5         4  162472  40618.00\n",
      "number of rows in dataset = 137970\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "2042  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "2233  ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1706  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1538  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "1537  ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW  Weekday  Hour_Counter  4-hr  Hour_Tot     Tot       Avg  \n",
      "2042 2016-03-05    5    False          2043   511         4  108862  27215.50  \n",
      "2233 2016-02-04    3     True          2234   559         4  111068  27767.00  \n",
      "1706 2016-03-04    4     True          1707   427         4  109231  27307.75  \n",
      "1538 2016-03-04    4     True          1539   385         4  108948  27237.00  \n",
      "1537 2016-02-03    2     True          1538   385         4  108948  27237.00  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case5_x = x3.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case5 = case5_x.groupby(['Region','Month','4-hr'],as_index=False).agg(aggregations)\n",
    "case5.columns = case5.columns.droplevel(0)\n",
    "case5.columns = ['Region','Month','4-hr','Hour_Tot','Tot','Avg']\n",
    "print(case5.head())\n",
    "print('number of rows in dataset =',case5.shape[0])\n",
    "case5.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_2daytype_month_4hr.csv')\n",
    "print()\n",
    "\n",
    "case5_x2 = pd.merge(case5_x,case5,on=['Region','Month','4-hr'],how='left')\n",
    "case5_x2 = case5_x2.sort_values(['Region',x_column])\n",
    "print(case5_x2.head())\n",
    "print('number of rows in dataset =',case5_x2.shape[0])\n",
    "case5_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_2daytype_month_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 6: Bi-monthly weekend/weekday day-types, 4-hour intervals\n",
    "#### Methodology: use groupby function and bimonthly groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Bimonth  4-hr  Hour_Tot     Tot       Avg\n",
      "0  ERC_REST        1     1         4  139677  34919.25\n",
      "1  ERC_REST        1     2         4  153285  38321.25\n",
      "2  ERC_REST        1     3         4  160446  40111.50\n",
      "3  ERC_REST        1     4         4  155314  38828.50\n",
      "4  ERC_REST        1     5         4  162472  40618.00\n",
      "number of rows in dataset = 137970\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "2042  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "2233  ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1706  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1538  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "1537  ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW  Weekday  Hour_Counter  4-hr  Bimonth  Hour_Tot     Tot  \\\n",
      "2042 2016-03-05    5    False          2043   511        2         4  108862   \n",
      "2233 2016-02-04    3     True          2234   559        2         4  111068   \n",
      "1706 2016-03-04    4     True          1707   427        2         4  109231   \n",
      "1538 2016-03-04    4     True          1539   385        2         4  108948   \n",
      "1537 2016-02-03    2     True          1538   385        2         4  108948   \n",
      "\n",
      "           Avg  \n",
      "2042  27215.50  \n",
      "2233  27767.00  \n",
      "1706  27307.75  \n",
      "1538  27237.00  \n",
      "1537  27237.00  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case6_x = x3.copy()\n",
    "case6_bimonth = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case6_bimonth = case6_bimonth.drop(['seasonal'], axis=1)\n",
    "case6_bimonth = case6_bimonth.rename(columns={'bimonthly':'Bimonth','month':'Month'})\n",
    "\n",
    "case6_x = pd.merge(case6_x, case6_bimonth, on='Month', how='left')\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case6 = case6_x.groupby(['Region','Bimonth','4-hr'],as_index=False).agg(aggregations)\n",
    "case6.columns = case6.columns.droplevel(0)\n",
    "case6.columns = ['Region','Bimonth','4-hr','Hour_Tot','Tot','Avg']\n",
    "print(case6.head())\n",
    "print('number of rows in dataset =',case6.shape[0])\n",
    "case6.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_2daytype_bimonth_4hr.csv')\n",
    "print()\n",
    "\n",
    "case6_x2 = pd.merge(case6_x,case6,on=['Region','Bimonth','4-hr'],how='left')\n",
    "case6_x2 = case6_x2.sort_values(['Region',x_column])\n",
    "print(case6_x2.head())\n",
    "print('number of rows in dataset =',case6_x2.shape[0])\n",
    "case6_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_2daytype_bimonth_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 7: Season-based months, weekend/weekday day-types, 4-hour intervals\n",
    "#### Methodology: groupby function and applied season groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region    Season  Weekday  4-hr  Hour_Tot    Tot      Avg\n",
      "0  ERC_REST  shoulder    False   355         2  57760  28880.0\n",
      "1  ERC_REST  shoulder    False   356         1  34354  34354.0\n",
      "2  ERC_REST  shoulder    False   357         1  37148  37148.0\n",
      "3  ERC_REST  shoulder    False   358         2  74416  37208.0\n",
      "4  ERC_REST  shoulder    False   359         1  37470  37470.0\n",
      "number of rows in dataset = 247154\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "2042  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "2233  ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1706  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1538  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "1537  ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW  Weekday  Hour_Counter  4-hr  Season_Group  Hour_Tot  \\\n",
      "2042 2016-03-05    5    False          2043   511             2         1   \n",
      "2233 2016-02-04    3     True          2234   559             2         3   \n",
      "1706 2016-03-04    4     True          1707   427             2         3   \n",
      "1538 2016-03-04    4     True          1539   385             2         2   \n",
      "1537 2016-02-03    2     True          1538   385             2         2   \n",
      "\n",
      "        Tot           Avg  \n",
      "2042  26989  26989.000000  \n",
      "2233  83494  27831.333333  \n",
      "1706  81445  27148.333333  \n",
      "1538  54128  27064.000000  \n",
      "1537  54128  27064.000000  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case7_x = x3.copy()\n",
    "case7_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case7_seasons = case7_seasons.drop(['bimonthly'], axis=1)\n",
    "case7_seasons = case7_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "\n",
    "case7_x = pd.merge(case7_x, case7_seasons, on='Month', how='left')\n",
    "#print(case4_load)\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case7 = case7_x.groupby(['Region','Season','Weekday','4-hr'],as_index=False).agg(aggregations)\n",
    "case7.columns = case7.columns.droplevel(0)\n",
    "case7.columns = ['Region','Season','Weekday','4-hr','Hour_Tot','Tot','Avg']\n",
    "print(case7.head())\n",
    "print('number of rows in dataset =',case7.shape[0])\n",
    "case7.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_2daytype_season_4hr.csv')\n",
    "print()\n",
    "\n",
    "case7_x2 = pd.merge(case7_x,case7,on=['Region','Season','Weekday','4-hr'],how='left')\n",
    "case7_x2 = case7_x2.sort_values(['Region',x_column])\n",
    "print(case7_x2.head())\n",
    "print('number of rows in dataset =',case7_x2.shape[0])\n",
    "case7_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_2daytype_season_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day type: Three (Weekday, Weekend, Peak Load)\n",
    "## Find Peak Load Days in Each Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Region R_Group R_Subgroup  Season  Month  Day  Hour   Load     Tot  \\\n",
      "0       ERC_REST     ERC       REST  winter      1    1     1  34807  917588   \n",
      "1       ERC_REST     ERC       REST  winter      1    2     1  34716  863132   \n",
      "2       ERC_REST     ERC       REST  winter      1    3     1  34736  934513   \n",
      "3       ERC_REST     ERC       REST  winter      1    4     1  35914  960576   \n",
      "4       ERC_REST     ERC       REST  winter      1    5     1  33845  916216   \n",
      "...          ...     ...        ...     ...    ...  ...   ...    ...     ...   \n",
      "551875   WECC_WY     WEC         WY  winter     12  361    24   1767   51381   \n",
      "551876   WECC_WY     WEC         WY  winter     12  362    24   1744   49456   \n",
      "551877   WECC_WY     WEC         WY  winter     12  363    24   1805   48194   \n",
      "551878   WECC_WY     WEC         WY  winter     12  364    24   1791   47653   \n",
      "551879   WECC_WY     WEC         WY  winter     12  365    24   1834   47420   \n",
      "\n",
      "            Max  \n",
      "0       1000605  \n",
      "1       1000605  \n",
      "2       1000605  \n",
      "3       1000605  \n",
      "4       1000605  \n",
      "...         ...  \n",
      "551875    56554  \n",
      "551876    56554  \n",
      "551877    56554  \n",
      "551878    56554  \n",
      "551879    56554  \n",
      "\n",
      "[551880 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "#create temporary DF to find peak\n",
    "test = x.copy()\n",
    "\n",
    "#groupby region, month, and day to sum the total day\n",
    "aggregations1 = {x_column:sum}\n",
    "test_sum = test.groupby(['Region','Month','Day'],as_index=False).agg(aggregations1)\n",
    "#test1.columns = test1.columns.droplevel(0)\n",
    "test_sum.columns = ['Region','Month','Day','Tot']\n",
    "#print(test_sum.head())\n",
    "#print('number of rows in dataset =',test_sum.shape[0])\n",
    "\n",
    "test3 = pd.merge(test,test_sum,on=['Region','Month','Day'],how='left')\n",
    "#print(test3)\n",
    "\n",
    "#groupby region and month to find maximum \n",
    "aggregations2 = {'Tot':max}\n",
    "test_max = test_sum.groupby(['Region','Month'],as_index=False).agg(aggregations2)\n",
    "test_max.columns = ['Region','Month','Max']\n",
    "#print(test_max.head())\n",
    "\n",
    "test4 = pd.merge(test3,test_max,on=['Region','Month'],how='left')\n",
    "print(test4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Region R_Group R_Subgroup  Season  Month  Day  Hour   Load  \\\n",
      "0       ERC_REST     ERC       REST  winter      1    1     1  34807   \n",
      "1       ERC_REST     ERC       REST  winter      1    2     1  34716   \n",
      "2       ERC_REST     ERC       REST  winter      1    3     1  34736   \n",
      "3       ERC_REST     ERC       REST  winter      1    4     1  35914   \n",
      "4       ERC_REST     ERC       REST  winter      1    5     1  33845   \n",
      "...          ...     ...        ...     ...    ...  ...   ...    ...   \n",
      "551875   WECC_WY     WEC         WY  winter     12  361    24   1767   \n",
      "551876   WECC_WY     WEC         WY  winter     12  362    24   1744   \n",
      "551877   WECC_WY     WEC         WY  winter     12  363    24   1805   \n",
      "551878   WECC_WY     WEC         WY  winter     12  364    24   1791   \n",
      "551879   WECC_WY     WEC         WY  winter     12  365    24   1834   \n",
      "\n",
      "             Date  DOW Day_Type  \n",
      "0      2016-01-01    4  Weekday  \n",
      "1      2016-01-01    4  Weekday  \n",
      "2      2016-01-01    4  Weekday  \n",
      "3      2016-01-01    4  Weekday  \n",
      "4      2016-01-01    4  Weekday  \n",
      "...           ...  ...      ...  \n",
      "551875 2011-12-31    5  Weekend  \n",
      "551876 2011-12-31    5  Weekend  \n",
      "551877 2011-12-31    5  Weekend  \n",
      "551878 2011-12-31    5  Weekend  \n",
      "551879 2011-12-31    5  Weekend  \n",
      "\n",
      "[551880 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "x_peak = x2.copy()\n",
    "\n",
    "x_peak = pd.merge(x_peak,test4,on=['Region','R_Group','R_Subgroup','Season','Month','Day','Hour',x_column],how='left')\n",
    "x_peak = x_peak.rename(columns={'Weekday':'Day_Type'})\n",
    "\n",
    "#Return True if the load total equals the day identified as the max\n",
    "x_peak.loc[x_peak['Day_Type'] == True, 'Day_Type'] = 'Weekday'\n",
    "x_peak.loc[x_peak['Day_Type'] == False, 'Day_Type'] = 'Weekend'\n",
    "x_peak.loc[x_peak['Tot'] == x_peak['Max'], 'Day_Type'] = 'Peak'\n",
    "x_peak = x_peak.drop(['Tot','Max'], axis=1)\n",
    "print(x_peak)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Monthly, Weekend/weekday/peak day-types, 24 hours (864 segments)\n",
    "#### Methodology: similar to two day type, just adding in peak day types to sort by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Month Day_Type  Hour  Hour_Tot    Tot      Avg\n",
      "0  ERC_REST      1     Peak     1         1  38567  38567.0\n",
      "1  ERC_REST      1     Peak     2         1  39107  39107.0\n",
      "2  ERC_REST      1     Peak     3         1  40311  40311.0\n",
      "3  ERC_REST      1     Peak     4         1  43115  43115.0\n",
      "4  ERC_REST      1     Peak     5         1  47186  47186.0\n",
      "number of rows in dataset = 42612\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "2042  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "2233  ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1706  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "\n",
      "           Date  DOW Day_Type  Hour_Counter  Hour_Tot     Tot           Avg  \n",
      "2042 2016-03-05    5  Weekend          2043        13  376380  28952.307692  \n",
      "2233 2016-02-04    3  Weekday          2234        29  837721  28886.931034  \n",
      "1706 2016-03-04    4  Weekday          1707        17  480375  28257.352941  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case1_x = x_peak.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case1 = case1_x.groupby(['Region','Month','Day_Type','Hour'],as_index=False).agg(aggregations)\n",
    "case1.columns = case1.columns.droplevel(0)\n",
    "case1.columns = ['Region','Month','Day_Type','Hour','Hour_Tot','Tot','Avg']\n",
    "print(case1.head())\n",
    "print('number of rows in dataset =',case1.shape[0])\n",
    "case1.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_3daytype_monthly_24hr.csv')\n",
    "print()\n",
    "\n",
    "case1_x2 = pd.merge(case1_x,case1,on=['Region','Month','Day_Type','Hour'],how='left')\n",
    "case1_x2 = case1_x2.sort_values(['Region',x_column])\n",
    "print(case1_x2.head(3))\n",
    "print('number of rows in dataset =',case1_x2.shape[0])\n",
    "case1_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_3daytype_monthly_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: Season, weekend/weekday/peak day-types, 24-hours (216 segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region    Season Day_Type  Hour  Hour_Tot     Tot       Avg\n",
      "0  ERC_REST  shoulder     Peak     1         4  132690  33172.50\n",
      "1  ERC_REST  shoulder     Peak     2         4  130802  32700.50\n",
      "2  ERC_REST  shoulder     Peak     3         4  131751  32937.75\n",
      "3  ERC_REST  shoulder     Peak     4         4  137910  34477.50\n",
      "4  ERC_REST  shoulder     Peak     5         4  150015  37503.75\n",
      "number of rows in dataset = 12919\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "2042  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "2233  ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1706  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "\n",
      "           Date  DOW Day_Type  Hour_Counter  Hour_Tot     Tot           Avg  \n",
      "2042 2016-03-05    5  Weekend          2043        13  376380  28952.307692  \n",
      "2233 2016-02-04    3  Weekday          2234        29  837721  28886.931034  \n",
      "1706 2016-03-04    4  Weekday          1707        17  480375  28257.352941  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case2_x = x_peak.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case2 = case2_x.groupby(['Region','Season','Day_Type','Hour'],as_index=False).agg(aggregations)\n",
    "case2.columns = case2.columns.droplevel(0)\n",
    "case2.columns = ['Region','Season','Day_Type','Hour','Hour_Tot','Tot','Avg']\n",
    "print(case2.head())\n",
    "print('number of rows in dataset =',case2.shape[0])\n",
    "case2.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_3daytype_season_24hr.csv')\n",
    "print()\n",
    "\n",
    "case2_x2 = pd.merge(case2_x,case2,on=['Region','Season','Day_Type','Hour'],how='left')\n",
    "case2_x2 = case2_x2.sort_values(['Region',x_column])\n",
    "print(case1_x2.head(3))\n",
    "print('number of rows in dataset =',case1_x2.shape[0])\n",
    "case2_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_3daytype_season_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3: Annual, weekend/weekday/peak day-types, 24-hours (72 segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region Day_Type  Hour  Hour_Tot     Tot           Avg\n",
      "0  ERC_REST     Peak     1        12  455893  37991.083333\n",
      "1  ERC_REST     Peak     2        12  450804  37567.000000\n",
      "2  ERC_REST     Peak     3        12  454384  37865.333333\n",
      "3  ERC_REST     Peak     4        12  473371  39447.583333\n",
      "4  ERC_REST     Peak     5        12  505999  42166.583333\n",
      "number of rows in dataset = 4536\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "2042  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "2233  ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1706  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "\n",
      "           Date  DOW Day_Type  Hour_Counter  Hour_Tot      Tot           Avg  \n",
      "2042 2016-03-05    5  Weekend          2043        92  2899625  31517.663043  \n",
      "2233 2016-02-04    3  Weekday          2234       247  8133375  32928.643725  \n",
      "1706 2016-03-04    4  Weekday          1707       261  8694501  33312.264368  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case3_x = x_peak.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case3 = case3_x.groupby(['Region','Day_Type','Hour'],as_index=False).agg(aggregations)\n",
    "case3.columns = case3.columns.droplevel(0)\n",
    "case3.columns = ['Region','Day_Type','Hour','Hour_Tot','Tot','Avg']\n",
    "print(case3.head())\n",
    "print('number of rows in dataset =',case3.shape[0])\n",
    "case3.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_3daytype_annual_24hr.csv')\n",
    "print()\n",
    "\n",
    "case3_x2 = pd.merge(case3_x,case3,on=['Region','Day_Type','Hour'],how='left')\n",
    "case3_x2 = case3_x2.sort_values(['Region',x_column])\n",
    "print(case3_x2.head(3))\n",
    "print('number of rows in dataset =',case3_x2.shape[0])\n",
    "case3_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_3daytype_annual_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 hour interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in 4 hour interval counter\n",
    "interval_4hr = pd.read_csv('inputs/sequential_hours.csv')\n",
    "interval_4hr = interval_4hr.drop(columns=['2-hr','8-hr','12-hr','24-hr','48-hr','120-hr'])\n",
    "#print(interval_4hr)\n",
    "\n",
    "#add an hour counter\n",
    "x_peak['Hour_Counter'] = (x_peak['Hour']) + (x_peak['Day'] - 1) * 24\n",
    "x_peak = x_peak.sort_values(by=['Region','Hour_Counter'])\n",
    "unique_hc = pd.Series(x_peak['Hour_Counter'].unique()).dropna()\n",
    "#print(unique_hc.tail(2))\n",
    "\n",
    "x_peak2 = pd.merge(x_peak,interval_4hr,on='Hour_Counter',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 4: Bi-monthly, weekend/weekday/peak day-types, 4-hour intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Bimonth Day_Type  4-hr  Hour_Tot     Tot       Avg\n",
      "0  ERC_REST        1     Peak    55         4  161100  40275.00\n",
      "1  ERC_REST        1     Peak    56         4  187905  46976.25\n",
      "2  ERC_REST        1     Peak    57         4  168812  42203.00\n",
      "3  ERC_REST        1     Peak    58         4  155705  38926.25\n",
      "4  ERC_REST        1     Peak    59         4  172847  43211.75\n",
      "number of rows in dataset = 243586\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "2042  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "2233  ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1706  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1538  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "1537  ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW Day_Type  Hour_Counter  4-hr  Bimonth  Hour_Tot    Tot  \\\n",
      "2042 2016-03-05    5  Weekend          2043   511        2         1  26989   \n",
      "2233 2016-02-04    3  Weekday          2234   559        2         3  83494   \n",
      "1706 2016-03-04    4  Weekday          1707   427        2         3  81445   \n",
      "1538 2016-03-04    4  Weekday          1539   385        2         2  54128   \n",
      "1537 2016-02-03    2  Weekday          1538   385        2         2  54128   \n",
      "\n",
      "               Avg  \n",
      "2042  26989.000000  \n",
      "2233  27831.333333  \n",
      "1706  27148.333333  \n",
      "1538  27064.000000  \n",
      "1537  27064.000000  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case4_x = x_peak2.copy()\n",
    "case4_bimonth = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case4_bimonth = case4_bimonth.drop(['seasonal'], axis=1)\n",
    "case4_bimonth = case4_bimonth.rename(columns={'bimonthly':'Bimonth','month':'Month'})\n",
    "\n",
    "case4_x = pd.merge(case4_x, case4_bimonth, on='Month', how='left')\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case4 = case4_x.groupby(['Region','Bimonth','Day_Type','4-hr'],as_index=False).agg(aggregations)\n",
    "case4.columns = case4.columns.droplevel(0)\n",
    "case4.columns = ['Region','Bimonth','Day_Type','4-hr','Hour_Tot','Tot','Avg']\n",
    "print(case4.head())\n",
    "print('number of rows in dataset =',case4.shape[0])\n",
    "case4.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_3daytype_bimonth_4hr.csv')\n",
    "print()\n",
    "\n",
    "case4_x2 = pd.merge(case4_x,case4,on=['Region','Bimonth','Day_Type','4-hr'],how='left')\n",
    "case4_x2 = case4_x2.sort_values(['Region',x_column])\n",
    "print(case4_x2.head())\n",
    "print('number of rows in dataset =',case4_x2.shape[0])\n",
    "case4_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_3daytype_bimonth_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 5: Season-based months, weekend/weekday/peak day-types, 4-hour intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Season_Group Day_Type  4-hr  Hour_Tot     Tot       Avg\n",
      "0  ERC_REST             1     Peak    55         4  161100  40275.00\n",
      "1  ERC_REST             1     Peak    56         4  187905  46976.25\n",
      "2  ERC_REST             1     Peak    57         4  168812  42203.00\n",
      "3  ERC_REST             1     Peak    58         4  155705  38926.25\n",
      "4  ERC_REST             1     Peak    59         4  172847  43211.75\n",
      "number of rows in dataset = 243586\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "2042  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "2233  ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1706  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1538  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "1537  ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW Day_Type  Hour_Counter  4-hr  Season_Group  Hour_Tot  \\\n",
      "2042 2016-03-05    5  Weekend          2043   511             2         1   \n",
      "2233 2016-02-04    3  Weekday          2234   559             2         3   \n",
      "1706 2016-03-04    4  Weekday          1707   427             2         3   \n",
      "1538 2016-03-04    4  Weekday          1539   385             2         2   \n",
      "1537 2016-02-03    2  Weekday          1538   385             2         2   \n",
      "\n",
      "        Tot           Avg  \n",
      "2042  26989  26989.000000  \n",
      "2233  83494  27831.333333  \n",
      "1706  81445  27148.333333  \n",
      "1538  54128  27064.000000  \n",
      "1537  54128  27064.000000  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case5_x = x_peak2.copy()\n",
    "case5_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case5_seasons = case5_seasons.drop(['bimonthly'], axis=1)\n",
    "case5_seasons = case5_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "\n",
    "case5_x = pd.merge(case5_x, case5_seasons, on='Month', how='left')\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case5 = case5_x.groupby(['Region','Season_Group','Day_Type','4-hr'],as_index=False).agg(aggregations)\n",
    "case5.columns = case5.columns.droplevel(0)\n",
    "case5.columns = ['Region','Season_Group','Day_Type','4-hr','Hour_Tot','Tot','Avg']\n",
    "print(case5.head())\n",
    "print('number of rows in dataset =',case5.shape[0])\n",
    "case5.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_3daytype_seasonbased_4hr.csv')\n",
    "print()\n",
    "\n",
    "case5_x2 = pd.merge(case5_x,case5,on=['Region','Season_Group','Day_Type','4-hr'],how='left')\n",
    "case5_x2 = case5_x2.sort_values(['Region',x_column])\n",
    "print(case5_x2.head())\n",
    "print('number of rows in dataset =',case5_x2.shape[0])\n",
    "case5_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_3daytype_seasonbased_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 6: Season, weekend/weekday/peak day-types, 4-hour intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region    Season Day_Type  4-hr  Hour_Tot     Tot       Avg\n",
      "0  ERC_REST  shoulder     Peak   535         4  120877  30219.25\n",
      "1  ERC_REST  shoulder     Peak   536         4  142982  35745.50\n",
      "2  ERC_REST  shoulder     Peak   537         4  155141  38785.25\n",
      "3  ERC_REST  shoulder     Peak   538         4  170205  42551.25\n",
      "4  ERC_REST  shoulder     Peak   539         4  165142  41285.50\n",
      "number of rows in dataset = 243586\n",
      "\n",
      "        Region R_Group R_Subgroup    Season  Month  Day  Hour   Load  \\\n",
      "2042  ERC_REST     ERC       REST  shoulder      3   86     3  26989   \n",
      "2233  ERC_REST     ERC       REST  shoulder      4   94     2  27006   \n",
      "1706  ERC_REST     ERC       REST  shoulder      3   72     3  27025   \n",
      "1538  ERC_REST     ERC       REST  shoulder      3   65     3  27059   \n",
      "1537  ERC_REST     ERC       REST  shoulder      3   65     2  27069   \n",
      "\n",
      "           Date  DOW Day_Type  Hour_Counter  4-hr  Hour_Tot    Tot  \\\n",
      "2042 2016-03-05    5  Weekend          2043   511         1  26989   \n",
      "2233 2016-02-04    3  Weekday          2234   559         3  83494   \n",
      "1706 2016-03-04    4  Weekday          1707   427         3  81445   \n",
      "1538 2016-03-04    4  Weekday          1539   385         2  54128   \n",
      "1537 2016-02-03    2  Weekday          1538   385         2  54128   \n",
      "\n",
      "               Avg  \n",
      "2042  26989.000000  \n",
      "2233  27831.333333  \n",
      "1706  27148.333333  \n",
      "1538  27064.000000  \n",
      "1537  27064.000000  \n",
      "number of rows in dataset = 551880\n"
     ]
    }
   ],
   "source": [
    "case6_x = x_peak2.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case6 = case6_x.groupby(['Region','Season','Day_Type','4-hr'],as_index=False).agg(aggregations)\n",
    "case6.columns = case6.columns.droplevel(0)\n",
    "case6.columns = ['Region','Season','Day_Type','4-hr','Hour_Tot','Tot','Avg']\n",
    "print(case6.head())\n",
    "print('number of rows in dataset =',case6.shape[0])\n",
    "case6.to_csv('../outputs/'+x_name+'/'+x_name+'_segments_3daytype_season_4hr.csv')\n",
    "print()\n",
    "\n",
    "case6_x2 = pd.merge(case6_x,case6,on=['Region','Season','Day_Type','4-hr'],how='left')\n",
    "case6_x2 = case6_x2.sort_values(['Region',x_column])\n",
    "print(case6_x2.head())\n",
    "print('number of rows in dataset =',case6_x2.shape[0])\n",
    "case6_x2.to_csv('../outputs/'+x_name+'/'+x_name+'_8760_3daytype_season_4hr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
