{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data here: \n",
    "## One and two day type \n",
    "## Three day type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day Type: One/two\n",
    "## Create Date and Day of Week columns in the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output files are written out in parent directory: C:\\Users\\cmarcy\\Desktop\\py_projects\\temporal\\temporal_working\\outputs\n"
     ]
    }
   ],
   "source": [
    "#importing packages needed for analysis\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from pandas import DataFrame\n",
    "\n",
    "path = os.getcwd()\n",
    "#print(path)\n",
    "\n",
    "#this code creates an output directory in the parent director, if one does not exist yet\n",
    "#Note: this is where all of the output files will be written, since outputs are large this saves space in git\n",
    "path = os.getcwd()\n",
    "parent = os.path.dirname(path)\n",
    "outputs_dir = parent+'\\outputs'\n",
    "if not os.path.exists(outputs_dir):\n",
    "    os.makedirs(outputs_dir)\n",
    "print('output files are written out in parent directory: '+outputs_dir)\n",
    "\n",
    "load_dur = pd.read_csv('../outputs/load_long_format.csv')\n",
    "solar_dur = pd.read_csv('../outputs/solar_long_format.csv')\n",
    "wind_dur = pd.read_csv('../outputs/wind_long_format.csv')\n",
    "\n",
    "## UNCOMMENT WHICH PROFILE TO BE USED\n",
    "x = load_dur\n",
    "x_name = 'load'\n",
    "x_name2 = 'Load'\n",
    "x_column = 'Load'\n",
    "\n",
    "#x = solar_dur\n",
    "#x_name = 'solar'\n",
    "#x_name2 = 'Solar_Gen'\n",
    "#choose TRG \n",
    "#x_column = 'TRG6'\n",
    "\n",
    "#x = wind_dur\n",
    "#x_name = 'wind'\n",
    "#x_name2 = 'Wind_Gen'\n",
    "#x_column = 'TRG4'\n",
    "\n",
    "x = x[['Region','R_Group','R_Subgroup','Season','Month','Day','Hour',x_column]]\n",
    "years = pd.read_csv('inputs/years.csv').dropna()\n",
    "\n",
    "#print(x.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region R_Group R_Subgroup  Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "0   FRCC    FRCC       FRCC  winter      1    1     1  17005    1  2011   \n",
      "1   FRCC    FRCC       FRCC  winter      1    2     1  15750    2  2011   \n",
      "2   FRCC    FRCC       FRCC  winter      1    3     1  16123    3  2011   \n",
      "3   FRCC    FRCC       FRCC  winter      1    4     1  18300    4  2011   \n",
      "4   FRCC    FRCC       FRCC  winter      1    5     1  17370    5  2011   \n",
      "\n",
      "        Date  DOW  Weekday  \n",
      "0 2011-01-01    5    False  \n",
      "1 2011-01-02    6    False  \n",
      "2 2011-01-03    0     True  \n",
      "3 2011-01-04    1     True  \n",
      "4 2011-01-05    2     True  \n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "daydata = pd.read_csv('inputs/days_365.csv')\n",
    "#print(daydata.tail())\n",
    "daydata = daydata.drop(columns='Month')\n",
    "x2 = pd.merge(x,daydata,on=['Day'],how='left')\n",
    "#print(x2.tail())\n",
    "\n",
    "#sets the year for each region\n",
    "x2['Year']=2011\n",
    "x2.loc[x2['R_Group'] == 'ERC', 'Year'] = 2016\n",
    "#print(x2.head())\n",
    "\n",
    "#Creates a date column\n",
    "x2 = x2.rename(columns={'Day':'DOY','DayofMo':'Day'})\n",
    "x2['Date']=pd.to_datetime(x2[['Year', 'Month', 'Day']], errors='coerce')\n",
    "#print(x2.tail())\n",
    "\n",
    "#convert date to a datetime type \n",
    "#x2['Date'] = pd.to_datetime(x2['Date'])\n",
    "x2['DOW'] = x2['Date'].dt.weekday\n",
    "\n",
    "#check if it is a weekday or not \n",
    "weekday = pd.read_csv('inputs/weekday.csv')\n",
    "x2 = pd.merge(x2,weekday,on='DOW',how='left')\n",
    "print(x2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Monthly, single day type, 24 hours (288 segments)\n",
    "#### Methodology: Using groupby function to group first by month, then by 24 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region  Month  Hour  Hour_Tot  Load_Tot      Load_Avg\n",
      "0   FRCC      1     1        31    626801  20219.387097\n",
      "1   FRCC      1     2        31    644817  20800.548387\n",
      "2   FRCC      1     3        31    694961  22418.096774\n",
      "3   FRCC      1     4        31    781019  25194.161290\n",
      "4   FRCC      1     5        31    836184  26973.677419\n",
      "number of rows in dataset = 288\n",
      "\n",
      "    R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "366    FRCC       FRCC    winter      1    2     2  15694    2  2011   \n",
      "73     FRCC       FRCC  shoulder      3   74     1  15738   15  2011   \n",
      "1      FRCC       FRCC    winter      1    2     1  15750    2  2011   \n",
      "\n",
      "          Date  DOW  Weekday  Hour_Tot  Load_Tot      Load_Avg  \n",
      "366 2011-01-02    6    False        31    644817  20800.548387  \n",
      "73  2011-03-15    1     True        31    519021  16742.612903  \n",
      "1   2011-01-02    6    False        31    626801  20219.387097  \n",
      "number of rows in dataset = 8760\n"
     ]
    }
   ],
   "source": [
    "case1_x = x2.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case1 = case1_x.groupby(['Region','Month','Hour'],as_index=False).agg(aggregations)\n",
    "case1.columns = case1.columns.droplevel(0)\n",
    "case1.columns = ['Region','Month','Hour','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case1.head())\n",
    "print('number of rows in dataset =',case1.shape[0])\n",
    "case1.to_csv('../outputs/'+x_name+'_segments_1daytype_monthly_24hr.csv')\n",
    "print()\n",
    "\n",
    "case1_x2 = pd.merge(case1_x,case1,on=['Region','Month','Hour'],how='left')\n",
    "case1_x2 = case1_x2.sort_values(['Region',x_column]).drop(case1_x2.columns[0], axis=1)\n",
    "print(case1_x2.head(3))\n",
    "print('number of rows in dataset =',case1_x2.shape[0])\n",
    "case1_x2.to_csv('../outputs/'+x_name+'_8760_1daytype_monthly_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: Season, single day-type, 24 hours (72 segments)\n",
    "#### Methodology: Use groupby function to group by season and hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region  Season_Group  Hour  Hour_Tot  Load_Tot      Load_Avg\n",
      "0   FRCC             1     1        90   1659148  18434.977778\n",
      "1   FRCC             1     2        90   1694864  18831.822222\n",
      "2   FRCC             1     3        90   1817817  20197.966667\n",
      "3   FRCC             1     4        90   2039870  22665.222222\n",
      "4   FRCC             1     5        90   2183989  24266.544444\n",
      "number of rows in dataset = 120\n",
      "\n",
      "    R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "366    FRCC       FRCC    winter      1    2     2  15694    2  2011   \n",
      "73     FRCC       FRCC  shoulder      3   74     1  15738   15  2011   \n",
      "1      FRCC       FRCC    winter      1    2     1  15750    2  2011   \n",
      "\n",
      "          Date  DOW  Weekday  Season_Group  Hour_Tot  Load_Tot      Load_Avg  \n",
      "366 2011-01-02    6    False             1        90   1694864  18831.822222  \n",
      "73  2011-03-15    1     True             2       122   2159594  17701.590164  \n",
      "1   2011-01-02    6    False             1        90   1659148  18434.977778  \n",
      "number of rows in dataset = 8760\n"
     ]
    }
   ],
   "source": [
    "case2_x = x2.copy()\n",
    "case2_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case2_seasons = case2_seasons.drop(['bimonthly'], axis=1)\n",
    "case2_seasons = case2_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "#print(case2_seasons)\n",
    "\n",
    "case2_x = pd.merge(case2_x, case2_seasons, on='Month', how='left')\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case2 = case2_x.groupby(['Region','Season_Group','Hour'],as_index=False).agg(aggregations)\n",
    "case2.columns = case2.columns.droplevel(0)\n",
    "case2.columns = ['Region','Season_Group','Hour','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case2.head())\n",
    "print('number of rows in dataset =',case2.shape[0])\n",
    "case2.to_csv('../outputs/'+x_name+'_segments_1daytype_season_24hr.csv')\n",
    "print()\n",
    "\n",
    "case2_x2 = pd.merge(case2_x,case2,on=['Region','Season_Group','Hour'],how='left')\n",
    "case2_x2 = case2_x2.sort_values(['Region',x_column]).drop(case2_x2.columns[0],axis=1)\n",
    "print(case2_x2.head(3))\n",
    "print('number of rows in dataset =',case2_x2.shape[0])\n",
    "case2_x2.to_csv('../outputs/'+x_name+'_8760_1daytype_season_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3: Monthly, weekend/weekday, 24 hours (576 segments)\n",
    "#### Metholodogy: Use groupby function to group by month, then weekend/weekday, then by 24 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region  Month  Hour  Hour_Tot  Load_Tot      Load_Avg\n",
      "0   FRCC      1     1        31    626801  20219.387097\n",
      "1   FRCC      1     2        31    644817  20800.548387\n",
      "2   FRCC      1     3        31    694961  22418.096774\n",
      "3   FRCC      1     4        31    781019  25194.161290\n",
      "4   FRCC      1     5        31    836184  26973.677419\n",
      "number of rows in dataset = 576\n",
      "\n",
      "    R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "366    FRCC       FRCC    winter      1    2     2  15694    2  2011   \n",
      "73     FRCC       FRCC  shoulder      3   74     1  15738   15  2011   \n",
      "1      FRCC       FRCC    winter      1    2     1  15750    2  2011   \n",
      "443    FRCC       FRCC  shoulder      3   79     2  15822   20  2011   \n",
      "72     FRCC       FRCC  shoulder      3   73     1  15831   14  2011   \n",
      "\n",
      "          Date  DOW  Weekday  Hour_Tot  Load_Tot      Load_Avg  \n",
      "366 2011-01-02    6    False        10    212142  21214.200000  \n",
      "73  2011-03-15    1     True        23    382919  16648.652174  \n",
      "1   2011-01-02    6    False        10    208342  20834.200000  \n",
      "443 2011-03-20    6    False         8    136110  17013.750000  \n",
      "72  2011-03-14    0     True        23    382919  16648.652174  \n",
      "number of rows in dataset = 8760\n"
     ]
    }
   ],
   "source": [
    "case3_x = x2.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case3 = case3_x.groupby(['Region','Month','Weekday','Hour'],as_index=False).agg(aggregations)\n",
    "case3.columns = case3.columns.droplevel(0)\n",
    "case3.columns = ['Region','Month','Weekday','Hour','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case1.head())\n",
    "print('number of rows in dataset =',case3.shape[0])\n",
    "case3.to_csv('../outputs/'+x_name+'_segments_2daytype_monthly_24hr.csv')\n",
    "print()\n",
    "\n",
    "case3_x2 = pd.merge(case3_x,case3,on=['Region','Month','Weekday','Hour'],how='left')\n",
    "case3_x2 = case3_x2.sort_values(['Region',x_column]).drop(case3_x2.columns[0],axis=1)\n",
    "print(case3_x2.head())\n",
    "print('number of rows in dataset =',case3_x2.shape[0])\n",
    "case3_x2.to_csv('../outputs/'+x_name+'_8760_2daytype_monthly_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 4: Season, weekend/weekday, 24 hours (144 segments)\n",
    "#### Methodology: groupby season, weekday, hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Region  Season_Group  Weekday  Hour  Hour_Tot  Load_Tot      Load_Avg\n",
      "235   FRCC             5     True    20        44   1435196  32618.090909\n",
      "236   FRCC             5     True    21        44   1307046  29705.590909\n",
      "237   FRCC             5     True    22        44   1191436  27078.090909\n",
      "238   FRCC             5     True    23        44   1111291  25256.613636\n",
      "239   FRCC             5     True    24        44   1057027  24023.340909\n",
      "number of rows in dataset = 240\n",
      "\n",
      "    R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "366    FRCC       FRCC    winter      1    2     2  15694    2  2011   \n",
      "73     FRCC       FRCC  shoulder      3   74     1  15738   15  2011   \n",
      "1      FRCC       FRCC    winter      1    2     1  15750    2  2011   \n",
      "443    FRCC       FRCC  shoulder      3   79     2  15822   20  2011   \n",
      "72     FRCC       FRCC  shoulder      3   73     1  15831   14  2011   \n",
      "\n",
      "          Date  DOW  Weekday  Season_Group  Hour_Tot  Load_Tot      Load_Avg  \n",
      "366 2011-01-02    6    False             1        27    512224  18971.259259  \n",
      "73  2011-03-15    1     True             2        87   1541095  17713.735632  \n",
      "1   2011-01-02    6    False             1        27    507490  18795.925926  \n",
      "443 2011-03-20    6    False             2        35    614008  17543.085714  \n",
      "72  2011-03-14    0     True             2        87   1541095  17713.735632  \n",
      "number of rows in dataset = 8760\n"
     ]
    }
   ],
   "source": [
    "case4_x = x2.copy()\n",
    "case4_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case4_seasons = case4_seasons.drop(['bimonthly'], axis=1)\n",
    "case4_seasons = case4_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "\n",
    "case4_x = pd.merge(case4_x, case4_seasons, on='Month', how='left')\n",
    "#print(case4_x)\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case4 = case4_x.groupby(['Region','Season_Group','Weekday','Hour'],as_index=False).agg(aggregations)\n",
    "case4.columns = case4.columns.droplevel(0)\n",
    "case4.columns = ['Region','Season_Group','Weekday','Hour','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case4.tail())\n",
    "print('number of rows in dataset =',case4.shape[0])\n",
    "case4.to_csv('../outputs/'+x_name+'_segments_2daytype_season_24hr.csv')\n",
    "print()\n",
    "\n",
    "case4_x2 = pd.merge(case4_x,case4,on=['Region','Season_Group','Weekday','Hour'],how='left')\n",
    "case4_x2 = case4_x2.sort_values(['Region',x_column]).drop(case4_x2.columns[0],axis=1)\n",
    "print(case4_x2.head())\n",
    "print('number of rows in dataset =',case4_x2.shape[0])\n",
    "case4_x2.to_csv('../outputs/'+x_name+'_8760_2daytype_season_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 hour interval day types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region R_Group R_Subgroup  Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "8755   FRCC    FRCC       FRCC  winter     12  361    24  17370   27  2011   \n",
      "8756   FRCC    FRCC       FRCC  winter     12  362    24  19366   28  2011   \n",
      "8757   FRCC    FRCC       FRCC  winter     12  363    24  19024   29  2011   \n",
      "8758   FRCC    FRCC       FRCC  winter     12  364    24  16730   30  2011   \n",
      "8759   FRCC    FRCC       FRCC  winter     12  365    24  17794   31  2011   \n",
      "\n",
      "           Date  DOW  Weekday  4-hr  \n",
      "8755 2011-12-27    1     True     6  \n",
      "8756 2011-12-28    2     True     6  \n",
      "8757 2011-12-29    3     True     6  \n",
      "8758 2011-12-30    4     True     6  \n",
      "8759 2011-12-31    5    False     6  \n"
     ]
    }
   ],
   "source": [
    "#read in 4 hour interval counter\n",
    "interval_4hr = pd.read_csv('inputs/interval_4hr.csv')\n",
    "#print(interval_4hr)\n",
    "\n",
    "x3 = pd.merge(x2,interval_4hr,on='Hour',how='left')\n",
    "print(x3.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 5: Monthly, single day type, 4 hour intervals (72 segments)\n",
    "#### Methodology: use groupby by month, 4 hour intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region  Month  4-hr  Hour_Tot  Load_Tot      Load_Avg\n",
      "0   FRCC      1     1       124   2747598  22158.048387\n",
      "1   FRCC      1     2       124   3359684  27094.225806\n",
      "2   FRCC      1     3       124   3143101  25347.588710\n",
      "3   FRCC      1     4       124   3171871  25579.604839\n",
      "4   FRCC      1     5       124   3273408  26398.451613\n",
      "number of rows in dataset = 72\n",
      "\n",
      "    R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "366    FRCC       FRCC    winter      1    2     2  15694    2  2011   \n",
      "73     FRCC       FRCC  shoulder      3   74     1  15738   15  2011   \n",
      "1      FRCC       FRCC    winter      1    2     1  15750    2  2011   \n",
      "443    FRCC       FRCC  shoulder      3   79     2  15822   20  2011   \n",
      "72     FRCC       FRCC  shoulder      3   73     1  15831   14  2011   \n",
      "\n",
      "          Date  DOW  Weekday  4-hr  Hour_Tot  Load_Tot      Load_Avg  \n",
      "366 2011-01-02    6    False     1       124   2747598  22158.048387  \n",
      "73  2011-03-15    1     True     1       124   2231100  17992.741935  \n",
      "1   2011-01-02    6    False     1       124   2747598  22158.048387  \n",
      "443 2011-03-20    6    False     1       124   2231100  17992.741935  \n",
      "72  2011-03-14    0     True     1       124   2231100  17992.741935  \n",
      "number of rows in dataset = 8760\n"
     ]
    }
   ],
   "source": [
    "case5_x = x3.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case5 = case5_x.groupby(['Region','Month','4-hr'],as_index=False).agg(aggregations)\n",
    "case5.columns = case5.columns.droplevel(0)\n",
    "case5.columns = ['Region','Month','4-hr','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case5.head())\n",
    "print('number of rows in dataset =',case5.shape[0])\n",
    "case5.to_csv('../outputs/'+x_name+'_segments_2daytype_month_4hr.csv')\n",
    "print()\n",
    "\n",
    "case5_x2 = pd.merge(case5_x,case5,on=['Region','Month','4-hr'],how='left')\n",
    "case5_x2 = case5_x2.sort_values(['Region',x_column]).drop(case5_x2.columns[0],axis=1)\n",
    "print(case5_x2.head())\n",
    "print('number of rows in dataset =',case5_x2.shape[0])\n",
    "case5_x2.to_csv('../outputs/'+x_name+'_8760_2daytype_month_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 6: Bi-monthly weekend/weekday day-types, 4-hour intervals\n",
    "#### Methodology: use groupby function and bimonthly groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region  Bimonth  Weekday  4-hr  Hour_Tot  Load_Tot      Load_Avg\n",
      "0   FRCC        1    False     1        72   1488088  20667.888889\n",
      "1   FRCC        1    False     2        72   1814946  25207.583333\n",
      "2   FRCC        1    False     3        72   1787406  24825.083333\n",
      "3   FRCC        1    False     4        72   1785866  24803.694444\n",
      "4   FRCC        1    False     5        72   1816454  25228.527778\n",
      "number of rows in dataset = 72\n",
      "\n",
      "    R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "366    FRCC       FRCC    winter      1    2     2  15694    2  2011   \n",
      "73     FRCC       FRCC  shoulder      3   74     1  15738   15  2011   \n",
      "1      FRCC       FRCC    winter      1    2     1  15750    2  2011   \n",
      "443    FRCC       FRCC  shoulder      3   79     2  15822   20  2011   \n",
      "72     FRCC       FRCC  shoulder      3   73     1  15831   14  2011   \n",
      "\n",
      "          Date  DOW  Weekday  4-hr  Bimonth  Hour_Tot  Load_Tot      Load_Avg  \n",
      "366 2011-01-02    6    False     1        1        72   1488088  20667.888889  \n",
      "73  2011-03-15    1     True     1        2       176   3366068  19125.386364  \n",
      "1   2011-01-02    6    False     1        1        72   1488088  20667.888889  \n",
      "443 2011-03-20    6    False     1        2        68   1222754  17981.676471  \n",
      "72  2011-03-14    0     True     1        2       176   3366068  19125.386364  \n",
      "number of rows in dataset = 8760\n"
     ]
    }
   ],
   "source": [
    "case6_x = x3.copy()\n",
    "case6_bimonth = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case6_bimonth = case6_bimonth.drop(['seasonal'], axis=1)\n",
    "case6_bimonth = case6_bimonth.rename(columns={'bimonthly':'Bimonth','month':'Month'})\n",
    "\n",
    "case6_x = pd.merge(case6_x, case6_bimonth, on='Month', how='left')\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case6 = case6_x.groupby(['Region','Bimonth','Weekday','4-hr'],as_index=False).agg(aggregations)\n",
    "case6.columns = case6.columns.droplevel(0)\n",
    "case6.columns = ['Region','Bimonth','Weekday','4-hr','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case6.head())\n",
    "print('number of rows in dataset =',case6.shape[0])\n",
    "case6.to_csv('../outputs/'+x_name+'_segments_2daytype_bimonth_4hr.csv')\n",
    "print()\n",
    "\n",
    "case6_x2 = pd.merge(case6_x,case6,on=['Region','Bimonth','Weekday','4-hr'],how='left')\n",
    "case6_x2 = case6_x2.sort_values(['Region',x_column]).drop(case6_x2.columns[0],axis=1)\n",
    "print(case6_x2.head())\n",
    "print('number of rows in dataset =',case6_x2.shape[0])\n",
    "case6_x2.to_csv('../outputs/'+x_name+'_8760_2daytype_bimonth_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 7: Season-based months, weekend/weekday day-types, 4-hour intervals\n",
    "#### Methodology: groupby function and applied season groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region    Season  Weekday  4-hr  Hour_Tot  Load_Tot      Load_Avg\n",
      "0   FRCC  shoulder    False     1       140   2507878  17913.414286\n",
      "1   FRCC  shoulder    False     2       140   3152037  22514.550000\n",
      "2   FRCC  shoulder    False     3       140   3846861  27477.578571\n",
      "3   FRCC  shoulder    False     4       140   4043491  28882.078571\n",
      "4   FRCC  shoulder    False     5       140   3739162  26708.300000\n",
      "number of rows in dataset = 36\n",
      "\n",
      "    R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "366    FRCC       FRCC    winter      1    2     2  15694    2  2011   \n",
      "73     FRCC       FRCC  shoulder      3   74     1  15738   15  2011   \n",
      "1      FRCC       FRCC    winter      1    2     1  15750    2  2011   \n",
      "443    FRCC       FRCC  shoulder      3   79     2  15822   20  2011   \n",
      "72     FRCC       FRCC  shoulder      3   73     1  15831   14  2011   \n",
      "\n",
      "          Date  DOW  Weekday  4-hr  Season_Group  Hour_Tot  Load_Tot  \\\n",
      "366 2011-01-02    6    False     1             1       108   2109600   \n",
      "73  2011-03-15    1     True     1             2       348   6642962   \n",
      "1   2011-01-02    6    False     1             1       108   2109600   \n",
      "443 2011-03-20    6    False     1             2       140   2507878   \n",
      "72  2011-03-14    0     True     1             2       348   6642962   \n",
      "\n",
      "         Load_Avg  \n",
      "366  19533.333333  \n",
      "73   19088.971264  \n",
      "1    19533.333333  \n",
      "443  17913.414286  \n",
      "72   19088.971264  \n",
      "number of rows in dataset = 8760\n"
     ]
    }
   ],
   "source": [
    "case7_x = x3.copy()\n",
    "case7_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case7_seasons = case7_seasons.drop(['bimonthly'], axis=1)\n",
    "case7_seasons = case7_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "\n",
    "case7_x = pd.merge(case7_x, case7_seasons, on='Month', how='left')\n",
    "#print(case4_load)\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case7 = case7_x.groupby(['Region','Season','Weekday','4-hr'],as_index=False).agg(aggregations)\n",
    "case7.columns = case7.columns.droplevel(0)\n",
    "case7.columns = ['Region','Season','Weekday','4-hr','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case7.head())\n",
    "print('number of rows in dataset =',case7.shape[0])\n",
    "case7.to_csv('../outputs/'+x_name+'_segments_2daytype_season_wkd_4hr.csv')\n",
    "print()\n",
    "\n",
    "case7_x2 = pd.merge(case7_x,case7,on=['Region','Season','Weekday','4-hr'],how='left')\n",
    "case7_x2 = case7_x2.sort_values(['Region',x_column]).drop(case7_x2.columns[0],axis=1)\n",
    "print(case7_x2.head())\n",
    "print('number of rows in dataset =',case7_x2.shape[0])\n",
    "case7_x2.to_csv('../outputs/'+x_name+'_8760_2daytype_season_wkd_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day type: Three (Weekday, Weekend, Peak Load)\n",
    "## Find Peak Load Days in Each Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Region  Month  Load_Max\n",
      "0    FRCC      1    839413\n",
      "1    FRCC      2    632009\n",
      "2    FRCC      3    629063\n",
      "3    FRCC      4    774706\n",
      "4    FRCC      5    761238\n",
      "5    FRCC      6    855573\n",
      "6    FRCC      7    848154\n",
      "7    FRCC      8    864191\n",
      "8    FRCC      9    798533\n",
      "9    FRCC     10    707155\n",
      "10   FRCC     11    657292\n",
      "11   FRCC     12    584382\n",
      "\n",
      "     Region R_Group R_Subgroup  Season  Month  Day  Hour   Load  Load_Tot  \\\n",
      "8755   FRCC    FRCC       FRCC  winter     12  361    24  17370    556346   \n",
      "8756   FRCC    FRCC       FRCC  winter     12  362    24  19366    546778   \n",
      "8757   FRCC    FRCC       FRCC  winter     12  363    24  19024    564643   \n",
      "8758   FRCC    FRCC       FRCC  winter     12  364    24  16730    542878   \n",
      "8759   FRCC    FRCC       FRCC  winter     12  365    24  17794    528679   \n",
      "\n",
      "      Load_Max  \n",
      "8755    584382  \n",
      "8756    584382  \n",
      "8757    584382  \n",
      "8758    584382  \n",
      "8759    584382  \n"
     ]
    }
   ],
   "source": [
    "#create temporary DF to find peak\n",
    "test = load_dur.copy()\n",
    "\n",
    "#groupby region, month, and day to sum the total day\n",
    "aggregations1 = {x_column:sum}\n",
    "md_sum = test.groupby(['Region','Month','Day'],as_index=False).agg(aggregations1)\n",
    "#test1.columns = test1.columns.droplevel(0)\n",
    "md_sum.columns = ['Region','Month','Day',x_name2+'_Tot']\n",
    "#print(test_sum.head())\n",
    "#print('number of rows in dataset =',test_sum.shape[0])\n",
    "\n",
    "md_sum2 = pd.merge(test,md_sum,on=['Region','Month','Day'],how='left')\n",
    "#print(test3)\n",
    "\n",
    "#groupby region and month to find maximum \n",
    "aggregations2 = {x_name2+'_Tot':max}\n",
    "md_max = md_sum2.groupby(['Region','Month'],as_index=False).agg(aggregations2)\n",
    "md_max.columns = ['Region','Month',x_name2+'_Max']\n",
    "print(md_max)\n",
    "print()\n",
    "\n",
    "peakd = pd.merge(test3,md_max,on=['Region','Month'],how='left')\n",
    "print(peakd.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region R_Group R_Subgroup  Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "0   FRCC    FRCC       FRCC  winter      1    1     1  17005    1  2011   \n",
      "1   FRCC    FRCC       FRCC  winter      1    2     1  15750    2  2011   \n",
      "2   FRCC    FRCC       FRCC  winter      1    3     1  16123    3  2011   \n",
      "3   FRCC    FRCC       FRCC  winter      1    4     1  18300    4  2011   \n",
      "4   FRCC    FRCC       FRCC  winter      1    5     1  17370    5  2011   \n",
      "\n",
      "        Date  DOW Day_Type  \n",
      "0 2011-01-01    5  Weekend  \n",
      "1 2011-01-02    6  Weekend  \n",
      "2 2011-01-03    0  Weekday  \n",
      "3 2011-01-04    1  Weekday  \n",
      "4 2011-01-05    2  Weekday  \n"
     ]
    }
   ],
   "source": [
    "x_peak = x2.copy()\n",
    "\n",
    "x_peak = pd.merge(x_peak,peakd,on=['Region','R_Group','R_Subgroup','Season','Month','Day','Hour',x_column],how='left')\n",
    "x_peak = x_peak.rename(columns={'Weekday':'Day_Type'})\n",
    "\n",
    "#Return True if the load total equals the day identified as the max\n",
    "x_peak.loc[x_peak['Day_Type'] == True, 'Day_Type'] = 'Weekday'\n",
    "x_peak.loc[x_peak['Day_Type'] == False, 'Day_Type'] = 'Weekend'\n",
    "x_peak.loc[x_peak[x_name2+'_Tot'] == x_peak[x_name2+'_Max'], 'Day_Type'] = 'Peak'\n",
    "x_peak = x_peak.drop([x_name2+'_Tot',x_name2+'_Max'], axis=1)\n",
    "print(x_peak.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Monthly, Weekend/weekday/peak day-types, 24 hours (864 segments)\n",
    "#### Methodology: similar to two day type, just adding in peak day types to sort by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region  Month Day_Type  Hour  Hour_Tot  Load_Tot  Load_Avg\n",
      "0   FRCC      1     Peak     1         1     32350   32350.0\n",
      "1   FRCC      1     Peak     2         1     33972   33972.0\n",
      "2   FRCC      1     Peak     3         1     37185   37185.0\n",
      "3   FRCC      1     Peak     4         1     41916   41916.0\n",
      "4   FRCC      1     Peak     5         1     44276   44276.0\n",
      "number of rows in dataset = 600\n",
      "\n",
      "     Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "300    FRCC    FRCC       FRCC    winter      1    2     2  15694    2  2011   \n",
      "4034   FRCC    FRCC       FRCC  shoulder      3   74     1  15738   15  2011   \n",
      "288    FRCC    FRCC       FRCC    winter      1    2     1  15750    2  2011   \n",
      "\n",
      "           Date  DOW Day_Type  Hour_Counter  Hour_Tot  Load_Tot      Load_Avg  \n",
      "300  2011-01-02    6  Weekend            26        10    212142  21214.200000  \n",
      "4034 2011-03-15    1  Weekday           337        23    382919  16648.652174  \n",
      "288  2011-01-02    6  Weekend            25        10    208342  20834.200000  \n",
      "number of rows in dataset = 8760\n"
     ]
    }
   ],
   "source": [
    "case1_x = x_peak.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case1 = case1_x.groupby(['Region','Month','Day_Type','Hour'],as_index=False).agg(aggregations)\n",
    "case1.columns = case1.columns.droplevel(0)\n",
    "case1.columns = ['Region','Month','Day_Type','Hour','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case1.head())\n",
    "print('number of rows in dataset =',case1.shape[0])\n",
    "case1.to_csv('../outputs/'+x_name+'_segments_3daytype_monthly_24hr.csv')\n",
    "print()\n",
    "\n",
    "case1_x2 = pd.merge(case1_x,case1,on=['Region','Month','Day_Type','Hour'],how='left')\n",
    "case1_x2 = case1_x2.sort_values(['Region',x_column])\n",
    "print(case1_x2.head(3))\n",
    "print('number of rows in dataset =',case1_x2.shape[0])\n",
    "case1_x2.to_csv('../outputs/'+x_name+'_8760_3daytype_monthly_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: Season, weekend/weekday/peak day-types, 24-hours (216 segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region    Season Day_Type  Hour  Hour_Tot  Load_Tot      Load_Avg\n",
      "0   FRCC  shoulder  Weekday     1        87   1541095  17713.735632\n",
      "1   FRCC  shoulder  Weekday     2        87   1553709  17858.724138\n",
      "2   FRCC  shoulder  Weekday     3        87   1661960  19102.988506\n",
      "3   FRCC  shoulder  Weekday     4        87   1886198  21680.436782\n",
      "4   FRCC  shoulder  Weekday     5        87   2013602  23144.850575\n",
      "number of rows in dataset = 168\n",
      "\n",
      "     Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "300    FRCC    FRCC       FRCC    winter      1    2     2  15694    2  2011   \n",
      "4034   FRCC    FRCC       FRCC  shoulder      3   74     1  15738   15  2011   \n",
      "288    FRCC    FRCC       FRCC    winter      1    2     1  15750    2  2011   \n",
      "\n",
      "           Date  DOW Day_Type  Hour_Counter  Hour_Tot  Load_Tot      Load_Avg  \n",
      "300  2011-01-02    6  Weekend            26        10    212142  21214.200000  \n",
      "4034 2011-03-15    1  Weekday           337        23    382919  16648.652174  \n",
      "288  2011-01-02    6  Weekend            25        10    208342  20834.200000  \n",
      "number of rows in dataset = 8760\n"
     ]
    }
   ],
   "source": [
    "case2_x = x_peak.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case2 = case2_x.groupby(['Region','Season','Day_Type','Hour'],as_index=False).agg(aggregations)\n",
    "case2.columns = case2.columns.droplevel(0)\n",
    "case2.columns = ['Region','Season','Day_Type','Hour','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case2.head())\n",
    "print('number of rows in dataset =',case2.shape[0])\n",
    "case2.to_csv('../outputs/'+x_name+'_segments_3daytype_season_24hr.csv')\n",
    "print()\n",
    "\n",
    "case2_x2 = pd.merge(case2_x,case2,on=['Region','Season','Day_Type','Hour'],how='left')\n",
    "case2_x2 = case2_x2.sort_values(['Region',x_column])\n",
    "print(case1_x2.head(3))\n",
    "print('number of rows in dataset =',case1_x2.shape[0])\n",
    "case2_x2.to_csv('../outputs/'+x_name+'_8760_3daytype_season_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3: Annual, weekend/weekday/peak day-types, 24-hours (72 segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region Day_Type  Hour  Hour_Tot  Load_Tot  Load_Avg\n",
      "0   FRCC     Peak     1         1     32350   32350.0\n",
      "1   FRCC     Peak     2         1     33972   33972.0\n",
      "2   FRCC     Peak     3         1     37185   37185.0\n",
      "3   FRCC     Peak     4         1     41916   41916.0\n",
      "4   FRCC     Peak     5         1     44276   44276.0\n",
      "number of rows in dataset = 72\n",
      "\n",
      "     Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "300    FRCC    FRCC       FRCC    winter      1    2     2  15694    2  2011   \n",
      "4034   FRCC    FRCC       FRCC  shoulder      3   74     1  15738   15  2011   \n",
      "288    FRCC    FRCC       FRCC    winter      1    2     1  15750    2  2011   \n",
      "\n",
      "           Date  DOW Day_Type  Hour_Counter  Hour_Tot  Load_Tot      Load_Avg  \n",
      "300  2011-01-02    6  Weekend            26       105   2051738  19540.361905  \n",
      "4034 2011-03-15    1  Weekday           337       259   5088736  19647.629344  \n",
      "288  2011-01-02    6  Weekend            25       105   2072468  19737.790476  \n",
      "number of rows in dataset = 8760\n"
     ]
    }
   ],
   "source": [
    "case3_x = x_peak.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case3 = case3_x.groupby(['Region','Day_Type','Hour'],as_index=False).agg(aggregations)\n",
    "case3.columns = case3.columns.droplevel(0)\n",
    "case3.columns = ['Region','Day_Type','Hour','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case3.head())\n",
    "print('number of rows in dataset =',case3.shape[0])\n",
    "case3.to_csv('../outputs/'+x_name+'_segments_3daytype_annual_24hr.csv')\n",
    "print()\n",
    "\n",
    "case3_x2 = pd.merge(case3_x,case3,on=['Region','Day_Type','Hour'],how='left')\n",
    "case3_x2 = case3_x2.sort_values(['Region',x_column])\n",
    "print(case3_x2.head(3))\n",
    "print('number of rows in dataset =',case3_x2.shape[0])\n",
    "case3_x2.to_csv('../outputs/'+x_name+'_8760_3daytype_annual_24hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 hour interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in 4 hour interval counter\n",
    "interval_4hr = pd.read_csv('inputs/sequential_hours.csv')\n",
    "interval_4hr = interval_4hr.drop(columns=['2-hr','8-hr','12-hr','24-hr','48-hr','120-hr'])\n",
    "#print(interval_4hr)\n",
    "\n",
    "#add an hour counter\n",
    "x_peak['Hour_Counter'] = (x_peak['Hour']) + (x_peak['Day'] - 1) * 24\n",
    "x_peak = x_peak.sort_values(by=['Region','Hour_Counter'])\n",
    "unique_hc = pd.Series(x_peak['Hour_Counter'].unique()).dropna()\n",
    "#print(unique_hc.tail(2))\n",
    "\n",
    "x_peak2 = pd.merge(x_peak,interval_4hr,on='Hour_Counter',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 4: Bi-monthly, weekend/weekday/peak day-types, 4-hour intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region  Bimonth Day_Type  4-hr  Hour_Tot  Load_Tot  Load_Avg\n",
      "0   FRCC        1     Peak    73         4    145423  36355.75\n",
      "1   FRCC        1     Peak    74         4    167231  41807.75\n",
      "2   FRCC        1     Peak    75         4    128113  32028.25\n",
      "3   FRCC        1     Peak    76         4    125565  31391.25\n",
      "4   FRCC        1     Peak    77         4    145710  36427.50\n",
      "number of rows in dataset = 1728\n",
      "\n",
      "     Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "300    FRCC    FRCC       FRCC    winter      1    2     2  15694    2  2011   \n",
      "4034   FRCC    FRCC       FRCC  shoulder      3   74     1  15738   15  2011   \n",
      "288    FRCC    FRCC       FRCC    winter      1    2     1  15750    2  2011   \n",
      "5486   FRCC    FRCC       FRCC  shoulder      3   79     2  15822   20  2011   \n",
      "3746   FRCC    FRCC       FRCC  shoulder      3   73     1  15831   14  2011   \n",
      "\n",
      "           Date  DOW Day_Type  Hour_Counter  4-hr  Bimonth  Hour_Tot  \\\n",
      "300  2011-01-02    6  Weekend            26     7        1         4   \n",
      "4034 2011-03-15    1  Weekday           337    85        2         8   \n",
      "288  2011-01-02    6  Weekend            25     7        1         4   \n",
      "5486 2011-03-20    6  Weekend           458   115        2         4   \n",
      "3746 2011-03-14    0  Weekday           313    79        2         8   \n",
      "\n",
      "      Load_Tot  Load_Avg  \n",
      "300      64198  16049.50  \n",
      "4034    146892  18361.50  \n",
      "288      64198  16049.50  \n",
      "5486     64511  16127.75  \n",
      "3746    146324  18290.50  \n",
      "number of rows in dataset = 8760\n"
     ]
    }
   ],
   "source": [
    "case4_x = x_peak2.copy()\n",
    "case4_bimonth = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case4_bimonth = case4_bimonth.drop(['seasonal'], axis=1)\n",
    "case4_bimonth = case4_bimonth.rename(columns={'bimonthly':'Bimonth','month':'Month'})\n",
    "\n",
    "case4_x = pd.merge(case4_x, case4_bimonth, on='Month', how='left')\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case4 = case4_x.groupby(['Region','Bimonth','Day_Type','4-hr'],as_index=False).agg(aggregations)\n",
    "case4.columns = case4.columns.droplevel(0)\n",
    "case4.columns = ['Region','Bimonth','Day_Type','4-hr','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case4.head())\n",
    "print('number of rows in dataset =',case4.shape[0])\n",
    "case4.to_csv('../outputs/'+x_name+'_segments_3daytype_bimonth_4hr.csv')\n",
    "print()\n",
    "\n",
    "case4_x2 = pd.merge(case4_x,case4,on=['Region','Bimonth','Day_Type','4-hr'],how='left')\n",
    "case4_x2 = case4_x2.sort_values(['Region',x_column])\n",
    "print(case4_x2.head())\n",
    "print('number of rows in dataset =',case4_x2.shape[0])\n",
    "case4_x2.to_csv('../outputs/'+x_name+'_8760_3daytype_bimonth_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 5: Season-based months, weekend/weekday/peak day-types, 4-hour intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region  Season_Group Day_Type  4-hr  Hour_Tot  Load_Tot  Load_Avg\n",
      "0   FRCC             1     Peak    73         4    145423  36355.75\n",
      "1   FRCC             1     Peak    74         4    167231  41807.75\n",
      "2   FRCC             1     Peak    75         4    128113  32028.25\n",
      "3   FRCC             1     Peak    76         4    125565  31391.25\n",
      "4   FRCC             1     Peak    77         4    145710  36427.50\n",
      "number of rows in dataset = 1434\n",
      "\n",
      "     Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "300    FRCC    FRCC       FRCC    winter      1    2     2  15694    2  2011   \n",
      "4034   FRCC    FRCC       FRCC  shoulder      3   74     1  15738   15  2011   \n",
      "288    FRCC    FRCC       FRCC    winter      1    2     1  15750    2  2011   \n",
      "5486   FRCC    FRCC       FRCC  shoulder      3   79     2  15822   20  2011   \n",
      "3746   FRCC    FRCC       FRCC  shoulder      3   73     1  15831   14  2011   \n",
      "\n",
      "           Date  DOW Day_Type  Hour_Counter  4-hr  Season_Group  Hour_Tot  \\\n",
      "300  2011-01-02    6  Weekend            26     7             1         4   \n",
      "4034 2011-03-15    1  Weekday           337    85             2        12   \n",
      "288  2011-01-02    6  Weekend            25     7             1         4   \n",
      "5486 2011-03-20    6  Weekend           458   115             2         8   \n",
      "3746 2011-03-14    0  Weekday           313    79             2        16   \n",
      "\n",
      "      Load_Tot      Load_Avg  \n",
      "300      64198  16049.500000  \n",
      "4034    224371  18697.583333  \n",
      "288      64198  16049.500000  \n",
      "5486    134726  16840.750000  \n",
      "3746    303255  18953.437500  \n",
      "number of rows in dataset = 8760\n"
     ]
    }
   ],
   "source": [
    "case5_x = x_peak2.copy()\n",
    "case5_seasons = pd.read_csv('inputs/season_bimonthly.csv')\n",
    "case5_seasons = case5_seasons.drop(['bimonthly'], axis=1)\n",
    "case5_seasons = case5_seasons.rename(columns={'seasonal':'Season_Group','month':'Month'})\n",
    "\n",
    "case5_x = pd.merge(case5_x, case5_seasons, on='Month', how='left')\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case5 = case5_x.groupby(['Region','Season_Group','Day_Type','4-hr'],as_index=False).agg(aggregations)\n",
    "case5.columns = case5.columns.droplevel(0)\n",
    "case5.columns = ['Region','Season_Group','Day_Type','4-hr','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case5.head())\n",
    "print('number of rows in dataset =',case5.shape[0])\n",
    "case5.to_csv('../outputs/'+x_name+'_segments_3daytype_seasonbased_4hr.csv')\n",
    "print()\n",
    "\n",
    "case5_x2 = pd.merge(case5_x,case5,on=['Region','Season_Group','Day_Type','4-hr'],how='left')\n",
    "case5_x2 = case5_x2.sort_values(['Region',x_column])\n",
    "print(case5_x2.head())\n",
    "print('number of rows in dataset =',case5_x2.shape[0])\n",
    "case5_x2.to_csv('../outputs/'+x_name+'_8760_3daytype_seasonbased_4hr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 6: Season, weekend/weekday/peak day-types, 4-hour intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region    Season Day_Type  4-hr  Hour_Tot  Load_Tot      Load_Avg\n",
      "0   FRCC  shoulder  Weekday     1        12    223100  18591.666667\n",
      "1   FRCC  shoulder  Weekday     2        12    290005  24167.083333\n",
      "2   FRCC  shoulder  Weekday     3        12    329694  27474.500000\n",
      "3   FRCC  shoulder  Weekday     4        12    342480  28540.000000\n",
      "4   FRCC  shoulder  Weekday     5        12    318975  26581.250000\n",
      "number of rows in dataset = 1044\n",
      "\n",
      "     Region R_Group R_Subgroup    Season  Month  DOY  Hour   Load  Day  Year  \\\n",
      "300    FRCC    FRCC       FRCC    winter      1    2     2  15694    2  2011   \n",
      "4034   FRCC    FRCC       FRCC  shoulder      3   74     1  15738   15  2011   \n",
      "288    FRCC    FRCC       FRCC    winter      1    2     1  15750    2  2011   \n",
      "5486   FRCC    FRCC       FRCC  shoulder      3   79     2  15822   20  2011   \n",
      "3746   FRCC    FRCC       FRCC  shoulder      3   73     1  15831   14  2011   \n",
      "\n",
      "           Date  DOW Day_Type  Hour_Counter  4-hr  Hour_Tot  Load_Tot  \\\n",
      "300  2011-01-02    6  Weekend            26     7         4     64198   \n",
      "4034 2011-03-15    1  Weekday           337    85        12    224371   \n",
      "288  2011-01-02    6  Weekend            25     7         4     64198   \n",
      "5486 2011-03-20    6  Weekend           458   115         8    134726   \n",
      "3746 2011-03-14    0  Weekday           313    79        16    303255   \n",
      "\n",
      "          Load_Avg  \n",
      "300   16049.500000  \n",
      "4034  18697.583333  \n",
      "288   16049.500000  \n",
      "5486  16840.750000  \n",
      "3746  18953.437500  \n",
      "number of rows in dataset = 8760\n"
     ]
    }
   ],
   "source": [
    "case6_x = x_peak2.copy()\n",
    "\n",
    "aggregations = {x_column:['count',sum,'mean']}\n",
    "case6 = case6_x.groupby(['Region','Season','Day_Type','4-hr'],as_index=False).agg(aggregations)\n",
    "case6.columns = case6.columns.droplevel(0)\n",
    "case6.columns = ['Region','Season','Day_Type','4-hr','Hour_Tot',x_name2+'_Tot',x_name2+'_Avg']\n",
    "print(case6.head())\n",
    "print('number of rows in dataset =',case6.shape[0])\n",
    "case6.to_csv('../outputs/'+x_name+'_segments_3daytype_season_4hr.csv')\n",
    "print()\n",
    "\n",
    "case6_x2 = pd.merge(case6_x,case6,on=['Region','Season','Day_Type','4-hr'],how='left')\n",
    "case6_x2 = case6_x2.sort_values(['Region',x_column])\n",
    "print(case6_x2.head())\n",
    "print('number of rows in dataset =',case6_x2.shape[0])\n",
    "case6_x2.to_csv('../outputs/'+x_name+'_8760_3daytype_season_4hr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
